
\input{../texlib/head}
\begin{document}
\ifdefined\BOOK
\else
\setcounter{chapter}{5}
\fi

\chapter{Master Theorem and Linear Time Sorts.} 

\section{Linear Time Sorts}
\paragraph{ Counting sort.}
Counting sort assumes that each of the $n$ input elements is an integer with a size at most $k$. It runs in $\Theta(n + k)$ time, so when $k = O(n)$, counting sort runs in $\Theta(n)$ time. It first determines, for each input element $x$, the number of elements less than or equal to $x$. Then, it uses this information to place element $x$ directly into its position in the output array. For example, if there are 17 elements less than or equal to $x$, then $x$ will be placed in position 17. However, we need to make some adjustments to this method to handle cases where multiple elements have the \textbf{same value}. We do not want all of them to end up in the same position.

%\begin{algbox}{Counting-sort$(A, n, k)$}
  \begin{algorithm}
  	let B and C be new arrays at size $n$ and $k$ \\ 
  	\For{ $i \in  [0, k]$}{
		$C_{i} \leftarrow 0 $
  	}
	\For{ $j \leftarrow [1, n]$}{
	  $C_{A{j}} \leftarrow C_{A_{j}} + 1 $
 	}
	\For{ $i \in  [1, k]$}{
	  $C_{i} \leftarrow C_{i} + C_{i – 1} $
 	}
	\For{ $j \in [n , 1]$}{
	  $B_{C_{A_{j}}} \leftarrow A_{j}$ \\
	  $C_{A_{j}} \leftarrow C_{A{j}} – 1$ \ \ // to handle duplicate values
	}
	  return $B$
\caption{Counting-sort$(A, n, k)$}
  \end{algorithm}
%\end{algbox}

Notice that the Counting sort can beat the lower bound of $\Omega\left(n \log n\right)$ only because it is not a comparison sort. In fact, no comparisons between input elements occur anywhere in the code. Instead, counting sort uses the actual values of the elements to index into an array.

An important property of the counting sort is that it is \textbf{stable}.

\begin{defbox}{Stable Sort.}
 We will say that a sorting algorithm is stable if elements with the same value appear in the output array in the same order as they do in the input array. \end{defbox}

Counting sort's stability is important for another reason: counting sort is often used as a subroutine in radix sort. As we shall see in the next section, in order for radix sort to work correctly, counting sort must be stable.


\paragraph{Radix sort}
\marginnote[Note 1: That introduction was copied word by word from a web source. Do not use for commercial purposes. ]{Note 1: That introduction was copied word by word from a web source. Do not use for commercial purposes.} Radix sort is the algorithm used by the card-sorting machines you now find only in computer museums. The cards have 80 columns, and in each column, a machine can punch a hole in one of 12 places. The sorter can be mechanically "programmed" to examine a given column of each card in a deck and distribute the card into one of 12 bins depending on which place has been punched. An operator can then gather the cards bin by bin, so that cards with the first place punched are on top of cards with the second place punched, and so on.

The Radix-sort procedure assumes that each element in the array $A$ has d digits, where digit 1 is the lowest-order digit and digit $d$ is the highest-order digit.


%\begin{algbox}{radix-sort(A, n, d)}
  \begin{algorithm}
    \For{ $ i \in [1,d]$ } {
        use a stable sort to sort array $A$ on digit $i$
    }
\caption{radix-sort($A$, $n$, $d$)}
  \end{algorithm}
%\end{algbox}

\paragraph{Correctness Proof.} By induction on the column being sorted.
\begin{itemize}
  \item Base. Where $d = 1$, the correctness follows immediately from the correctness of our base sort subroutine. 
  \item Induction Assumption. Assume that Radix-sort is correct for any array of numbers containing at most $d-1$ digits. 
  \item Step. Let $A^{\prime}$  be the algorithm output. Consider $x,y \in A$. Assume without losing generality that $x > y$. Denote by $x_{d}, y_{d}$ their $d$-digit and by $x_{/d}, y_{/d}$ the numbers obtained by taking only the first  $d-1$ digits of $x,y$. Separate in two cases:

    \begin{itemize}
      \item   If $x_{d} > y_{d}$ then a scenario in which $x$ appear prior to $y$ is  imply contradiction to the correctness of our subroutine.
      \item   So consider the case in which $x_{d} = y_{d}$. In that case, it must hold that $x_{/d} > y_{/d}$. Then the appearance of $x$ prior to $y$ either contradicts the assumption that the base algorithm we have used is stable or that $x$ appears before $y$ at the end of the $d-1$ iteration. Which contradicts the induction assumption. 
    \end{itemize}
 \end{itemize}

The analysis of the running time depends on the stable sort used as the intermediate sorting algorithm. When each digit lies in the range $0$ to $k – 1$ (so that it can take on $k$ possible values), and $k$ is not too large, counting sort is the obvious choice. Each pass over $n$ $d$-digit numbers then takes $\Theta(n + k)$ time. There are $d$ passes, and so the total time for radix sort is $\Theta\left(d(n + k)\right)$.

\paragraph{Bucket sort. \ctt{Only if there is time.} }
\marginnote[Note 2:  We will return to analyze the expected (average) running time after the lecture on probability.
]{Note 2: We will return to analyze the expected (average) running time after the lecture on probability.
}Bucket sort divides the interval [0, 1) into n equal-sized subintervals, or buckets, and then distributes the n input numbers into the buckets. Since the inputs are uniformly and independently distributed over [0, 1), we do not expect many numbers to fall into each bucket. To produce the output, we simply sort the numbers in each bucket and then go through the buckets in order, listing the elements in each.

%\begin{algbox}{bucket-sort(A, n)}
  \begin{algorithm}
    	let B[0 : n – 1] be a new array \\
	\For{ $i \leftarrow [0, n – 1]$}{
	   make $B_{i}$ an empty list
       	}
	\For{ $i \leftarrow [1, n]$}{
	    insert $A_{i}$ into list $B_{ \lfloor n A_{i} \rfloor} ]$
       	}
	\For{ $i \leftarrow [0, n – 1]$}{
	    sort list $B_{i}$
       	}
	concatenate the lists $B_{0}, B_{1}, .. , B_{n – 1}$ together and\\
	return the concatenated lists
\caption{bucket-sort($A$, $n$)}
  \end{algorithm}
%\end{algbox}

%\input{../texlib/tail}

\section{Sorting in Comparison Model.}

We have learned in the lecture that the runs of any sorting algorithm, which does not assume anything about the input's structure except the ability to compare elements in pairs, can be modeled as a binary tree. At each node, the algorithm compares two elements and, based on the result, moves to either the left or right child. Eventually, we reach the leaf of the tree and output the sorted elements. Notice that the nodes only exist in our imagination; the algorithm is not aware of their existence. We will name that tree the comparison tree.

The height of the comparison tree is (at least) the running time of the algorithm. We are going to demonstrate how to use these ideas to lower-bound the running time of the find-peak problem in the comparison model.


\begin{example}
  Prove that no algorithm can find a peak in less than $\Theta\left( \log n \right)$ time.
\end{example}
\begin{proof}
  Consider the following inputs for the problem, $A^{j}$ will be a tringle that get's is point at position $j$. Namely:  
  \begin{equation*}
      A^{j}_{i} = \begin{cases}
        i & i < j \\
         j + j - i & \text{else}
      \end{cases}
  \end{equation*} 
  Now, let's assume, in contradiction, that there exists an algorithm $\mathcal{A}$ in the comparison model that runs in less than $\log n$ time. This implies that the comparison tree representation of its running has a height less than $\log n$ and the number of its leaves, each associated with a possible output, is less than $n$. Remember that we have $n$ inputs of the form $A^{j}$. 

  By applying the pigeonhole principle, we can conclude that there are distinct $j$ and $j^{\prime}$ such that $\mathcal{A}$ reaches the same leaf when given $A^{j}$ and $A^{j^{\prime}}$ as inputs. Since their peaks are set at different positions, it follows that the algorithm will output a wrong answer for at least one of them.
\end{proof}

%\paragraph{Question.} Suppose that $\mathcal{A}$ outputs some number depending on the input, for example, ''Return $A$[$A$[7]]''. Does the argument above still work (and if so, why)?


\begin{example}
  Prove that no algorithm, in the comparison model, can merge two sorted arrays in time $\Theta(n^{1-\varepsilon})$ for some $\varepsilon>0$.
\end{example}
\begin{proof}
 Assuming, for contradiction, that there exists an algorithm $\mathcal{A}$ in the comparison model that merges two given sorted arrays in time $\Theta\left( n^{1-\varepsilon} \right)$. Now, consider the sorting algorithm obtained by replacing the merge subroutine in merge-sort with $\mathcal{A}$. On one hand, our new algorithm is a sorting algorithm in the comparison model (otherwise it would contradict the correctness of $\mathcal{A}$), while on the other hand, its running time is equal to:
  \begin{equation*}
    \begin{split}
      T(n) = n^{1-\varepsilon} + 2T(n/2) \Rightarrow T(n) = \Theta(n)
    \end{split}
  \end{equation*}
  This contradicts our $\Omega(n\log n)$ lower bound for sorting in the comparison model.
\end{proof}

\newpage

\section{Binaries Trees and How to Encode Them. \ctt{Only if there is time.}} We have already seen, in heaps, that organizing our data in a graph-like structure can offer a speed advantage. For future applications, and in particular for maintaining data in sorted order, we will have to encode our data using binary trees. These trees may not be almost complete and also have to support pointer manipulations, specifically placing a binary tree as a left or right subtree of a given node. To enable this, we will have to treat the \textbf{right}, \textbf{left}, and \textbf{parent} as variables, in contrast to heaps where they are determined completely by the node index. We begin this section by stating definitions.


\begin{definition}
  \begin{enumerate}
    \item Binary Tree: A tree in which any vertex has at most two children.
    \item A descendant of vertex $x$ is a vertex in the subtree whose root is $x$. A left descendant of vertex $x$ is either a vertex in the subtree whose root is the left child of $x$, or $x$'s left child.
    \item An ancestor of $x$ is a vertex to which $x$ belongs as a descendant.
    \item A leaf is a vertex without children.
    \item Height of vertex $x$ is the length of the longest simple path (without cycles) between $x$ and one of the leaves.
    \item Height of the tree is the height of its root, which is usually denoted by $h$.
\end{enumerate}
\end{definition}

We encode a binary tree by associating a field to each vertex $x$, representing its right, left children, and parent. We use the notation $x$.left to refer to the left child of $x$, although the physical implementation may differ conceptually. For example, the way binary trees are implemented in Cormen is through 4 arrays. The first stores the value of $x$, while the others store pointers of specific types. For instance, the array LEFT, where LEFT.$x$ stores the left child of $x$.


If nothing else has been mentioned, then we can assume that we can add additional fields to the vertices.


\section{Binary Search Trees.} Binary search tree (BST) is a binary tree which any node $x$ of it: 
\begin{enumerate}
  \item Contains a field key, storing a number $x$.key. 
  \item Any left descendant $y$ of $x$ satisfies $y$.key $\le$ $x$.key. 
  \item Any right descendant $y$ of $x$ satisfies $y$.key $\ge$ $x$.key. 
\end{enumerate}

\paragraph{Question.} Let $T$ be a binary search tree, Where are the minimum and maximum values of $T$? (most left and right nodes). 

\begin{definition}
Let $T$ be a binary search tree, and let $x$ be a node belonging to it. The predecessor of $x$ will be defined as a vertex $y$ such that $y$.key $\leq x$.key and $y$.key is maximal among the nodes satisfying this condition. If we were to set the values of $T$ in sorted order, then the predecessor of $x$ would be located on its left. The successor of $x$ will be defined as $y$, where $x$ is the predecessor of $y$.
\end{definition}

\paragraph{Functionalitiy of BST.}
\begin{enumerate}
\item Search($T$, key): returns a pointer to the vertex whose key equals key. 
  \item Min($T$): returns a pointer to the vertex with the minimum value in $T$.  
  \item Max($T$): returns a pointer to the vertex with the maximum value in $T$.  
  \item Predecessor($x$): returns a pointer to the predecessor of $x$.  
  \item Successor($x$): returns a pointer to the successor of $x$. 
  \item Insert($T$, key): inserts key into $T$ (creates a new vertex).   
  \item Delete($T$, $x$): removes $x$ from $T$. 
  \item Inorder($T$): outputs $T$'s keys in sorted order.

%\begin{algorithm}[H]
%\SetAlgoLined
%\KwData{$T$ - tree, $x$ - vertex to delete}
%\KwResult{removes $x$ from $T$}
%\Set
\end{enumerate}


\begin{example}Questions from past exams.
  \begin{enumerate}
    \item Write an algorithm that, given a binary search tree $T$, returns a max heap containing all the elements of $T$. What is the lower bound for this problem? Explain.
    \item Write an algorithm that, given a heap $H$, returns a binary search tree containing all the elements of $H$. What is the lower bound for this problem? Explain.
  \end{enumerate}
\end{example}

\paragraph{Solution.} 
  \begin{enumerate}
    \item Remember that any array sorted in reverse order is also a max heap. This is because if $A_{i} \ge A_{j}$ for any $i < j$, then it follows that $A_{i} \ge A_{2i}, A_{2i+1}$ for any $i$. Therefore, we will output the values of $T$ in reverse order. We do this by applying Inorder in reverse, as given in \Cref{alg:treetoheap}. The running time is $\Theta(n)$ and of course it is also the lower bound (as we must read all the inputs for printing it).
      \begin{algorithm}
       \KwData{$r$ - a vertex in BST}
       
       \If{$r$.right is not empty/Null}{
         Reverse-Order($r$.right)
       }
       Print $r$.key \\
       \If{$r$.left is not empty/Null}{
         Reverse-Order($r$.left)
       }
        \caption{Reverse-Order }
\label{alg:treetoheap}
      \end{algorithm}
    \item We will initialize an empty binary search tree and then add the elements to it one by one. Each insertion will cost the current height of the tree, so it might sum up to $\Theta(n^{2})$. However, next week we will see how those insertions could be done in a way that preserves the balance of the tree, namely guaranteeing that the height of the tree will remain logarithmic.
  \end{enumerate}

\begin{example} Adding fields to BSTs. We would like to support $k$-smallest element extraction. Suggest a field that will be used for computing the $k$-smallest element, explain how to maintain it, and write an algorithm that extracts the $k$-smallest element. 
\end{example}

\paragraph{Solution.} We will associate with each vertex of the tree a field that counts the number of nodes whose keys are lower than it (the size of its left subtree). Let's denote it as left-size. The code is given in \Cref{alg:kstat}. Guideline for proving correctness:
Consider the root, and let's separate into cases based on how many elements the root is greater than.
\begin{enumerate}
  \item If the root is strictly greater than $k-1$ elements, then the $k$-smallest element is in its left subtree and is also the $k-1$ smallest element in that subtree.
  \item If the root is greater than strictly fewer than $k-1$ elements, then it is clear that the $k$-smallest element is also greater than it and located in its right subtree. However, in contrast to the previous case, here the order of the $k$-smallest element of the whole tree in the subtree will be the substitution between $k$ and the number of nodes in the $\{$ left-subtree $\cup$ root $\}$.
  \item The third case is when the root is greater than exactly $k-1$ elements, which by definition sets it as the $k$-smallest element.
\end{enumerate}
In the first two cases, we are calling Get-Order on trees (BST with our additional field) with smaller height. Therefore, it is clear how one would prove correctness by induction. We will see in the next recitation how to maintain the tree, update the field, and guarantee that the height of the tree will remain logarithmic.
  \begin{algorithm}
  \KwData{$r$ - a vertex in BST, $k$-stat parameter}
       $m \leftarrow r$.left-size \\ 
       \If{$m = k -1$ }{
         \Return $r$.key 
       }
      \If{$m > k -1$ }{
        \Return Get-Order($r$.left, $k$)
       }\If{$m < k -1$ }{
         \Return Get-Order($r$.right, $k - (m+1)$)
       }
        \caption{Get-Order }
\label{alg:kstat}
      \end{algorithm}
%\begin{example}
%
%\end{example}
%\begin{example}
%
%\end{example}
%\begin{example}
%
%\end{example}
%

      \newpage
      \newpage

\section{Appendix - BST Methods Source: A Mixture of Cormen, Wikipedia, and Codeforces}

\begin{algorithm}
\SetAlgoLined
\KwData{$T$ - tree, key - key to search for}
\KwResult{pointer to vertex with key equal to key}
    \If{$T$ is empty}{
        \Return null\;
    }
    \If{$T$.key equals key}{
        \Return $T$\;
    }
    \If{key is less than $T$.key}{
      \Return Search($T$.left, key)\;
    }
    \Else{
      \Return Search($T$.right, key)\;
    }

\caption{Search} 
\end{algorithm}

\begin{algorithm}
\SetAlgoLined
\KwData{$T$ - tree}
\KwResult{pointer to vertex with minimum value in $T$}
    \If{$T$ is empty}{
        \Return null\;
    }
    \If{$T$.left is empty}{
        \Return $T$\;
    }
    \Return Min($T$.left)\;
\caption{Min} 
\end{algorithm}

\begin{algorithm}
\SetAlgoLined
\KwData{$T$ - tree}
\KwResult{pointer to vertex with maximum value in $T$}
    \If{$T$ is empty}{
        \Return null\;
    }
    \If{$T$.right is empty}{
        \Return $T$\;
    }
    \Return Max($T$.right)\;
    \caption{ Max }
\end{algorithm}

\begin{algorithm}
\SetAlgoLined
\KwData{$x$ - vertex}
\KwResult{pointer to predecessor of $x$}
    \If{$x$.left is not empty}{
      \Return Max($x$.left)\;
    }
    $y \leftarrow x$.parent\;
    \While{$y$ is not empty and $x$ is $y$.left}{
        $x \leftarrow y$\;
        $y \leftarrow y$.parent\;
    }
    \Return $y$\;
\caption{Predecessor}
\end{algorithm}

\begin{algorithm}
\SetAlgoLined
\KwData{$x$ - vertex}
\KwResult{pointer to successor of $x$}
    \If{$x$.right is not empty}{
      \Return Min($x$.right)\;
    }
    $y \leftarrow x$.parent\;
    \While{$y$ is not empty and $x$ is $y$.right}{
        $x \leftarrow y$\;
        $y \leftarrow y$.parent\;
    }
    \Return $y$\;
\caption{Successor}
\end{algorithm}

\begin{algorithm}
\SetAlgoLined
\KwData{$T$ - tree, key - key to insert}
\KwResult{inserts key into $T$}
    newNode $\leftarrow$ create new vertex with key $key$\;
    $y \leftarrow$ null\;
    $x \leftarrow T$\;
    \While{$x$ is not empty}{
        $y \leftarrow x$\;
        \If{$key$ is less than $x$.key}{
            $x \leftarrow x$.left\;
        }
        \Else{
          $x \leftarrow x$.right\;
        }
    }
    newNode.parent $\leftarrow y$\;
    \If{$y$ is null}{
        $T \leftarrow$ newNode\;
    }
    \ElseIf{$key$ is less than $y$.key}{
        $y$.left $\leftarrow$ newNode\;
    }
    \Else{
        $y$.right $\leftarrow$ newNode\;
    }
\caption{Insert}
\end{algorithm}

%\begin{algorithm}
%\SetAlgoLined
%\KwData{$T$: The input tree, $x$: The key to be deleted}
%\If{$x$ is in $T$}{
%  Remove $x$ from $T$\;
%}
%\caption{Delete($T$, $x$)}
%\end{algorithm}
%
%2. Inorder($T$):
\begin{algorithm}
\SetAlgoLined
\KwData{$T$: The input tree}
\If{$T$ is not empty}{
  Inorder($T$.left)\;
  Output $T$.key\;
  Inorder($T$.right)\;
}
\caption{Inorder($T$)}
\end{algorithm}


\input{../texlib/tail}



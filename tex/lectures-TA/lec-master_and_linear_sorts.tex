
\input{../texlib/head}
\begin{document}
\ifdefined\BOOK
\else
\setcounter{chapter}{2}
\fi

\chapter{Master Theorem and Linear Time Sorts.} 

\section{Master Theorem, one Theorem to bound them all. }
The master theorem is the result of the recursive expansion. It handles recursive functions at the form of \(T\left(n\right) = a\cdot T\left( \frac{n}{b} \right) + f\left(n\right) \), for positive function \(f : \mathbb{N} \rightarrow \mathbb{R}^{+} \).       

\begin{defbox}{Master Theorem, simple version.} First, Consider the case that \(f = n^c\). Let \( a \ge 1, b > 1\) and \( c \ge 0 \). then: 
\begin{enumerate}
    \item if \(\frac{a}{b^c} < 1 \) then \( T\left(n\right) = \Theta \left( n^c \right) \) \ \ \ \textbf{(\(f\) wins)}.
    \item if \(\frac{a}{b^c} = 1 \) then \( T\left(n\right) = \Theta \left( n^c \log_{b} \left(n\right) \right) \).
    \item if \(\frac{a}{b^c} > 1 \) then \( T\left(n\right) = \Theta \left( n^{\log_{b} \left(a\right)} \right) \) \ \ \ \textbf{(\(f\) loose)}.
  \end{enumerate}
\end{defbox}

\begin{example}\
  \begin{enumerate}
    \item \( T(n)  =4T(\frac{n}{2})+d\cdot n \Rightarrow
      T(n) = \Theta(n^2)\) according to case (3).
    \item \(T(n)  = 3T(\frac{n}{2}) + d\cdot n \Rightarrow T(n) = \Theta ( n^{\log_{2}(3)})\)
      also due to case (3).
  \end{enumerate}
\end{example}
\begin{defbox}{Master Theorem, strong version.} 
Now, let's generalize the simple version for arbitrary positive \(f\) and let~\(a~\ge~1~,~b~>~1\). 

\newcommand{\logab}{\log_{b} \left(a\right)}

\begin{enumerate}
    \item if  \(f\left(n\right) = O \left( n^{\logab - \varepsilon }\right)\) for some \( \varepsilon > 0 \) then \( T\left(n\right) = \theta \left( n^{\logab} \right) \) \ \ \ \textbf{(\(f\) loose)}.
    
    \item if  \(f\left(n\right) = \Theta \left( n^{\logab} \right) \) then \( T\left(n\right) = \Theta \left( n^{\logab}  \log\left(n\right)\right) \)
    
    \item if there exist \(\varepsilon >0 ,c<1\) and \(n_0 \in \mathbb{N} \) such that  \(f\left(n\right) = \Omega \left( n^{\logab + \varepsilon }\right)\) and for every \( n > n_0 \) \(a \cdot f\left( \frac{n}{b} \right) \le c f\left(n\right)\)  then \( T\left(n\right) = \theta \left( f\left(n\right) \right) \) \ \ \ \textbf{(\(f\) wins)}.
    
\end{enumerate}
\end{defbox}
\newcommand{\TT}[2]{#1 T\left(\frac{n}{#2}\right)}

\begin{example}\ 
\begin{enumerate}
    \item \( T(n) =  T(\frac{2n}{3}) + 1 \rightarrow f(n) = 1 =\Theta ( n^{\log_{\frac{3}{2}} (1)})\) matches the second case. i.e  \( T(n) = \Theta ( n^{\log_{\frac{3}{2}} (1)}\log n )\).
    
    \item \( T(n) = \TT{3}{4} + n\log n \rightarrow f(n) = \Omega( n^{\log_{4}(3) + \varepsilon}  ) \) and notice that \( af( \frac{n}{b})~=\frac{3n}{4}\log(\frac{n}{4})\). 

      Thus, it's matching the third case. \(\Rightarrow T(n) = \Theta(n\log n)\).
    
    \item \(T(n) = 3T( n^{\frac{1}{3}}) + \log\log n\). 

      Let $ m = \log n $, thus: $$\Rightarrow T( n) = T (2^m ) = 3T(2^{\frac{m}{3}} ) + \log m $$  
      And denote by $ S = S(m) = T(2^m) $ so we get the recursive form: $$\rightarrow S(m) = 3T(2^{\frac{m}{3}} ) + \log m = 3S(\frac{m}{3} ) + \log m $$ 
      So we can apply the master theorem according to the first case:  \begin{equation*}
        \begin{split}
      \log m &= O(m^{\log_{3}(3)-\varepsilon} ) \\ 
      & \rightarrow T(n) = T(2^m) = S(m) = \Theta(m) = \Theta( \log(n)) 
        \end{split}
      \end{equation*}
\end{enumerate}
\end{example}



\section{Linear Time Sorts (Assumptions over the inputs)}
You will see in the next lecture that any comparison sort, namely a sorting algorithm that assumes nothing about the given input except that its items can be compared, has an \(\Omega(n\log n)\) lower bound on its running time. We will now see how adding assumptions about the input structure might help us bypass the limit and achieve an asymptotically better running time.

\begin{example} Assume that input is permutation over $\{1 ,2 ,3, .., n \}$. For example: \begin{equation*}
    \begin{split}
  A = 5,1,2,3,6,4,7,8
    \end{split}
  \end{equation*}
Write an algorithm that sorts given the assumption. What is its running time?


\textbf{Solution.}  Just return the sequence $1,2,..,n$. Requires $\Theta(n)$ to print the numbers. 
\end{example}
\begin{example} Assume that input is array contains only $0$'s and $1$'s. For example: \begin{equation*}
    \begin{split}
  A = 0,1,1,1,0,0,1,0,1
    \end{split}
  \end{equation*}
Write an algorithm that sorts given the assumption. What is its running time?

\textbf{Solution.} First, count the zeros. Let $x$ be the amount. Then print $0$ $x$ times and print $1$ $n-x$ times. This can be done in $\Theta(n)$ time.
\end{example}



\paragraph{Counting sort.}
Counting sort assumes that each of the $n$ input elements is an \textbf{integer} with a size at most $k$. It runs in $\Theta(n + k)$ time, so when $k = O(n)$, counting sort runs in $\Theta(n)$ time. It first determines, for each input element $x$, the number of elements less than or equal to $x$. Then, it uses this information to place element $x$ directly into its position in the output array. For example, if there are 17 elements less than or equal to $x$, then $x$ will be placed in position 17. However, we need to make some adjustments to this method to handle cases where multiple elements have the \textbf{same value}. We do not want all of them to end up in the same position.

  \begin{algorithm}
  	let B and C be new arrays at size $n$ and $k$ \\ 
  	\For{ $i \in  [0, k]$}{
		$C_{i} \leftarrow 0 $
  	}
	\For{ $j \leftarrow [1, n]$}{
	  $C_{A{j}} \leftarrow C_{A_{j}} + 1 $
 	}
	\For{ $i \in  [1, k]$}{
	  $C_{i} \leftarrow C_{i} + C_{i – 1} $
 	}
	\For{ $j \in [n , 1]$}{
	  $B_{C_{A_{j}}} \leftarrow A_{j}$ \\
	  $C_{A_{j}} \leftarrow C_{A{j}} – 1$ \ \ // to handle duplicate values
	}
	  return $B$
\caption{Counting-sort$(A, n, k)$}
  \end{algorithm}

Notice that the Counting sort can beat the lower bound of $\Omega\left(n \log n\right)$ only because it is not a comparison sort. In fact, no comparisons between input elements occur anywhere in the code. Instead, counting sort uses the actual values of the elements to index into an array.

An important property of the counting sort is that it is \textbf{stable}.

\begin{defbox}{Stable Sort.}
 We will say that a sorting algorithm is stable if elements with the same value appear in the output array in the same order as they do in the input array. \end{defbox}

Counting sort's stability is important for another reason: counting sort is often used as a subroutine in radix sort. As we shall see in the next section, in order for radix sort to work correctly, counting sort must be stable.


\paragraph{Radix sort}
\marginnote[Note 1: That introduction was copied word by word from a web source. Do not use for commercial purposes. ]{Note 1: That introduction was copied word by word from a web source. Do not use for commercial purposes.} Radix sort is the algorithm used by the card-sorting machines you now find only in computer museums. The cards have 80 columns, and in each column, a machine can punch a hole in one of 12 places. The sorter can be mechanically "programmed" to examine a given column of each card in a deck and distribute the card into one of 12 bins depending on which place has been punched. An operator can then gather the cards bin by bin, so that cards with the first place punched are on top of cards with the second place punched, and so on.

The Radix-sort procedure assumes that each element in the array $A$ has d digits, where digit 1 is the lowest-order digit and digit $d$ is the highest-order digit.


%\begin{algbox}{radix-sort(A, n, d)}
  \begin{algorithm}
    \For{ $ i \in [1,d]$ } {
        use a stable sort to sort array $A$ on digit $i$
    }
\caption{radix-sort($A$, $n$, $d$)}
  \end{algorithm}
%\end{algbox}

\paragraph{Correctness Proof.} By induction on the column being sorted.
\begin{itemize}
  \item Base. Where $d = 1$, the correctness follows immediately from the correctness of our base sort subroutine. 
  \item Induction Assumption. Assume that Radix-sort is correct for any array of numbers containing at most $d-1$ digits. 
  \item Step. Let $A^{\prime}$  be the algorithm output. Consider $x,y \in A$. Assume without losing generality that $x > y$. Denote by $x_{d}, y_{d}$ their $d$-digit and by $x_{/d}, y_{/d}$ the numbers obtained by taking only the first  $d-1$ digits of $x,y$. Separate in two cases:

    \begin{itemize}
      \item   If $x_{d} > y_{d}$ then a scenario in which $x$ appear prior to $y$ is  imply contradiction to the correctness of our subroutine.
      \item   So consider the case in which $x_{d} = y_{d}$. In that case, it must hold that $x_{/d} > y_{/d}$. Then the appearance of $x$ prior to $y$ either contradicts the assumption that the base algorithm we have used is stable or that $x$ appears before $y$ at the end of the $d-1$ iteration. Which contradicts the induction assumption. 
    \end{itemize}
 \end{itemize}

The analysis of the running time depends on the stable sort used as the intermediate sorting algorithm. When each digit lies in the range $0$ to $k – 1$ (so that it can take on $k$ possible values), and $k$ is not too large, counting sort is the obvious choice. Each pass over $n$ $d$-digit numbers then takes $\Theta(n + k)$ time. There are $d$ passes, and so the total time for radix sort is $\Theta\left(d(n + k)\right)$.

\paragraph{Bucket sort.} % \ctt{Only if there is time.} }
\marginnote[Note 2:  We will return to analyze the expected (average) running time after the lecture on probability.
]{Note 2: We will return to analyze the expected (average) running time after the lecture on probability.
}Bucket sort divides the interval [0, 1) into n equal-sized subintervals, or buckets, and then distributes the n input numbers into the buckets. Since the inputs are uniformly and independently distributed over [0, 1), we do not expect many numbers to fall into each bucket. To produce the output, we simply sort the numbers in each bucket and then go through the buckets in order, listing the elements in each.

%\begin{algbox}{bucket-sort(A, n)}
  \begin{algorithm}
    	let B[0 : n – 1] be a new array \\
	\For{ $i \leftarrow [0, n – 1]$}{
	   make $B_{i}$ an empty list
       	}
	\For{ $i \leftarrow [1, n]$}{
	    insert $A_{i}$ into list $B_{ \lfloor n A_{i} \rfloor} ]$
       	}
	\For{ $i \leftarrow [0, n – 1]$}{
	    sort list $B_{i}$
       	}
	concatenate the lists $B_{0}, B_{1}, .. , B_{n – 1}$ together and\\
	return the concatenated lists
\caption{bucket-sort($A$, $n$)}
  \end{algorithm}
\input{../texlib/tail}



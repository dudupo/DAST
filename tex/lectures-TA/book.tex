\input{../texlib/head.tex}
\newcommand*{\RECITATION}{}%
\begin{document}
\chapter{Induction and Asymptotic Notations.}
Computer science differs from other scientific disciplines in that it focuses not on solving or making discoveries, but on questioning how good is our current understanding. The fact that one has successfully come up with an idea for a certain problem immediately raises the question of optimality. At the most basic level, we would like to answer what is the 'best' \marginnote[Note 1: text for left-hand side text]{Note 1: text for right-hand side of pages, it is set justified.} program that exists for a particular problem. To do so, we must have a notation that allows us to determine if an algorithm is indeed solving the task, quantify its performance, and compare it to other algorithms. In this chapter, we introduce this basic notation. The chapter is divided into two main parts: the first is about induction, a mathematical technique for proving claims, and the second presents asymptotic notation, which we use to describe the behavior of algorithms over large inputs.
 

% \section{Time Planning.}
% \begin{enumerate}
%     \item Welcome words, general information, listing the recitation topics \& QA. 7\textbf{m} (section 1).
%     \item Explain (remind) induction and presenting the first first example. 10\textbf{m}.
%     \item Second weak induction example 4\textbf{m}.
%     \item Strong induction 7\textbf{m}.
%     \item Introduce, the big O. explain the object shortly, plot graphs. yet still not formal definitions. 7\textbf{m}.  
%     \item Shoot the definitions of the \(O, \Omega, \Theta\) at one. 7\textbf{m}.
%     \item 3\textbf{m} spare + break. 
%     \item Review again the asymptotic notations, with examples from section 4. Q\&A. 7\textbf{m}
%     \item Lemma 21 7\textbf{m}
%     \item Claim 22 7\textbf{m}
%     \item The claim beneath Claim 22 7\textbf{m}
%     \item Example 23 7\textbf{m} (should take more time, the above claims will pay the tax). 
%     \item Logarithmic Rules 7\textbf{m}. 
% \end{enumerate}
% \section{General Course Information.}
% \begin{enumerate}
%     \item Introduce yourself
%     \item course mail: huji.dast.2022a@gmail.com
%     \item targilim scoring: 0.85 \( \cdot \) Test + 0.15 \( \cdot \) Average(\(N - 2\)). 
% \item  introduction to the course: \begin{enumerate}
%     \item  It’s important \& fun: \begin{enumerate}
%         \item We going to learn some data structures: Heaps, Trees, Hash Tables
%         \item And some algorithms: Sorting, Minimal Spanning Tree, Shortest Path
%     \end{enumerate}
%     \item Doing your homework by yourself is the best way to improve your solving problems skill.
% \end{enumerate}
% \end{enumerate}

% \paragraph{Abstract.} Today we will cover induction, infinite series, and asymptotic notation. These tools will come in handy (in the next couple of weeks) when we want to find the runtime complexity of an algorithm, specifically using the ’Master Theorem’, to give asymptotic bounds for recursion relations, and to prove loop invariants using (finite) induction.

\section{Induction.}
%Suppose that a teacher who standing in front of his class is willing to prove that he can reach the door. One obivieus way to do so is to actully reach the door, that's it, move physiclly to her and declere sucsses. For small classes contian small nummer of students that protcol even might be efficient,lasting less than servel secondes. But what if the class is really big one? mabye at the length and wide of a football stadium? In that case, the proving by doing might take time. So the obvious question to ask, what else can we do? is there more efficient way to prove that? 

%Suppose that a teacher, who's standing in front of his class, is willing to prove that he can reach the door at the corner. One obvious way to do so is to actually reach the door, that's it, move physically to it and declare success. For small classes containing a small number of students, this protocol might even be efficient, lasting less than several seconds. But what if the class is really big, maybe the length and width of a football stadium? In that case, the proving by doing might take time. So the obvious question to ask is, what else can we do? Is there a more efficient way to prove this?

Suppose that a teacher, who is standing in front of his class, is willing to prove that he can reach the door at the corner. One obvious way to do so is to actually reach the door; that is, move physically to it and declare success. For small classes containing a small number of students, this protocol might even be efficient, lasting less than several seconds. But what if the class is really big, maybe the length and width of a football stadium? In that case, proving by doing might take time. So the obvious question to ask is, what else can we do? Is there a more efficient way to prove this?

Indeed, there is. Instead of proving that he can reach the door, he can prove that while he do not stand next to the door, nothing can stop him from keeping moving forward. If that is indeed the case, then it's clear that not reaching the door in the end would be a contradiction to being just one step away from it (why?), which, in turn, would also contradict being two steps away from it. Repeating this argument leads to a contradiction for the fact that the teacher was in the classroom at the beginning.

\paragraph{What is induction?}~\begin{enumerate}
    \item A mathematical proof technique. It is essentially used to prove that a property \(P(n)\) holds for every natural number \(n\).
    \item The method of induction requires two cases to be proved:
    \begin{enumerate}
        \item The first case, called the base case, proves that the property holds for the first element.
        \item The second case, called the induction step, proves that if the property holds for one natural number, then it holds for the next natural number.
    \end{enumerate}
    \item The domino metaphor. 
\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \node[anchor=south west,inner sep=0] at (0,0) {\includegraphics[scale=2]{./dommino-256x256}};
    \node (A) at (6.7,5.5) { $P(n-1)$ };
    \node (B) at (9,5.7) { $P(n)$ };
    \node (C) at (7, 2.9) { };
    \node (D) at (7.4, 3.1) { };
    \draw [draw = blue, thick,
         arrows={
          - Stealth[length=8pt,open,bend,sep] %Computer Modern Rightarrow [sep] 
         %- Latex[blue!50,length=8pt,bend,line width=0pt]
         %  Stealth[length=8pt,open,bend,sep]
       }]
    (C) edge [bend right=35] (A)
    (D) edge [bend left=15] (B);
    %\draw[red,ultra thick,rounded corners] (7.5,5.3)  (9.4,6.2);
\end{tikzpicture}
    \caption{ domino line falling.}
  \label{fig:domino}
\end{figure}
\end{enumerate}




\subsection{Two Types of Induction Strategies.}
We keep in mind two types of induction strategies: the first is Weak Induction, which refers to proofs that, in their step stage, only require assuming the correctness of the previous step, i.e. $P(n-1)$, in order to prove correctness for $P(n)$. This differs from the Strong Induction strategy, for which the step stage assumes the correctness of the claim for any value less than $n$, namely assuming that all $P(1), P(2), ..., P(n-1)$ are correct.


\Cref{example:chockstrong} and \Cref{example:sumweak2} demonstrate the use of weak induction to prove the formulas for arithmetic and geometric sums. \Cref{example:sumweak} uses strong induction to determine the number of splits required to separate a chocolate bar.

\begin{example}[Weak induction] 
  \label{example:sumweak}
Prove that $ \forall n \in  \mathbb{N}$: 
  \begin{equation*}
    \begin{split}
  \sum_{i=0}^{n}{i} = \frac{n(n+1)}{2}
    \end{split}
  \end{equation*}
  \begin{proof} By induction.
  \begin{enumerate}
    \item Base. For \(n = 1\), \(\sum_{i=0}^{1}{1} = 1 = \frac{(1+1)\cdot 1}{2} \). 
      \item Assumption. Assume that the claim holds for \(n\). 
      \item Step. 
\begin{equation*}
  \begin{split}
 \sum_{i=0}^{n+1}{i} = & \left( \sum_{i=0}^{n}{i} \right) + n+1 = \frac{n(n+1)}{2} + n + 1 \\
 = & \frac{n(n+1) + 2\cdot (n+1)}{2} = \frac{(n+1)(n+2)}{2} 
  \end{split}
\end{equation*}
  \end{enumerate}
\end{proof}
\end{example}
\begin{example}[Weak induction.] 
  \label{example:sumweak2}
  Let \(q\in \mathbb{R}^{+} / \{1\}\), consider the geometric series \( 1,q,q^2,q^3....q^k\). Prove that the sum of the first \(k\) elements is \begin{equation*}
     1+q+q^2+...+q^{k-1}+q^k = \frac{q^{k+1}-1}{q-1}
\end{equation*}

\begin{proof} By induction. 
  \begin{enumerate}
    \item      Base. For \(n = 1\), we get \( \frac{q^{k+1}-1}{q-1} = \frac{q-1}{q-1} = 1\). 
    \item      Assumption. Assume that the claim holds for an integer \(k\). 
    \item      Step. 
      \begin{equation*}
	\begin{split}
	  1+q+q^2+... & + q^{k-1}+q^k + q^{k+1} =  \frac{q^{k}-1}{q-1} + q^{k+1}  \\
	  &= \frac{q^{k+1}-1 +q^{k+1}\left(q-1\right) }{q-1}  \\ &=\frac{\textcolor{red}{q^{k+1}}-1 +q^{k+2} - \textcolor{red}{q^{k+1}}  }{q-1} \\ &= \frac{q^{k+2}-1}{q-1} 
	\end{split}
      \end{equation*}
  \end{enumerate}
\end{proof}
\end{example}
% \ctt{Finish the induction proof and add alternative proof by counting. I am not sure what is favored, exposing both ways (at the first example) will make clear that induction is only a single proofing tool and surly not the only one. Yet from didactic point of view, it might confuses. }

\begin{example}[Strong induction] 
  \label{example:chockstrong}
  Let there be a chocolate bar that consists of \(n\) square chocolate blocks. Then it takes exactly \(n - 1\) snaps to separate it into the \(n\) squares no matter how we split it.

  \begin{proof} By strong induction. 
    \begin{enumerate}
      \item	Base. For \(n = 1\), it is clear that we need \(0\) snaps. 
      \item	Assumption. Assume correctness for \textbf{every} \(m < n \).
      \item	Step. We have in our hand the given chocolate bar with \(n\) square chocolate blocks. Then we may snap it anywhere we like, to get two new chocolate bars: one with some \( k \in [n]\) chocolate blocks and one with \(n - k\) chocolate blocks. From the induction assumption, we know that it takes \(k - 1\) snaps to separate the first bar, and \(n - k - 1\) snaps for the second one. And to sum them up, we got exactly \[ (k - 1) + (n - k - 1) + 1 = n - 1 \] snaps.
    \end{enumerate}
\end{proof}
\end{example}
\section{Asymptotic Notations.}


\begin{definition}
  Let \( f, g : \mathbb{N} \rightarrow \mathbb{R}^{+} \). We say that \( f = O(g)\)  if there exists $ N \in \mathbb{N}$ and $c > 0$  such that for all $n \ge N$ we have $f(n) \le c \cdot g(n)$. 
\end{definition}
\begin{example}
  For example, if \(f(n) = n + 10 \) and \( g(n) = n^2\)
, then \(f = O(g) \) (Draw the graphs) for \(n \ge 5 \):
\(f(n) = n + 10 \le n + 2n = 3n \le n \cdot n = n^2\)
\end{example}
\begin{example}
  Also if \( f(n) = 5n\) and \(g(n) = n^2\), then \(f(n) = O(g(n))\) 
\end{example}
\begin{figure}[h]
  \label{fig:bigO}
\begin{tikzpicture}
    \draw[->]  (0,0)coordinate(O) -- (11,0) node[anchor=north] {$n$};
    \draw[->]  (0,0) -- (0,5);

    \draw[domain=0:6,smooth,variable=\x,blue,name path=c1] plot ({\x},{0.5*( (0.5*\x)^2+2*sin(\x r)+1)})node[right]{$f_1(n)$};
    \draw[domain=0:11,smooth,variable=\x,red,name path=c2] plot ({\x},{0.5*(0.2*\x+0.5*sin(\x r)+2)})node[right]{$f_2(n)$};
\fill[red,name intersections={of=c1 and c2}]
    (intersection-1) circle (2pt);

\draw[dashed] (intersection-1) -- (intersection-1|-O) node[below]{$N$};
\end{tikzpicture}
\caption{Here $f_{1} (x) \sim  x^{2} + \sin(x)$, and $f_{2} \sim x + \sin(x)$, both defined up to additive and multiplicative terms. So, $f_{1} = \Omega(f_{2})$ and $f_{2} = O(f_{1})$. } 
\end{figure}


\begin{definition} 
Let \( f, g : \mathbb{N} \rightarrow \mathbb{R} \)
We say that \(f = \Omega(g)\) if \(g = O(f)\), equivalently,
there exist $N \in \mathbb{N}$ and $ c > 0$ s.t. $\forall n \ge N \Rightarrow c· g(n) \le f(n)$
\end{definition}

\begin{example}
For example, if \(f(n) = n + 10\) and \(g(n) = n^2\)
, then \(g= \Omega(f)\)
\end{example}

\begin{definition} 
Let $ f, g : \mathbb{N} \rightarrow \mathbb{R}$, We say that $f = \Omega(g)$ if:
there exist $N \in \mathbb{N}$ and $\exists c > 0$ s.t $ \forall n \ge N \Rightarrow f(n) \ge c \cdot g(n)$.
\end{definition}



\begin{definition}
Let \( f, g : \mathbb{N} \rightarrow \mathbb{R} \), We say that \(f = \Theta(g)\) if:
\(f = O(g)\) and \(f = \Omega(g)\)
That is, we say that \(f = \Theta(g)\) if:
\( \exists N \in \mathbb{N}, \exists c_1, c_2 > 0\) s.t. \(\forall n \ge N \ c_1\cdot g(n) \le f(n) \le c_2 \cdot g(n)\)
\end{definition}


\begin{figure}[h]
  \label{fig:bigTheta}
\begin{tikzpicture}
    \draw[->]  (0,0)coordinate(O) -- (11,0) node[anchor=north] {$n$};
    \draw[->]  (0,0) -- (0,5);

    \draw[domain=0:11,smooth,variable=\x,blue,name path=c1] plot ({\x},{0.5*( 0.5*\x+2*sin(\x r)+1)})node[right]{$f_1(n)$};
    \draw[domain=0:11,smooth,variable=\x,red,name path=c2] plot ({\x},{0.5*(0.2*\x+0.5*sin(\x r)+2)})node[right]{$f_2(n)$};
\fill[red,name intersections={of=c1 and c2}]
    (intersection-1) circle (2pt)
    (intersection-2) circle (2pt)
        (intersection-3) circle (2pt) ;
\end{tikzpicture}
\caption{Here both $f_{1}(x)$ and $f_{2}(x)$ behave like $\sim  x+ \sin(x)$ up to additive and multiplicative terms. So, $f_{1} = \Theta(f_{2})$.} 
\end{figure}



\begin{example} For every \(f : \mathbb{N} \rightarrow \mathbb{R}, f(n) = \Theta(f(n))\) \end{example}
\begin{example} If \(p(n) = n^5\) and \(q(n) = 0.5n^5 + n\), then \(p(n) = \Theta(q(n))\)\end{example}
But why is this example true? This next Lemma helps for intuition:
\begin{lemma}
  \( \lim_{n \rightarrow \infty} \frac{f(n)}{g(n)} < \infty \Rightarrow f(n) = O(g(n)) \)
\end{lemma}

\begin{proof} Assume that \(l = \lim_{n \rightarrow \infty} \frac{f(n)}{g(n)} < \infty\). Then for some \(N \in \mathbb{N}\) we have that for all \(n \ge N\):
\( \frac{f(n)}{g(n)} < l + 1 \Rightarrow f(n) < (l + 1) · g(n) \)
Which is exactly what we wanted.
\end{proof}
\section{Examples with proofs.}
% EXAMPLES WITH PROOFS

\begin{claim} \(n = O(2^n)\)
\end{claim}
(This must seem very silly, but even though we have a strong feeling it’s true, we still need to learn how to PROVE it)
\begin{proof} We will prove by induction that \( \forall n \ge 1 \) , \(2^n \ge n\), and that will suffice.
  \begin{enumerate}
    \item      Base. \(n = 1\), so it is clear that: \(n = 1 < 2 = 2^n\)
    \item      Assumption. Assume that \(n < 2^n\) for some \(n\).
    \item      Step. We will prove for \(n + 1\). It holds that:
      \begin{equation*}
	n + 1 < 2^{n} + 1 < 2^n + 2^n = 2^{n+1}
      \end{equation*}
  \end{enumerate}
\end{proof}
\begin{claim}
Let $p(n)$ be a polynomial of degree $d$ and let $q(n)$ be a polynomial
of degree $k$. Then:
\begin{enumerate}
\item $d\leq k\Rightarrow p(n)=O(q(n))$ (set upper bound over the quotient)
\item $d\geq k\Rightarrow p(n)=\Omega(q(n))$ (an exercise)
\item $d=k\Rightarrow p(n)=\Theta(g(n))$ (an exercise)
\end{enumerate}
\end{claim}
\begin{proof}Proof (Of 1) First, let's write down $p(n), g(n)$ explicitly:


% \begin{paragraph}
%     Combinatorial Review, Induction, and Asymptotic Notations. 
% \end{paragraph}
\[
p(n)=\sum_{i=0}^{d}\alpha_{i}n^{i},\ g(n)=\sum_{j=0}^{k}\beta_{j}n^{j}
\]
Now let's manipulate their quotient:
\[
\frac{p(n)}{q(n)}=\frac{\sum_{i=0}^{d}\alpha_{i}n^{i}}{\sum_{j=0}^{k}\beta_{j}n^{j}}=\frac{\sum_{i=0}^{d}\alpha_{i}n^{i}}{\sum_{j=0}^{k}\beta_{j}n^{j}}\cdot\frac{n^{k-1}}{n^{k-1}}=\frac{\sum_{i=0}^{d}\alpha_{i}n^{i-k+1}}{\sum_{j=0}^{k}\beta_{j}n^{j-k+1}}\leq
\]

\[
\leq\frac{\sum_{i=0}^{d}\alpha_{i}}{\beta_{k}} < \infty
\]
And now we can use the lemma that we have proved earlier. 
\end{proof}

\section{Logarithmic Rules.}

Just a quick reminder of logarithmic rules:
\begin{enumerate}
\item $log_{a}x\cdot y=log_{a}x+log_{a}y$
\item $log_{a}\frac{x}{y}=log_{a}x-log_{a}y$
\item $log_{a}x^{m}=m\cdot log_{a}x$
\item Change of basis: $\frac{log_{a}x}{log_{a}y}=log_{y}x$
\end{enumerate}
And so we get that:
\begin{remark}
For every $x,a,b\in\mathbb{R},$ we have that $log_{a}x=\Theta(log_{b}x)$
\end{remark}

\begin{example} Let \(f(n)\) be defined as:
\begin{equation*}
f(n) = \left\{ \begin{array}{rcl}
& f\left( \lfloor  \frac{n}{2} \rfloor \right) + 1 & \mbox{for }  n > 1  \\
& 5 & \mbox{else}  
\end{array}\right.
\end{equation*}
Let’s find an asymptotic upper bound for \(f(n)\). let's guess \( f(n) = O(\log(n)) \).

\begin{proof}  We’ll prove by strong induction that : \(f(n) < c \log(n) - 1\) for \(c =8 \) And that will be enough (why? This implies \(f(n) = O(\log(n))\)).
  \begin{enumerate}
    \item      Base. \(n = 2\). Clearly, \(f(2) = 6 < 8 \)
    \item      Assumption. Assume that for every m < n, this claim holds.
    \item      Step. Then we get:
      \begin{equation*} \begin{split}
	  f(n) &= f\left( \lfloor  \frac{n}{2} \rfloor \right) + 1 \le c\log\left(  \lfloor\frac{n}{2}\rfloor\right)  + 1  \\ & \le c\log\left(n\right) - c\log\left(2\right) + 1 
	  \le c\log\left(n\right) \ \ \ \text{for } c = 8
	\end{split}
      \end{equation*}
  \end{enumerate}
\end{proof}
\end{example}

% \section{Another Example.}


% Now let’s try to analyze something we’re going to encounter in this course: Recursively defined functions (Like Fibonacci’s
% sequence).


% \paragraph{Example 23.} Let \(f(n)\) be defined as:
% \begin{equation*}
% f(n) = \left\{ \begin{array}{rcl}
% & 2f\left( \lfloor  \frac{n}{2} \rfloor \right) + 1 & \mbox{for }  n > 1  \\
% & 1 & \mbox{else}  
% \end{array}\right.
% \end{equation*}
% Let’s find an asymptotic upper bound for \(f(n)\).
% We can unravel this recursion rule, and to make things easier let’s just assume that n is a power of 2. So we get:
%     \begin{equation*}
%     f(n) = 2f\left(\frac{n}{2}\right) + 1 =2\left(2f\left(\frac{n}{4}\right) + 1\right) + 1 = ... = nf(1)+ \left( n -1 \right) = 2n-1
%     \end{equation*}

% Proof. \textbf{(Bad proof)}
% Proof by strong induction. We’ll try to prove using \(N = 1, c = 10\), and we’ll try to show that \(f(n) < c \cdot n\).
% Base: \(n = 1\). Clearly we get that \(f(1) = 1 < 10 = c \cdot 1\)

% Assumption: Assume that for every \(m < n\), this claim holds.
% Step: We get:
% \begin{equation*}
%     f(n) = 2f\left( \lfloor  \frac{n}{2} \rfloor \right) + 1 \le 2c\lfloor  \frac{n}{2} \rfloor  + 1 \le c\cdot n + 1
% \end{equation*}
% But this is not a proof... (why? Didn’t prove hypothesis)


% Proof. \textbf{(Good proof)} We’ll try to prove something a little different. We’ll prove by strong induction that (with the same \(N, c\) ): \(f(n) < c \cdot n - 1\) And that will be enough (why? This implies \(f(n) = O(n)\)).
% Base: \(n = 1\). Clearly, \(f(1) = 1 < 9 = c \cdot 1 - 1\)
% Assumption: Assume that for every m < n, this claim holds.
% Step: Then we get:
% \begin{equation*}
%     f(n) = 2f\left( \lfloor  \frac{n}{2} \rfloor \right) + 1 \le 2\left(c  \frac{n}{2} -1 \right)  + 1 \le c\cdot n - 1
% \end{equation*}

% Remark 24. Note that in the definition of big \(O\) notation (12), the property only needs to hold from some \(N\) and on. In this
% last proof, we chose that \(N\) to be \(N = 1\), but it does not has to be 1.


% \section{SERIES AND CONVERGENCE.}

% \paragraph{Definition 3.} Let \( \{a_n\}^{\infty}_{n = N} \) be a sequence of real number. The sequence of partial sums corresponding to this sequence is \(S_k = \sum^{k}_{n=N}{a_n} \) an which is also a sequence of real numbers, with indices ranging from \(N\) to \(\infty\).
% \paragraph{Definition 4.} Let \( \{a_n\}^{\infty}_{n=N} \) be a sequence of real number. We say that the infinite series \( \sum^{\infty}_{n=N}{a_n} \)  converges when the sequence of partial sums converges, i.e. \(S_{k \rightarrow \infty} \rightarrow L \in \mathbb{R} \). In which case we say \( \sum^{\infty}_{n=N}{a_n} = L \). Otherwise, we say it diverges.

% Remark 5. Note that we can always ’pad’ the series with zeros, and this does not alter its convergence status. So we shall talk about series which their initial index is zero (or one).
% \paragraph{Example 6.} The Harmonic series \( \sum^{\infty}_{n=1}{\frac{1}{n}}\) diverges.

% Proof. Assume by contradiction it doesn’t, then there is a bound \( \lim_{k\rightarrow \infty} S_k = S \in \mathbb{R} \). So \(\lim_{k\rightarrow \infty} S_{2k} = S \in  R \). Thus \( \lim_{k} S_{2k}-S_{k} = 0 \), but \( S_{2k}-S_{k} = \frac{1}{n+1} + ... + \frac{1}{2n} \ge n \cdot \frac{1}{2n} = \frac{1}{2} \)
% , which is a contradiction, as we assumed \( \lim_{k\rightarrow \infty} S_{2k}-S_{k} = 0 \).

% \paragraph{Proposition 7.} If \( \{ S_k \}^{\infty}_{n=0} \) an converges, then \( a_{n\rightarrow\infty} \rightarrow 0 \). Proof. Consider \( \lim_{n\rightarrow\infty} [S_{n} - S{_n-1}]\).

% \paragraph{Example 8.} Let \(a_{n}= q^n\) be a sequence of real numbers, \(q \in \mathbb{R}\).
% \begin{enumerate}
%     \item If \(q < 1\), then \(S_k\) converges.
%     \item If \( q \ge 1\), then \(S_k\) diverges.
% \end{enumerate}
% Proof.
% \begin{enumerate}
%     \item Note that \[(1-q)S_{k} = \left(1-q\right)\left(1 + q + q^2 .. q^k \right) = \left(1 + q + q^2 + .. + q^k - q - q^2 - .. - q^{k+1} \right) = 1 - q^{k+1}   \] therefore \(S_k = \frac{1-q^{k+1}}{1-q} \), which    converges for \(k \rightarrow \infty \). That is \( S_k \rightarrow \frac{1}{1-q} \).
%     \item Exercise.
% \end{enumerate}
% \subsection{Convergence Tests}
% \paragraph{Theorem 9.} (Direct Comparison Test) Let \( \{a_n\}^{\infty}_{n = N} \),\( \{b_n\}^{\infty}_{n = N} \) be real, non-negative sequences, and assume \( \exsits M \in \mathbb{N}\) s.t. \( \forall n \ge M\), \( a_n \le b_n\). Then:
% \begin{enumerate}
%     \item If \(\sum^{\infty}_{n=1}{b_n}\) converges, so does \(\sum^{\infty}_{n=1}{a_n}\).
%     \item If \(\sum^{\infty}_{n=1}{a_n}\) diverges, so does \(\sum^{\infty}_{n=1}{b_n}\).
% \end{enumerate}


% Proof. Exercise (Guidance: Show that the partial sums corresponding to the sequence \(b_n\) are larger than the partial sums
% corr. to \(a_n\), and use a result from Infi about limits).

% \paragraph{Theorem 10.} (Limit comparison Test, generalized) Let \( \{a_n\}^{\infty}_{n = N} \),\( \{b_n\}^{\infty}_{n = N} \) be real, non-negative sequences,
% and assume \( \exsits M \in \mathbb{N}, r > 0, R > 0 \)  s.t \( \forall n \ge M, r \le \frac{a_n}{b_n} \le R\). Then either \( \sum_{n=1}^{\infty}{a_n} \)
% and \( \sum_{n=1}^{\infty}{b_n} \) both converge or they both diverge.
% Proof. Use the previous theorem (direct comparison) with \(rb_n \le a_n \le Rb_n\) along with limit arithmetics.

% \paragraph{Example 11.} \( \sum_{n=1}^{\infty}{\frac{1}{n^2}} \) converges.
% Proof. Note that \( \frac{1}{n^2} \le \frac{1}{n(n-1)} \), so it is enough to prove that the series 
% \( \sum_{n=2}^{\infty}{\frac{1}{n(n-1)}}\) converges by Direct Comparison. Notice that the partial sums are:
% \begin{equation*}
%     S_k = \sum_{n=2}^{k}{\frac{1}{n(n-1)}} = \sum_{n=2}^{k}{\frac{1}{n-1} - \frac{1}{n}} = \\ \left(1 - \frac{1}{2} \right) + \left( \frac{1}{2} - \frac{1}{3} \right) + ... + \left( \frac{1}{k-1} - \frac{1}{k} \right) = \\ 1 - \frac{1}{k}   
% \end{equation*}
% so the sequence of partial sums clearly converges (to 1).
% Proof. (Alternative) Look at the sides of the squares. We could always fit one more line of 2
% k
% squares for elements 2
% k until 2
% k+1 − 1 of the
% series. The total area of the rect angle here is 2.




\newcommand*{\RECITATION}{}%
\newcommand*{\BOOK}{}%


\chapter{Introduction to Algorithms.}
%, Correctness and Efficiency
\ifdefined\RECITATION
\else
Computer science differs from other scientific disciplines in that it focuses not on solving or making discoveries, but on questioning how good is our current understanding. The fact that one has successfully come up with an idea for a certain problem immediately raises the question of optimality. At the most basic level, we would like to answer what is the 'best' \marginnote[Note 1: text for left-hand side text]{Note 1: text for right-hand side of pages, it is set justified.} program that exists for a particular problem. To do so, we must have a notation that allows us to determine if an algorithm is indeed solving the task, quantify its performance, and compare it to other algorithms. In this chapter, we introduce this basic notation. The chapter is divided into two main parts: the first is about induction, a mathematical technique for proving claims, and the second presents asymptotic notation, which we use to describe the behavior of algorithms over large inputs.
\fi
\section{Peaks-Finding.}
\begin{example}[Leading Example.]
Consider an \(n\)-length array $A$ such that~$A_1,A_2,....,A_n~\in~\mathbb{R}$. We will say that $A_{j}$ is a peak (local minimum) if he's greater than his neighbors. Namely, $A_{i} \ge A_{i\pm1}$ if $i\pm 1 \in [n]$. Whenever $i\pm 1$ is not in the range $[n]$, we will define the inequality $A_{i} \ge A_{i\pm 1}$ to hold trivially. For example, for $n=1$, $A_{1}=A_{n}$ is always a peak. Write an algorithm that, given $A$, returns the position of an arbitrary peak.
\end{example}


\begin{example}{Warming up.} \label{example:func} How many peaks do the following arrays contain?
  \begin{enumerate}
    \item $A[i] = 1$   $\forall i \in [n]$
    \item $A[i] = \begin{cases}
        i & i < n/2 \\
        n/2 - i & \text{else}
      \end{cases}$
    \item $A[i] = i $  $\forall i \in [n]$
  \end{enumerate}
\end{example}

\section{Naive solution.}
To better understand the problem, let's first examine a simple solution before proposing a more intriguing one. Consider the algorithm examining each of the items $A_{i}$ one by one.
\begin{algorithm}
\KwResult{returns a peak of \(A_1 ... A_n \in \mathbb{R}^n \)  }
\caption{naive peak-find alg.}
 \For{ \(i \in [n] \) } { 
   \If { $A_{i}$ is a peak }{
        \Return $i$
        }
    } 
\end{algorithm}
\paragraph{Correctness.}
We will say that an algorithm is correct, with respect to a given task, if it computes the task for any input. Let's prove that the above algorithm is doing the job. 

\begin{proof}
%  Assume towards contradiction that there exists an $n$-length array $A$ such that the algorithm peak-find fails to find one of its peaks, in particular, the Alg. either returns $j^{\prime}\in [n]$ such that $A_{j^{\prime}}$ is not a peak or not return at all (never reach line (3)). Let's handle first the case in which retruning indeed occurred. Denote by $j$ the first position of a peak in $A$, and note that if the algorithm gets to line (2) in the $j$th iteration then either it returns $j$ or $A_{j}$ is not a peak.
%  Hence it must hold that $j^{\prime} < j$. But a satisfaction of the condition on line (2) can happen only if $A_{j^{\prime}}$ is a peak, which contradicts the minimality of $j$.
%
%  In the case that no position has been returned, In particular the algorithm didn't return in any of the first $j$ iterations and get's in to iteration number $j+1$, which means that the conidition on line (2) was not satisfied in contradiction to the fact that $A_{j}$ is a peak. 
%
Assume towards contradiction that there exists an $n$-length array $A$ such that the algorithm peak-find fails to find one of its peaks, in particular, the Alg. either returns $j^{\prime}\in [n]$ such that $A_{j^{\prime}}$ is not a peak or does not return at all (never reach line (3)). Let's handle first the case in which returning indeed occurred. Denote by $j$ the first position of a peak in $A$, and note that if the algorithm gets to line (2) in the $j$th iteration then either it returns $j$ or $A_{j}$ is not a peak.
  Hence it must hold that $j^{\prime} < j$. But satisfaction of the condition on line (2) can happen only if $A_{j^{\prime}}$ is a peak, which contradicts the minimality of $j$. In the case that no position has been returned, it follows that the algorithm didn't return in any of the first $j$ iterations and gets to iteration number $j+1$, which means that the condition on line (2) was not satisfied in contradiction to the fact that $A_{j}$ is a peak.
\end{proof}

\paragraph{Running Time.} 
Question, How would you compare the performance of two different algorithms? What will be the running time of the naive peak-find algorithm? On the lecture you will see a well-defined way to treat such questions, but for the sake of getting the general picture, let's assume that we pay for any comparison a quanta of processing time, and in overall, checking if an item in a given position is a peak, cost at most $c\in \mathbb{N}$ time, a constant independent on $n$.

Question, In the worst case scenario, how many local checks does peak-finding do? For the third example in~\Cref{example:func}, the naive algorithm will have to check each item, so the running time adds up to at most $c \cdot n$.

\section{Naive alg. recursive version.}
Now, we will show a recursive version of the navie peak-find algorithm for demonstrating how correctness can be proved by induction. 
\begin{algorithm}
\KwResult{returns a peak of \(A_1 ... A_n \in \mathbb{R}^n \)  }
\caption{naive recursive peak-find alg.}
 \If { $A_{1} \ge A[2]$ or $n=1$ }{
   \Return 1
 }
 \Return 1 + peak-find$(A_{2}, .. A_{n})$
 \end{algorithm}

 \begin{claim} \label{claim:subarray} 
Let $A = A_1, \dots, A_n$ be an array, and $A' = A_2, A_3, \dots, A_n$ be the $n-1$ length array obtained by taking all of $A$'s items except the first. If $A_1 \le A_2$, then any peak of $A'$ is also a peak of $A$.
\end{claim}
\begin{proof} 
Let $A^{\prime}_{j}$ be a peak of $A^{\prime}$. Split into cases upon on the value of $j$. If $n-1 > j > 1$, then $A^{\prime}_{j} \ge A^{\prime}_{j \pm 1}$, but for any $j \in [2, n-2]$ we have $A^{\prime}_{j} = A_{j+1}$ and therefore $A_{j+1} \ge A_{j+1 \pm 1} \Rightarrow A_{j+1}$ is a peak in $A$. If $j^{\prime} = 1$, then $A^{\prime}_{1} > A^{\prime}_{2} \Rightarrow A_{2} \ge A_{3}$ and by combining the assumption that $A_{1} \le A_{2}$ we have that $A_{2} \ge A_{1}, A_{3}$. So $A_{2} = A^{\prime}_{1}$ is also a peak. The last case $j = n-1$ is left as an exercise.
\end{proof}

One can prove a much more general claim by following almost the same argument presented above.

 \begin{claim} \label{claim:subarraymiddle} 
   Let $A = A_1, \dots, A_n$ be an array, and $A' = A_{j+1}, A_{j+2}, \dots, A_n$ be the $n-j$ length array. If $A_j \le A_{j+1}$, then any peak of $A'$ is also a peak of $A$.
\end{claim}
%\section{Induction. (Might not appear in the recitation.)} 
%\ifdefined\RECITATION
%\else
%Suppose that a teacher, who is standing in front of his class, is willing to prove that he can reach the door at the corner. One obvious way to do so is to actually reach the door; that is, move physically to it and declare success. For small classes containing a small number of students, this protocol might even be efficient, lasting less than several seconds. But what if the class is really big, maybe the length and width of a football stadium? In that case, proving by doing might take time. So the obvious question to ask is, what else can we do? Is there a more efficient way to prove this?
%
%Indeed, there is. Instead of proving that he can reach the door, he can prove that while he do not stand next to the door, nothing can stop him from keeping moving forward. If that is indeed the case, then it's clear that not reaching the door in the end would be a contradiction to being just one step away from it (why?), which, in turn, would also contradict being two steps away from it. Repeating this argument leads to a contradiction for the fact that the teacher was in the classroom at the beginning.
%\fi
%\paragraph{What is induction?}~\begin{enumerate}
%    \item A mathematical proof technique. It is essentially used to prove that a property \(P(n)\) holds for every natural number \(n\).
%    \item The method of induction requires two cases to be proved:
%    \begin{enumerate}
%        \item The first case, called the base case, proves that the property holds for the first element.
%        \item The second case, called the induction step, proves that if the property holds for one natural number, then it holds for the next natural number.
%    \end{enumerate}
%    \item The domino metaphor. 
%\end{enumerate}
%\paragraph{The two types of induction, their steps, and why it makes sense} (Strong vs Weak) - Emphasize the change in the induction step.
%\begin{example}[Weak induction] Prove that $ \forall n \in  \mathbb{N}$,$\sum_{i=0}^{n}{i} = \frac{n(n+1)}{2}$.
%\begin{proof} Base: For \(n = 1\), \(\sum_{i=0}^{1}{1} = 1 = \frac{(1+1)\cdot 1}{2} \). Assumption: Assume that the claim holds for \(n\).Step: 
%\begin{equation*}
%  \begin{split}
% \sum_{i=0}^{n+1}{i} = & \left( \sum_{i=0}^{n}{i} \right) + n+1 = \frac{n(n+1)}{2} + n + 1 \\
% = & \frac{n(n+1) + 2\cdot (n+1)}{2} = \frac{(n+1)(n+2)}{2} 
%  \end{split}
%\end{equation*}
%\end{proof}
%\end{example}
%\begin{example}[Weak induction.] Let \(q\in \mathbb{R} / \{1\}\), consider the geometric series \( 1,q,q^2,q^3....q^k...\). Prove that the sum of the first \(k\) elements is \begin{equation*}
%     1+q+q^2+...+q^{k-1}+q^k = \frac{q^{k+1}-1}{q-1}
%\end{equation*}
%
%\begin{proof} Base: For \(n = 1\), we get \( \frac{q^{k+1}-1}{q-1} = \frac{q-1}{q-1} = 1\). 
%Assumption: Assume that the claim holds for \(k\). then:
%Step: 
%\begin{equation*}
%\begin{split}
%    1+q+q^2+...+q^{k-1}+q^k + q^{k+1} &=  \frac{q^{k}-1}{q-1} + q^{k+1}  = \frac{q^{k+1}-1 +q^{k+1}\left(q-1\right) }{q-1} = \\ &\frac{\textcolor{red}{q^{k+1}}-1 +q^{k+2} - \textcolor{red}{q^{k+1}}  }{q-1} = \frac{q^{k+2}-1}{q-1} 
%\end{split}
%\end{equation*}
%\end{proof}
%\end{example}
%
%\begin{example}[Strong induction] 
%  Let there be a chocolate bar that consists of \(n\) square chocolate blocks. Then it takes exactly \(n - 1\) snaps to separate it into the \(n\) squares no matter how we split it.
%
%  \begin{proof} By strong induction. Base: For \(n = 1\), it is clear that we need \(0\) snaps. Assumption: Assume that for \textbf{every} \(m < n \), this claim holds.
%
%
%Step: We have in our hand the given chocolate bar with \(n\) square chocolate blocks. Then we may snap it anywhere we like, to get two new chocolate bars: one with some \( k \in [n]\) chocolate blocks and one with \(n - k\) chocolate blocks. From the induction assumption, we know that it takes \(k - 1\) snaps to separate the first bar, and \(n - k - 1\) snaps for the second one. And to sum them up, we got exactly \[ (k - 1) + (n - k - 1) + 1 = n - 1 \] snaps.
%\end{proof}
%\end{example}

We are ready to prove the correctness of the recursive version by induction using \Cref{claim:subarray}. 

\begin{enumerate}
  \item Base, single element array. Trivial. 
  \item Assumption, Assume that for any $m$-length array, such that $m<n$ the alg returns a peak. 
  \item Step, consider an array $A$ of length $n$. If $A_1$ is a peak, then the algorithm answers affirmatively on the first check, returning $1$ and we are done. If not, namely $A_1 < A_2$, then by using \Cref{claim:subarray} we have that any peak of $A' = A_2, A_3, \dots, A_n$ is also a peak of $A$. The length of $A'$ is $n-1 < n$. Thus, by the induction assumption, the algorithm succeeds in returning on $A'$ a peak which is also a peak of $A$.
\end{enumerate}

\section{An attempt for sophisticated solution.}
We saw that we can find an arbitrary peak at $c\cdot n$ time, which raises the question, can we do better? Do we really have to touch all the elements to find a local maxima? Next, we will see two attempts to catch a peak at logarithmic cost. The first attempt fails to achieve correctness, but analyzing exactly why will guide us on how to come up with both an efficient and correct algorithm.
\begin{algorithm}
\KwResult{returns a peak of \(A_1 ... A_n \in \mathbb{R}^n \)  }
\caption{fail attempt for more sophisticated alg. }
        $ i \leftarrow  \lceil n/2 \rceil $\\
        \If { $A_{i}$ is a peak }{
          \Return $i$
        }
        \Else { 
          \Return $i - 1  + $ find-peak$\left(A_{i},A_{i+1}..A_{n}\right)$
        }
\end{algorithm}

Let's try to 'prove' it.  
\begin{enumerate}
 \item Base, single element array. Trivial. 
  \item Assumption, Assume that for any $m$-length array, such that $m<n$ the alg returns a peak. 
  \item Step. If $A_{n/2}$ is a peak, we're done. What happens if it isn't? Is it still true that any peak of $A_{i},A_{i+1}, \ldots, A_{n}$ is also a peak of $A$? Consider, for example, $A[i] = n - i$.
\end{enumerate}

\section{Sophisticated solution.}
The example above points to the fact that we would like to have a similar claim to \Cref{claim:subarraymiddle} that relates the peaks of the split array to the original one.  
\begin{algorithm}
\KwResult{returns a peak of \(A_1 ... A_n \in \mathbb{R}^n \)  }
\caption{sophisticated alg.}
$ i \leftarrow  \lceil n/2 \rceil $\\
        \If { $A_{i}$ is a peak }{
          \Return $i$
        }
        \ElseIf { $A_{i-1} \le A_{i}$ } { 
          \Return $i + $ find-peak$\left(A_{i+1}..A_{n}\right)$
        }
        \Else { 
          \Return find-peak$\left(A_{1},A_{2},A_{3}..A_{i-1}\right)$
        }
\end{algorithm}
Let's prove correction by induction.
\begin{proof}
\begin{enumerate}
  \item Base, single element array. Trivial. 
  \item Assumption, Assume that for any $m$-length array, such that $m<n$ the alg returns a peak. 
  \item Step, Consider an array $A$ of length $n$. If $A_{\lceil n/2 \rceil}$ is a peak, then the algorithm answers affirmatively on the first check, returning $\lceil n/2 \rceil$ and we are done. If not, then either $A_{\lceil n/2 \rceil} < A_{\lceil n/2 \rceil - 1}$ or $A_{\lceil n/2 \rceil} < A_{\lceil n/2 \rceil + 1}$. We have already handled the first case, that is, using \Cref{claim:subarraymiddle} we have that any peak of $A' = A_{\lceil n/2 \rceil + 1}, A_{\lceil n/2 \rceil + 2}, \dots, A_n$ is also a peak of $A$. The length of $A'$ is $n/2 < n$. So by the induction assumption, in the case where $A_{\lceil n/2 \rceil} < A_{\lceil n/2 \rceil - 1}$ the algorithm returns a peak. In the other case, we have $A_{\lceil n/2 \rceil} < A_{\lceil n/2 \rceil + 1}$ (otherwise $A_{\lceil n/2 \rceil}$ would be a peak). We leave finishing the proof as an exercise.
\end{enumerate}

\end{proof}


What's the running time? Denote by $T\left( n \right)$ an upper bound on the running time. We claim that $T(n) \le c_{1} \log (n) - c_{2}$, let's prove it by induction.
\begin{proof}
     
\begin{enumerate}
  \item Base. For the base case, $n \le 3$ we get that $c_{1} \log(1) + c_{2} =  c_{2}$ on the other hand only a single check made by the algorithm, so indeed the base case holds (Choosing $c_{2} \ge$ the cost of a single check).
  \item Induction Assumption. Assume that for any $m < n$, the algorithm runs in at most $c_{1} \log(m) + c_{2}$ time.
  \item Step. Notice that in the worst case, $\lceil n/2 \rceil$ is not a peak, and the algorithm calls itself recursively immediately after paying $c_{2}$ in the first check. Hence: \begin{equation*}
      \begin{split}    
	T\left(n\right) & \le c_{2} + T\left(n/2\right) \le c_{2} + c_{1} \log\left( \lceil n/2 \rceil \right) + c_{2}\\
	& = c_{1} - c_{1} + c_{2} + c_{1} \log\left( \lceil n/2 \rceil \right) + c_{2}\\
	& = c_{1} \log (2) +  c \log\left( \lceil n/2 \rceil  \right) + 2c_{2} - c_{1} \\
	& = c_{1} \log\left(2 \left(\lceil n/2 \rceil \right) \right) + 2c_{2} - c_{1}
      \end{split}
    \end{equation*}
    So, choosing $c_{1} > c_{2}$ gives $2c_{2} - c_{1} < c_{2}$ and therefore:
    \begin{equation*}
      \begin{split}
	T\left(n\right) \le c_{1} \log \left( n  \right) + c_{2}  
      \end{split}
    \end{equation*}
\end{enumerate}
\end{proof}




% \section{Time Planning.}
% \begin{enumerate}
%     \item Welcome words, general information, listing the recitation topics \& QA. 7\textbf{m} (section 1).
%     \item Explain (remind) induction and presenting the first first example. 10\textbf{m}.
%     \item Second weak induction example 4\textbf{m}.
%     \item Strong induction 7\textbf{m}.
%     \item Introduce, the big O. explain the object shortly, plot graphs. yet still not formal definitions. 7\textbf{m}.  
%     \item Shoot the definitions of the \(O, \Omega, \Theta\) at one. 7\textbf{m}.
%     \item 3\textbf{m} spare + break. 
%     \item Review again the asymptotic notations, with examples from section 4. Q\&A. 7\textbf{m}
%     \item Lemma 21 7\textbf{m}
%     \item Claim 22 7\textbf{m}
%     \item The claim beneath Claim 22 7\textbf{m}
%     \item Example 23 7\textbf{m} (should take more time, the above claims will pay the tax). 
%     \item Logarithmic Rules 7\textbf{m}. 
% \end{enumerate}
% \section{General Course Information.}
% \begin{enumerate}
%     \item Introduce yourself
%     \item course mail: huji.dast.2022a@gmail.com
%     \item targilim scoring: 0.85 \( \cdot \) Test + 0.15 \( \cdot \) Average(\(N - 2\)). 
% \item  introduction to the course: \begin{enumerate}
%     \item  It’s important \& fun: \begin{enumerate}
%         \item We going to learn some data structures: Heaps, Trees, Hash Tables
%         \item And some algorithms: Sorting, Minimal Spanning Tree, Shortest Path
%     \end{enumerate}
%     \item Doing your homework by yourself is the best way to improve your solving problems skill.
% \end{enumerate}
% \end{enumerate}

% \paragraph{Abstract.} Today we will cover induction, infinite series, and asymptotic notation. These tools will come in handy (in the next couple of weeks) when we want to find the runtime complexity of an algorithm, specifically using the ’Master Theorem’, to give asymptotic bounds for recursion relations, and to prove loop invariants using (finite) induction.



\newcommand*{\EXERCISE}{}%
%\input{../tex/texlib/head.tex}


 

\newcommand*{\RECITATION}{}%


\ifdefined\SOLUTION
  \section{DAST. Exercise 0. Solution.}
\else
  \section{DAST. Exercise 0.}
\fi
\subsection{Peaks-Finding.} Recall the logarithmic-time version of find-peak. Prove its correctness. You can continue directly from the point we have reached to in the recitation.

\begin{algorithm}
\KwResult{returns a peak of \(A_1 ... A_n \in \mathbb{R} \)  }
\caption{sophisticated alg.}
$ i \leftarrow  \lceil n/2 \rceil $\\
        \If { $A_{i}$ is a peak }{
          \Return $i$
        }
        \ElseIf { $A_{i-1} \le A_{i}$ } { 
          \Return $i + $ find-peak$\left(A_{i+1}..A_{n}\right)$
        }
        \Else { 
          \Return find-peak$\left(A_{1},A_{2},A_{3}..A_{i-1}\right)$
        }
\end{algorithm}

\ifdefined\SOLUTION
  \paragraph{Solution.} Guideline. For completing the induction step, we have to prove a similar claim to 1.3.2, stated as follows. Let $A = A_{1},A_{2},\dots,A_{n-1},A_{n}$ be a $n$-length array. Suppose that $A_{j-1}>A_{j}$ and define $A^{\prime}$ to be the array obtained by taking the elements $A_{1},A_{2}..A_{j-1}$. Then any peak of $A^{\prime}$ is also a peak of $A$.
\fi


\ifdefined\CHECK 
\paragraph{For checkers}

\begin{enumerate}
  \item Accept any solution that proves the correctness in either of the two splits (left or right), no matter which.
  \item Solutions in which it is not proven that any peak of the subarray is a peak of the original array are incorrect.
\end{enumerate}
\fi

\subsection{Peaks-Finding on the Cycle.} Consider the following variation of the peaks-finding problem. We will define the peaks again to be the local maximas of given array $A$. However instead of thinking on position ${1,2..n}$ as coordinates on the line we will think about them as cordinates on the cycle. Namely $A_{1}$ is the right neighbor of $A_{n}$. 


Show a series of arrays $\left\{ A_{n} \right\}_{n=3}^{\infty}$ for which the recursive version of peak-find always fails. Explain in one line why the naive algorithm is still correct (You don't have to provide a formal correctness proof).


\ifdefined\SOLUTION
  \paragraph{Solution.} Guideline. For any $n \ge 3$ define $A_{n}$ as follow:
\begin{equation*}
  \begin{split}
    A_{i} = \begin{cases}
      n & \text{ if } i = 1 \\
      i - 1 & \text{ else } 
    \end{cases}
  \end{split}
\end{equation*}

So we have that 
\begin{equation*}
  \begin{split}
    A_{\lceil n/2 \rceil - 1 } = \lceil n/2 \rceil - 2 \le A_{\lceil n/2 \rceil } = \lceil n/2 \rceil - 1 \le A_{\lceil n/2 \rceil + 1 } = \lceil n/2 \rceil  
  \end{split}
\end{equation*}
Hence, the algorithm will answer affirmatively on line (5) and will look in recursion on the right half of the array which doesn't contain a peak. Yet the naive version iterates over the whole array, in our case it will check if $A_{1} \ge A_{2},A_{n}$, answer affirmatively and returns 1. In fact, as the global maximum is a peak it's guaranteed that for any array set on the cycle the naive version returns a peak.



\fi

\ifdefined\CHECK 
\paragraph{For checkers}
\begin{enumerate}
  \item Reject solutions that include only a finite number inputs.
  \item In case the students assume that the recursive call compares between $A_n$ and $A_{i+1} = A'_1$, check it with forgiveness and accept the solution. (The original intent was that the recursive algorithm would treat the peaks as if $A$ were on the line, but they still got the idea and deserve full credit).
\end{enumerate}
\fi




\subsection{$k$-range peaks.} In this section we define $k$-range peak to be all the items satisfiey:  
\begin{equation*}
  \begin{split}
    A_{i}= \max_{|i-j| \le k } \left\{ A_{j} \right\} 
  \end{split}
\end{equation*}

Write an algorithm that find a $k$-range peak in at most $k\log(n)$ up to constant factor indepandet on $k$ and $n$. Prove correctness, and bound the running time. 
\ifdefined\SOLUTION
  \paragraph{Solution.} Consider \Cref{alg:krange}. 
Let's prove correction by induction.
\begin{proof}
\begin{enumerate}
  \item Base, for $n \le k$. Trivial. 
  \item Assumption, Assume that for any $m$-length array, such that $m<n$ the alg returns a peak. 
  \item Step, Consider an array $A$ of length $n$. If $A_j$ is a $k$-range peak, then the algorithm answers affirmatively on the first check and we're done. If not, then either $i < j$ or $j < i$. 

    Consider the first case, $i < j \Rightarrow A_{j-k}, A_{j-k-1},..A_{j-1} \le A_j$. Using a variation of Claim 1.3.2 (which we didn't prove, and you had to do so), we have that any $k$-range peak of $A' = A_j, A_{j + 1},.. \dots, A_n$ is also a $k$-range peak of $A$. The length of $A'$ is equal to or less than $n/2 < n$. So, by the induction assumption, in the case where $i < j$, the algorithm returns a $k$-range peak of $A'$, denoted by $l$ its position in $A'$ and notice that its corresponding position in $A$ is $j-1+l$.

    In the other case, we have $j < i$ (otherwise $j = i$ would be a peak). The second case $i > j$ is similar. Again, using a variation of Claim 1.3.2, $A_{j+k},A_{j+k-1},..A_{j+1} \le A_j$ and therefore any $k$-range peak of $A' = A_1,A_2..A_j$ is also a $k$-range peak of $A$. And again, $A'$ has length less than $n/2$ and hence, by the induction assumption, the algorithm returns a $k$-range peak of $A'$ which is also a $k$-range peak of $A$.
\end{enumerate}
\end{proof}

\begin{algorithm}
\KwResult{returns a $k$-range peak of \(A_1 ... A_n \in \mathbb{R} \)  }
\caption{$k$-range peak-find alg.} \label{alg:krange}
        $i \leftarrow  \lceil n/2 \rceil $\\
        $j \leftarrow \arg \max_{|i-j|\le k}{ A_{j} }$ \\
        \If { $A_{j}$ is a $k$-range peak }{
          \Return $j$
        }
        \ElseIf { $i < j$ } { 
          \Return $j -1 + $ find-peak$\left(A_{j},A_{j+1}..A_{n}\right)$
        }
        \Else { 
          \Return find-peak$\left(A_{1},A_{2},A_{3}..A_{j}\right)$
        }
\end{algorithm}

Running time, Observing that in most worst-case scenarios, in any iteration, we observe $\sim 2k+1$ elements and then call one of the recursive calls on an array of size at most $\lceil n /2 \rceil$. Therefore, the running time $T(n)$ in that case obeys the following recursive rule:
\begin{equation*}
  \begin{split}
    T\left( n \right) \le c\cdot k + T\left( \lceil n / 2\rceil  \right)
  \end{split}
\end{equation*}
Thus, if we denote $c^{\prime}=c\cdot k$, we obtain the same recursive formula we solved in the reaction $\Rightarrow T\left( n \right) = \Theta\left( \log n \right)$.

\fi
\ifdefined\CHECK 
\paragraph{For checkers}
\begin{enumerate}
  \item Reject solutions that do not contain a formal correctness proof.
  \item Forgive any solution that does not contain a formal computation of the time complexity.
\end{enumerate}
\fi

\ifdefined\DPEAK

\subsection{Extract $d$ peaks.} Write an algorithm that given $A$ prints $d$ different peaks. The algorithm has to run in time at most $3d\log(n)$ up to constant factor indepandet on $d$ and $n$. In this section you don't have to prove correctness. 

\ifdefined\SOLUTION

 \begin{algorithm}
\KwResult{Given $d$ returns $d$ peaks of \(A_1 ... A_n \in \mathbb{R} \)  }
\caption{sophisticated alg.}
  \label{alg:count}
  \If { $d = 0$ } {
    \Return  $L$
 }
$ i \leftarrow  \lceil n/2 \rceil $\\
        \If { $A_{i}$ is a peak }{
          print $i, A_{i}$
        }
        \ElseIf { $A_{i-1} \le A_{i}$ } { 
          \For { $ j \in $find-peak$\left(d, A_{i},A_{i+1}..A_{n}\right)$ } {
            $ L \leftarrow L \cup i -1 + j $  \\
             $d \leftarrow d -1$
           }
          } 
        \Else { 
          \For { $ j \in $find-peak$\left(d, A_{1},A_{2},A_{3}..A_{i-1}\right)$ } {
            $ L \leftarrow L \cup i -1 + j $  \\
             $d \leftarrow d -1$
          }
        }
        \Return L     
\end{algorithm}

\paragraph{Solution.} Guideline. We suggest \Cref{alg:count}. Here we treat to the register store $d$ in most premtive way, in particular unshered between different level of the recursion. Yet, solutions which assume otherwise are valid. It's clear that exectly $d$ peaks are returned, it's less clear why the running time of the algorithm is $d\log n$. 
\fi
\fi

  
\subsection{2D-peaks.} Let $A$ be a $n\times n$ matrix. We will say that $A_{ij}$ is a peak if: 
\begin{equation*}
  \begin{split}
    A_{ij} \ge A_{i-1,j}, A_{i+1,j}, A_{i,j-1}, A_{i,j+1}
  \end{split}
\end{equation*} 
And again if the indices exceed the matrix frame, we define the inequality as satisfied trivially (For example if $A_{1,1} \ge A_{1,2}, A_{2,1}$ then $A_{1,1}$ is a peak).

Write an algorithm that find a peak in $A$. Solutions that runs in at most $c n\log n $ are sufficient. Prove correctness, and bound the running time. 

\ifdefined\SOLUTION
  \newpage
  \paragraph{Solution.} Let $B$ be a $n$-length array defined as $B_{i} = \max_{j}{A_{i,j}}$.
  \begin{claim}
    \label{claim:maxA}
    For any $l \in [n]$ such that $B_l$ is a peak in $B$, we have that for $k \leftarrow \arg \max_{j} A_{l,j}$, the element $A_{l,k}$ is a peak in $A$.
\end{claim}

\begin{proof}
By definition of $B$:
  \begin{equation*}
    \begin{split}
      A_{lk} = B_{l} & \ge B_{l-1} = \max_{j}{A_{l-1,j}} \ge A_{l-1,k}\\
               B_{l} & \ge  B_{l+1} = \max_{j}{A_{l+1,j}}  \ge A_{l+1,k}\\
               B_{l} & \ge A_{l,k+1}\\
               B_{l} & \ge A_{l,k-1} 
    \end{split}
  \end{equation*}
\end{proof}
Notice that \Cref{claim:maxA} already gives us an $O(n^2)$ time algorithm. First compute $B$ in $O(n^2)$ time, then call find-peak on $B$ to return an index $l$ such that $B_l$ is a peak in $B$, thus it remains to return $\arg_k \max A_{l,k}$ (which can be done in $O(n)$ time). 



We are ready to present the $n\log n$ time algorithm. Instead of computing $B$ at the beginning, we are going to compute $B_{i}$ only when we need to access it. That is, any time that find-peak will compare between two elements of $B$ $B_{i},B_{j}$ we will compute their value. Thus, each access to $B$ costs $O(n)$ time. However, as the running time of the original peak-find is at most $O(\log(n))$, it follows that the total number of accesses is also bounded by $O(\log n)$, so in total the running time is at most $O(n\log n)$. The pseudocode is presented in \Cref{alg:2D}.
\begin{algorithm} 
  \KwResult{returns a peak of \(A_{1,1}, ... A_{n,n} \in \mathbb{R} \)  }
\caption{2D peak-find alg.} \label{alg:2D}
$l \leftarrow $ Call to find-peak($B_{1}, B_{2} .. B_{n}$) \\ 
\ \ when compute $B_{i}$ on every accsses.  \\
$k \leftarrow \arg \max_{k}A_{l,k}$\\
\Return $l, k$  
\end{algorithm}

Correctness. Assuming contradiction, suppose there exists a matrix $A$ on which \Cref{alg:2D} fails to return a peak. \Cref{claim:maxA} guarantees that if $l$ is indeed a peak, then $A_{l,k}$ for $k = \arg \max_{k}A_{l,k}$ is a peak in $B$. Hence, the algorithm fails only if find-peak of fails to return the position of a peak in $B$, In other words, we have found an array $B$ on which find-peak fails. Thus contradicting the correctness of find-peak.
\fi

\ifdefined\CHECK 
\paragraph{For checkers}
\begin{enumerate}
  \item Forgive any incorrect solution. 
\end{enumerate}
\fi


\ifdefined\DPEAK
\ifdefined\SOLUTION


\subsection{2D-peaks of a tensor.} Consider the problem above, but assume that $A$ is the outer product of two positive vectors $u,v \in \mathbb{R}^{+}$, namely $A_{ij} = v_i \times u_j$.

Write an algorithm that returns a peak in at most $c \cdot 2 \log(n)$ time for some constant $c$.


  \paragraph{Solution.} Guideline. First prove the following claim:  

  \begin{claim}
If $A_{ij}$ is a peak of 
\begin{equation*}
  \begin{split}
    A_{1,j}, A_{2,j}, \dots, A_{n,j}
  \end{split}
\end{equation*}
then $A_{i,l}$ is a peak of 
\begin{equation*}
  \begin{split}
    A_{1,l}, A_{2,l}, \dots, A_{n,l}
  \end{split}
\end{equation*}
  \end{claim}
 % \begin{proof}

  % \end{proof} 
Then consider the following algorithm: first look for a peak in $A_{1,1},A_{2,1}..A_{n,1}$, denote its position by $\left\{ 1,l \right\}$. Now, look for a peak in the $l$ column, namely in $A_{1,l},A_{2,l}..A_{n,l}$.
\fi 
\fi




    
\chapter{Correctness - Recitation 2} 
%\author{Correctness proofs and computational complexity. }
%\makechapter 
%\begin{paragraph}
    Proving algorithms correctness is necessary to guarantee that our code computes its goal for every given input. In that recitation, we will examine several algorithms, analyze theirs running time and memory consumption, and prove they are correct.   
%\end{paragraph}


\begin{example}[Leading Example.]
Consider \(n\) numbers \(a_1,a_2,....,a_n \in \mathbb{R}\). Given set \(Q\) of \(|Q|\) queries, such each query \(q \in Q\) is a tuple \( (i,j) \in [n] \times [n] \). Write an algorithm that calculates the \(\max_{i\le k\le j}{a_k} \). 
\end{example}
\section{Correctness And Loop Invariant.}

\paragraph{Correctness.} We will say that an algorithm \( \mathcal{A}\) (an ordered set of operations) computes \( f:D_1 \rightarrow D_2 \) if for every \(x \in D_1\) the following equality holds \(f(x) = \mathcal{A}(x)\). Sometimes when it's obvious what is the goal function \(f\), we will abbreviate and say that \( \mathcal{A}\) is correct.       

Examples of functions \(f\) might be: file saving, summing numbers, or posting a message in the forum.  

\paragraph{Loop Invariant.} Loop Invariant \marginpar[Note 1: text for left-hand side text]{Note 1: text for right-hand side of pages, it is set justified.}is a property that characteristic a loop segment code  and satisfy the following conditions: 
\begin{enumerate}
    \item Initialization. The property holds (even) before the first iteration of the loop.   
    \item Conservation. As long as one performs the loop iterations, the property still holds.
    \item (optional) Termination. Exiting from the loop carrying information.
\end{enumerate}

\begin{example} Before dealing with the hard problem, let us face the naive algorithm to find the maximum of a given array.

\begin{algorithm}
\KwResult{returns the maximum of \(a_1 ... a_n \in \mathbb{R}^n \)  }
\caption{naive maximum alg.}
\Fn{max( $a_{1} .. a_{n}$ )}{
 \For{ \(i \in [n] \) } { 
        \( j \leftarrow 1 \) \\
        \ \\
        \While{ \(j \le [n] \) and \( a_i \ge a_j \) } {
        \( j \leftarrow j + 1 \)    
        }
        \\
        \ \\ 
        \If { \(j = n+1\) }{
        \Return \(a_i\)
        }
    } 
\Return \( \Delta \) \\  
}
\end{algorithm}

\begin{claim} Consider the while loop. The property: \textit{"for every \(j^\prime < j \le n+1 \Rightarrow a_{j^\prime} \le a_i \)"} is a loop invariant that is associated with it. 
\end{claim}
\begin{proof} first, the initialization condition holds, as the at the first iteration \(j=1\) and therefore the property is trivial.
Assume by induction, that for every \(m < j\) the property is correct, and consider the \(j\)-th iteration. If back again to line (5), then it means that \( (j-1) < n\) and \( a_{j-1} \le a_{i} \). Combining the above with the induction assumption yields that \(a_i \ge a_{j-1},a_{j-2}, ... a_{1}\).    
\end{proof}

\paragraph{Correctness Proof.} Split into cases, First if the algorithm return result at line (9), then due to the loop invariant, combining the fact that \( j = n + 1\), it holds that for every \(j^\prime  \le n < j \Rightarrow a_i \ge a_{j^\prime} \)  i.e \(a_i\) is the maximum of \(a_1, .... a_n \). The second case, in which the algorithm returns \( \Delta \) at line number (10) contradicts the fact that \(n\) is finite, and left as an exercise.  the running time is \( O(n^2) \) and the space consumption is \(O(n)\). 

\paragraph{Loop Invariant In The Cleverer Alg.} Consider now the linear time algorithm:

\begin{algorithm}[H]
% \SetAlgoLined
\KwResult{returns the maximum of \(a_1 ... a_n \in \mathbb{R}^n \)  }
\Fn{max( $a_{i} .. a_{j}$) }{
 \ \\ 
 let \(b \leftarrow a_1 \) \\ 
 \ \\ 
 \For{\(i \in [2, n] \) } { 
        \(b \leftarrow \max \left(b, a_i \right) \)
    } 
 return \( b \) 
 }
 \caption{maximum alg.}
\end{algorithm}

What is the Loop Invariant here? \textit{"at the \(i\)-th iteration, \(b = \max{ \{ a_1 ... a_{i-1} \} } \)"}. The proof is almost identical to the naive case.   
\end{example}
\section{Non-Linear Space Complexity Algorithms. }
\paragraph{Sub-Array Maximum.} Consider the leading example; It's easy to write an algorithm that answers the queries at a total time of a \( O\left( |Q| \cdot n \right) \) by answers separately on each query. Can we achieve a better upper bound?



%
%
\begin{algorithm}
  \caption{Sub-Array. \(O(n^2)\) space alg.}
% \SetAlgoLined
\KwResult{print the \( \max{\{ a_i ... a_j \} }\) for each query \((i,j) \in Q \) }
\Fn{max( $a_i ... a_j$ )}{
 let \(A \leftarrow \mathbb{M}^{n\times n} \) \\ 
 \ \\ 
 \For{\(i \in [n] \) } {
    \( A_{i,i} \leftarrow a_i\)
 }
 \ \\
 \For{ \(k \in [1, n]\) }{
    \For{\(i \in [n] \) } {
        \If{ \(i+k \le n\) }{
        \(A_{i,i+k} \leftarrow \max \left(A_{i,i+k-1}, a_{i+k} \right) \)
        }
    } 
}
\ \\
\For { \( q \in Q \) }{
    \(i,j \leftarrow q \) \\
    print \( A_{i,j}\)
}
}
\end{algorithm}
%

\paragraph{Claim.} Consider the outer loop at the \(k\)-th step. The following is a loop invariant: \[for \ every \ k^\prime < k ,\ s.t \ i + k^\prime \le n \Rightarrow A_{i,i+k^\prime} = \max{ \{ a_{i}, a_{i+1}, ... ,a_{i + k^\prime} \} }\]  
\textbf{Proof.} The initialization condition trivially holds, assume by induction that \( A_{i,i+k-1} = \max{\{a_i ... a_{i+k-1}\}}\) at beginning of \( k \) iteration. By the fact that \( \max(x,y,z)= \max(\max(x,y),z) \) we get that
\begin{equation*}
\begin{split}
 \max{\{a_1 ... a_{i + k-1}, a_{i+ k} \}} = \max{\{ \max{ \{ a_1 ... a_{i + k-1} \} }, a_{i+ k} \}} =  \max{\{A_{i,i+k-1}, a_{i+ k} \}}
 \end{split}    
 \end{equation*} And the right term is exactly the value which assigned to \(A_{i,i+k}\) in the end of the\(k\)-th iteration. Thus in the beginning of \( k+1 \) iteration the property is still conserved.

\paragraph{ \(O\left(n\log n\right)\) Space Solution.} Example for \(O\left(n\log n + |Q|\log n\right)\) time and \(O\left(n\log n\right)\) space algorithm. Instead of storing the whole matrix, we store only logarithmic number of rows.   

%
%{}
\begin{algorithm}
% \SetAlgoLined
  \caption{Sub-Array. \(O(n \log n )\) space alg.}
\KwResult{print the \( \max{\{ a_i ... a_j \} }\) for each query \((i,j) \in Q \) }
\Fn{max( $a_i ... a_j$ )}{
 let \(A \leftarrow \mathbb{M}^{n\times \log n} \) \\ 
 \ \\
 \For{\(i \in [n] \) } {
    \( A_{i,1} \leftarrow a_i\)
 }
 \ \\
 \For{ \(k \in [2,4,..,2^m,...,n]\) }{
    \For{\(i \in [n] \) } {
        \If{ \(i+k \le n\) }{
        \(A_{i,k} \leftarrow \max \left(A_{i,\frac{k}{2}},A_{i+\frac{k}{2}, \frac{k}{2}} \right) \)
        }
    } 
}
\ \\
\For { \( q \in Q \) }{
    \(i,j \leftarrow q \) \\
    decompose \(j - i \) into binary representation \(2^{t_1} + 2^{t_2} + .. +2^{t_l}\) \\
    print \( \max { \{ A_{i,2^{t_1}},A_{i+ 2^{t_1}, 2^{t_2} }, ... , A_{i+ 2^{t_1} + 2^{t_2} +.. 2^{t_{l-1}}, 2^{t_l}} \} }\)
}
}
\end{algorithm}
%






\setcounter{chapter}{2}
\chapter{Recursive Analysis.} 
%\author{Master theorem and recursive trees.}
% 
%\begin{paragraph}
%    One of the standard methods to analyze the running time of algorithms is to express recursively the number of operations that are made. In the following recitation, we will review the techniques to handle such formulation (solve or bound).  
%\end{paragraph}
%

\section{Bounding recursive functions by hands.} Our primary tool to handle recursive relation is the Master Theorem, which was proved in the lecture. As we would like to have a more solid grasp, let's return on the calculation in the proof over a specific case. 
Assume that your algorithm analysis has brought the following recursive relation:
    \begin{example}\( T\left(n\right)  = \left\{ \begin{array}{rcl}
& 4T\left(\frac{n}{2}\right)+c\cdot n & \mbox{for }  n > 1  \\
& 1 & \mbox{else}  
\end{array}\right. \). Thus, the running time is given by \begin{equation*}
    \begin{split}
 & T\left(n\right)  = 4T\left(\frac{n}{2}\right)+c\cdot n=  4\cdot4T\left(\frac{n}{4}\right)+4c\cdot\frac{n}{2}+c\cdot n = ... = \\ & \overset{\text{\textcolor{red}{critical}}}{\overbrace{4^{h}T(1)}} + c\cdot n\left(1+\frac{4}{2}+\left(\frac{4}{2}\right)^{2}...+\left(\frac{4}{2}\right)^{h-1}\right) = 4^{h} + c\cdot n\cdot\frac{2^{h}-1}{2-1}
    \end{split}
\end{equation*}
We will call the number of iteration till the stopping condition the recursion height, and we will denote it by \(h\) . What should be the recursion height? \( 2^{h} = n \Rightarrow h =\log\left(n\right) \). So in total we get that the algorithm running time equals \( \Theta\left(n^2\right)\). 
\end{example}
\textbf{Question}, Why is the term \( 4^{h} T(1) \) so critical? Consider the case \(T\left(n\right) =  4T\left(\frac{n}{2}\right) + c \) .One popular mistake is to forget the final term, which yields a linear solution \( \Theta(n)\) (instead of quadric \( \Theta(n^2)\)).   

    \begin{example}\( T\left(n\right)  = \left\{ \begin{array}{rcl}
& 3T\left(\frac{n}{2}\right) + c\cdot n & \mbox{for }  n > 1  \\
& 1 & \mbox{else}  
\end{array}\right. \), and then the expanding yields: 
\begin{equation*}
    \begin{split}
        T\left(n\right) & = 3T\left(\frac{n}{2}\right) + c\cdot n = 3^2 T\left(\frac{n}{2^2}\right) + \frac{3}{2}cn + c\cdot n \\ 
        & =  \overset{\text{\textcolor{red}{critical}}}{\overbrace{3^{h}T(1)}} + cn\left(1 + \frac{3}{2} + \left(\frac{3}{2}\right)^2 + ...  + \left(\frac{3}{2}\right)^{h-1} \right) \\
        & h = \log_{2}\left(n\right) \Rightarrow T\left(n\right) = 3^{h}T(1) + c\cdot \textcolor{red}{n}\cdot \left(\left(\frac{3}{\textcolor{red}{2}}\right)^{\log_{2}{n}}\right) / \left(\frac{3}{2} - 1\right) \\ 
        & = \theta \left( 3^{\log_{2}(n)} \right) =  \theta \left( n^{\log 3} \right)  
    \end{split}
\end{equation*}
where \(n^{\log 3}  \sim n^{1.58} < n^2 \).
\end{example}


\section{Master Theorem, one Theorem to bound them all. }
As you might already notice, the same pattern has been used to bound both algorithms. The master theorem is the result of the recursive expansion. it classifies recursive functions at the form of \(T\left(n\right) = a\cdot T\left( \frac{n}{b} \right) + f\left(n\right) \), for positive function \(f : \mathbb{N} \rightarrow \mathbb{R}^{+} \).       

\begin{defbox}{Master Theorem, simple version.} First, Consider the case that \(f = n^c\). Let \( a \ge 1, b > 1\) and \( c \ge 0 \). then: 
\begin{enumerate}
    \item if \(\frac{a}{b^c} < 1 \) then \( T\left(n\right) = \Theta \left( n^c \right) \) \ \ \ \textbf{(\(f\) wins)}.
    \item if \(\frac{a}{b^c} = 1 \) then \( T\left(n\right) = \Theta \left( n^c \log_{b} \left(n\right) \right) \).
    \item if \(\frac{a}{b^c} > 1 \) then \( T\left(n\right) = \Theta \left( n^{\log_{b} \left(a\right)} \right) \) \ \ \ \textbf{(\(f\) loose)}.
  \end{enumerate}
\end{defbox}

\begin{example}  \( T\left(n\right)  =4T\left(\frac{n}{2}\right)+d\cdot n \Rightarrow
T\left(n\right) = \Theta\left(n^2\right)\) according to case (3). And \(T\left(n\right)  = 3T\left(\frac{n}{2}\right) + d\cdot n \Rightarrow T\left(n\right) = \Theta \left( n^{\log_{2}\left(3\right)}\right)\)
also due to case (3).
\end{example}
\begin{defbox}{Master Theorem, strong version.} 
Now, let's generalize the simple version for arbitrary positive \(f\) and let~\(a~\ge~1~,~b~>~1\). 

\newcommand{\logab}{\log_{b} \left(a\right)}

\begin{enumerate}
    \item if  \(f\left(n\right) = O \left( n^{\logab - \varepsilon }\right)\) for some \( \varepsilon > 0 \) then \( T\left(n\right) = \theta \left( n^{\logab} \right) \) \ \ \ \textbf{(\(f\) loose)}.
    
    \item if  \(f\left(n\right) = \Theta \left( n^{\logab} \right) \) then \( T\left(n\right) = \Theta \left( n^{\logab}  \log\left(n\right)\right) \)
    
    \item if there exist \(\varepsilon >0 ,c<1\) and \(n_0 \in \mathbb{N} \) such that  \(f\left(n\right) = \Omega \left( n^{\logab + \varepsilon }\right)\) and for every \( n > n_0 \) \(a \cdot f\left( \frac{n}{b} \right) \le c f\left(n\right)\)  then \( T\left(n\right) = \theta \left( f\left(n\right) \right) \) \ \ \ \textbf{(\(f\) wins)}.
    
\end{enumerate}
\end{defbox}
\newcommand{\TT}[2]{#1 T\left(\frac{n}{#2}\right)}

\begin{example} 
\begin{enumerate}
    \item \( T\left(n\right) =  T\left(\frac{2n}{3}\right) + 1 \rightarrow f\left(n\right) = 1 =\Theta \left( n^{\log_{\frac{3}{2}} \left(1\right)}\right)\) matches the second case. i.e  \( T\left(n\right) = \Theta \left( n^{\log_{\frac{3}{2}} \left(1\right)}\log n \right)\).
    
    \item \( T\left(n\right) = \TT{3}{4} + n\log n \rightarrow f\left(n\right) = \Omega\left( n^{\log_{4}\left(3\right) + \varepsilon}  \right) \) and notice that \( af\left( \frac{n}{b}\right) = \frac{3n}{4}\log\left(\frac{n}{4}\right)\) . Thus, it's matching to the third case. \(\Rightarrow T\left(n\right) = \Theta\left(n\log n\right)\).
    
    \item \(T\left(n\right) = 3T\left( n^{\frac{1}{3}}\right) + \log\log n\). Let \( m = \log n \Rightarrow T\left( n\right) = T \left(2^m \right) = 3T\left(2^{\frac{m}{3}} \right) + \log m\).  Denote by \(S = S\left(m\right) = T\left(2^m\right) \rightarrow S\left(m\right) = 3T\left(2^{\frac{m}{3}} \right) + \log m = 3S\left(\frac{m}{3} \right) + \log m\). And by the fact that \(\log m = O\left(m^{\log_{3}\left(3\right)-\varepsilon} \right) \rightarrow T\left(n\right) = T\left(2^m\right) = S\left(m\right) = \Theta\left(m\right) = \Theta\left( \log(n)\right) \).  
\end{enumerate}
\end{example}

\section{Recursive trees.}
There are still cases which aren't treated by the \textit{Master Theorem}. For example consider the function \(T\left(n\right) = 2T\left(\frac{n}{2}\right) + n\log n \). Note, that \(f = \Omega\left( n^{\log_{b}(a)} \right) = \Omega\left(n\right)\). Yet for every \( \varepsilon > 0 \Rightarrow f = n\log n = O\left( n^{1+\varepsilon} \right) \) therefore the third case  doesn't hold. How can such cases still be analyzed? 

\paragraph{Recursive trees Recipe}
    \begin{enumerate}
        \item draw the computation tree, and calculate it's height. in our case, \( h = \log n \).
        \item calculate the work which done over node at the \(k\)-th level, and the number of nodes in each level. in our case, there are \(2^k\) nodes and over each we perform \(f(n) = \frac{n}{2^k} \log\left( \frac{n}{2^k}\right)\) operations. 
        \item sum up the work of the \(k\)-th level.
        \item finally, the total time is the summation over all the \( k \in [h]\) levels. 
    \end{enumerate}
applying the above, yields 
\begin{equation*} 
\begin{split} 
T\left(n\right) & =  \sum_{k=1}^{\log{n}}{n\cdot\log \left( \frac{n}{2^k}\right)} = n\sum_{k=1}^{\log{n}}{ \left( \log n - \log 2^k \right) } 
  = n\sum_{k=1}^{\log{n}}{ \left( \log n - k \right) } \\
  & = \Theta \left( n \log^2 \left(n\right)  \right) 
\end{split}
\end{equation*}

\begin{example}
Consider merge sort variation such that instead of splitting the array into two equals parts it's split them into different size arrays. The first one contains \( \frac{n}{10} \) elements while second contains the others \( \frac{9n}{10}\) elements.

%
%{non-equal-merge alg.}
\begin{algorithm}
\SetAlgoLined
\KwResult{returns the sorted permutation of \(x_1 ... x_n \in \mathbb{R}^n \)  }
 \ \\ 
 \If{ \(n \le 10 \) }
    { return bubble-sort \( (x_1 ... x_n)\) } 
 \ \\ 
 
 \Else {
 define \(S_{l} \leftarrow x_1 ... x_{\frac{n}{10}-2}, x_{\frac{n}{10}-1} \) \\
 define \(S_{r} \leftarrow   x_{\frac{n}{10}},x_{\frac{n}{10}+1} ...,x_n \) \\
 \ \\ 
 \( R_l \leftarrow \) non-equal-merge\( \left( S_l \right) \) \\ 
 \( R_r \leftarrow \) non-equal-merge\( \left( S_r \right) \) \\
 \ \\
 return Merge(\(R_l, R_r\))
  
 }
 \caption{non-equal-merge alg.} 
\label{alg:unmerge}
\end{algorithm}
%
Note, that the master theorem achieves an upper bound, 
\begin{equation*}
    \begin{split}
    T\left(n\right) & = n +  T\left(\frac{n}{10}\right) + T\left(\frac{9n}{10}\right) \le n +  2 T\left(\frac{9n}{10}\right) \\
     \Rightarrow  T\left(n\right) & = O \left( n^{\log_{\frac{10}{9}}\left(2\right)} \right) \sim O \left( n^{ 6 } \right)  
    \end{split}
\end{equation*}
Yet, that bound is far from been tight. Let's try to count the operations for each node. Let's try another direction. 

\begin{claim}
Let \(n_i\) be the size of the subset which is processed at the \(i\)-th node. Then for every \(k\):
\begin{equation*}
    \sum_{i \in \text{k level}}{n_i} \le n
\end{equation*}
\end{claim}
\begin{proof} Assuming otherwise implies that there exist index \(j\) such that \(x_j\) appear in at least two different nodes in the same level, denote them by \(u,v\). As they both are in the same level, non of them can be ancestor of the other. denote by \(m \in \mathbb{N}\) the input size of the sub array which is processed by the the lowest common ancestor of \(u\) and \(v\), and by \(j^\prime \in [m]\) the position of \(x_j\) in that sub array. By the definition of the algorithm it steams that \(j^\prime < \frac{m}{10} \) and \(j^\prime \ge \frac{m}{10}\). contradiction.  The height of the tree is bounded by \( \log_{\frac{9}{10}} \left(n\right) \). Therefore the total work equals \( \Theta \left( n\log n \right) \). Thus, the total running time equals to:
\begin{equation*}
    T(n) = \sum_{k \in \text{levels}}{\sum_{i \in \text{k level}}{f\left(n_i\right)}} = \sum_{k \in \text{levels}}{\sum_{i \in \text{k level}}{n_i}} \le n\log n  
\end{equation*}
\end{proof}
\end{example}

\begin{figure} 
  
\scalebox{1.2}{ 

\begin{tikzpicture}[level distance=3cm,
  level 1/.style={sibling distance=12cm},
  level 2/.style={sibling distance=6cm}]
 \node [rectangle, minimum width=1.2cm, minimum height=0.2cm, draw] (root) {};
 \node [rectangle, minimum width=0.4cm, minimum height=0.2cm, below left   of=root,   xshift=-1.5cm,  draw] (left1)  {};
 \node [rectangle, minimum width=0.8cm, minimum height=0.2cm, below right  of=root,   xshift=1.5cm,   draw] (right1) {};
 \node [rectangle, minimum width=0.2cm, minimum height=0.2cm, below left   of=left1,  xshift=-0.5cm,  draw] (left2)  {};
 \node [rectangle, minimum width=0.2cm, minimum height=0.2cm, below right  of=left1,  xshift=0.5cm,   draw] (right2) {};
 \node [rectangle, minimum width=0.4cm, minimum height=0.2cm, below left   of=right1, xshift=-0.5cm,  draw] (left3)  {};
 \node [rectangle, minimum width=0.4cm, minimum height=0.2cm, below right  of=right1, xshift=0.5cm,   draw] (right3) {};
 \node [rectangle, minimum width=0.1cm, minimum height=0.2cm, below left   of=left2,  xshift=-0.05cm, draw] (left4)  {};
 \node [rectangle, minimum width=0.1cm, minimum height=0.2cm, below right  of=left2,  xshift=0.05cm,  draw] (right4) {};
 \node [rectangle, minimum width=0.1cm, minimum height=0.2cm, below left   of=right2, xshift=-0.05cm, draw] (left5)  {};
 \node [rectangle, minimum width=0.1cm, minimum height=0.2cm, below right  of=right2, xshift=0.05cm,  draw] (right5) {};
 \node [rectangle, minimum width=0.2cm, minimum height=0.2cm, below left   of=left3,  xshift=-0.1cm,  draw] (left6)  {};
 \node [rectangle, minimum width=0.2cm, minimum height=0.2cm, below right  of=left3,  xshift=0.1cm,   draw] (right6) {};
 \node [rectangle, minimum width=0.2cm, minimum height=0.2cm, below left   of=right3, xshift=-0.1cm,  draw] (left7)  {};
 \node [rectangle, minimum width=0.2cm, minimum height=0.2cm, below right  of=right3, xshift=0.1cm,   draw] (right7) {}; 
  \draw (root) -- (left1);
  \draw (root) -- (right1);
  \draw (left1) -- (left2);
  \draw (left1) -- (right2);
  \draw (right1) -- (left3);
  \draw (right1) -- (right3);
  \draw (left2) -- (left4);
  \draw (left2) -- (right4);
  \draw (right2) -- (left5);
  \draw (right2) -- (right5);
  \draw (left3) -- (left6);
  \draw (left3) -- (right6);
  \draw (right3) -- (left7);
  \draw (right3) -- (right7);
\end{tikzpicture}
}
\caption{ 
The tree matches the recursive calls made by \Cref{alg:unmerge}. Each node presents a rectangle with a length equal to the array given as input to the recursive call. The length of all the elements in a single level is equal to the original array length, thus we have that the linear work in each level sums up to $\Theta(n)$.
}
\end{figure}
\ifdefined\BOOK


\section{Appendix. Recursive Functions In Computer Science. (Beyond the scope of the 2024 course.)}


%\ctt{The current section repeats over part of the content above as it was designed to be self-contained. Also, notice that this part is considered as optional material and you are not required to remember the following algorithms for the final exam. Its primary goal is to expose you to "strange" running times. }


\begin{example}[Leading Example. numbers multiplication.]
Let \(x,y\) be an \(n\)'th digits numbers over \( \mathbb{F}^{n}_{2} \). It's known that summing such a pair requires a linear number of operations. Write an algorithm that calculates the multiplication \(x\cdot y\). 
\end{example}
\begin{example}[Long multiplication.]
To understand the real power of the dividing and conquer method, let's first examine the known solution from elementary school.  In that technics, we calculate the power order and the value of the digit separately and sum up the results at the end. Formally: \(x \leftarrow \sum_{i=0}^{n}{x_{i}2^{i}}\) Thus, \[ x\cdot y =\left( \sum_{i=0}^{n}{x_{i}2^{i}} \right) \left( \sum_{i=0}^{n}{y_{i}2^{i}} \right) =  \sum_{i,j \in [n]\times[n] }{ x_{i}y_{j}2^{i+j} }\] the above is a sum up over \(n^2\) numbers, each at length \(n\) and therefore the total running time of the algorithm is bounded by \( \theta(n^3) \). \ctt{ notice that adding \(1\) to \(1111111111...1\) requires \(O(n)\) }.
\end{example}
\begin{example}[Recursive Approach.] We could split \(x\) into the pair \(x_{l}, x_{r}\) such that \(x = x_{l} + 2^{\frac{n}{2}}x_{r} \). Then the multiplication of two \(n\)-long numbers will be reduced to sum up over multiplication of a quartet. Each at length \(\frac{n}{2}\). Thus, the running time is given by \begin{equation*}
    \begin{split}
 x\cdot y & = \left(x_{l} + 2^{\frac{n}{2}}x_{r}\right)\left(y_{l} + 2^{\frac{n}{2}}y_{r}\right) = x_{l}y_{l} + 2^{\frac{n}{2}} \left( x_{l}y_{r} + x_{r}y_{l} \right) + 2^{n}x_{r}y_{r} \\ &  \Rightarrow T\left(n\right)  =4T\left(\frac{n}{2}\right)+c\cdot n=4\cdot4T\left(\frac{n}{4}\right)+4c\cdot\frac{n}{2}+c\cdot n = ... = \\ & c\cdot n\left(1+\frac{4}{2}+\left(\frac{4}{2}\right)^{2}...+\left(\frac{4}{2}\right)^{h-1}\right) + 4^{h}T(1) = n^{2} + c\cdot n\cdot\frac{2^{h}-1}{2-1}
    \end{split}
\end{equation*}
We will call the number of iteration till the stopping condition the recursion height, and we will denote it by \(h\) . What should be the recursion height? \( 2^{h} = n \Rightarrow h =\log\left(n\right) \). So in total we get that multiplication could be achieved by performs \( \Theta\left(n^2\right)\) operations. 
\end{example}
\paragraph{Karatsuba algorithm.}
%Many years it was believed that multiplication can't done by less then $\Omega\left(n^2\right)$ time; until Karatsuba found the following algorithm \cite{Karatsuba1963MultiplicationOM}. Let $z_{0},z_{1}z_{3}$ defined as follow: \begin{equation*}
It was once thought that multiplication could not be done in less than $\Omega\left(n^2\right)$ time; however, Karatsuba discovered an algorithm \cite{Karatsuba1963MultiplicationOM} that proved this wrong. Let $z_{0},z_{1}z_{3}$ be defined as follows:
\begin{equation*}
z_0, z_1, z_2 \leftarrow x_{l}y_{r}, x_{l}y_{r} + x_{r}y_{l}, x_{r}y_{r}
\end{equation*}Notice that \( z_1 = \left(x_{l}+x_{r}\right)\left(y_{l}+y_{r}\right) - z_{0} -z_{1} \). summarize the above yields the following pseudo code. 

%
%{Karatsuba alg.}
\begin{algorithm}
\SetAlgoLined
\KwResult{returns the multiplication \(x\cdot y\) where \(x,y \in \mathbb{F}^{n}_{2}\) }
 \ \\ 
 \If{ \(x,y \in \mathbb{F}_{2}\) }
    { return \(x \cdot y\) } 
 \ \\ 
 
 \Else {
 define \(x_{l} , x_{r} \leftarrow x \) and \(y_{l} , y_{r} \leftarrow x \) \ \ \ \ \ // \( O \left(n\right) \). \\ 
 \ \\ 
 calculate \(z_0 \leftarrow \text{Karatsuba}\left(x_{l},y_{l}\right)\) \\
 \ \ \ \ \ \ \ \ \ \ \ \ \(z_2 \leftarrow \text{Karatsuba}\left(x_{r},y_{r}\right)\) \\ 
 \ \ \ \ \ \ \ \ \ \ \ \ \(z_1 \leftarrow \text{Karatsuba}\left(x_{r} + x_{l} ,y_{l} + y_{r} \right) - z_0 - z_2 \) \\ 
 \ \\
 return \(z_0 + 2^{\frac{n}{2}}z_1 + 2^{n}z_2\) \ \ \ \ \  // \( O \left(n\right) \). 
 }
\end{algorithm}
%
Let's analyze the running time of the algorithm above, assume that \(n = 2^{m}\) and then the recursive relation is 
\begin{equation*}
    \begin{split}
        T\left(n\right) & = 3T\left(\frac{n}{2}\right) + c\cdot n = 3^2 T\left(\frac{n}{2^2}\right) + \frac{3}{2}cn + c\cdot n = cn\left(1 + \frac{3}{2} + \left(\frac{3}{2}\right)^2 + ...  + \left(\frac{3}{2}\right)^{h-1} \right) + ) + 3^{h}T(1) \\
        & h = \log_{2}\left(n\right) \Rightarrow T\left(n\right) = n^{\log_{2}{3}} +  c\cdot \textcolor{red}{n}\cdot \left(\left(\frac{3}{\textcolor{red}{2}}\right)^{\log_{2}{n}}\right) / \left(\frac{3}{2} - 1\right) = \theta \left( 3^{\log_{2}(n)} \right) =  \theta \left( n^{\log 3} \right)  
    \end{split}
\end{equation*}
where \(n^{\log 3}  \sim n^{1.58} < n^2 \).

\fi





%\newcommand*{\EXERCISE}{}%
%\input{../tex/texlib/head.tex}


 

%\newcommand*{\RECITATION}{}%


\setcounter{chapter}{2}
\ifdefined\SOLUTION
  \section{DAST. Exercise 2. Solution.}
\else
  \section{DAST. Exercise 2.}
\fi

\ifdefined\CHECK

\paragraph{Checking Prioritiy.} (3.3) and (3.6) are more important than (3.5), so if there is no time, (3.5) can be skipped.


\paragraph{}

\fi


\ctt{ 
  Please note that sections 1, 2, and 4 are optional and do not need to be submitted. However, please ensure that you feel capable of solving them. The problems in the following chapter, 7, 8, and 9, are also optional but are more challenging. It is completely acceptable if you are not able to manage them at this stage of the course.
 }

 \subsection{Small $o,\omega$, True or False? (Not a Mandatory). }
 Let $f,g,h,k : \mathbb{N} \rightarrow \mathbb{R}^{+}$. Either prove or provide a counterexample:
\begin{enumerate}
  \item $f = o(g) \Leftrightarrow g = \omega(f)$. 
  \item $f = O(g) \Rightarrow f = o(g)$. 
  \item If $h = o(f)$ and $k = o(g)$ then $h/k = o(f/g)$.  
\end{enumerate}

\ifdefined\SOLUTION
\fi

\subsection{Logaritmic Arithmetic Recap. (Not a Mandatory).} 
 Prove that for any $x,y > 0$:
\begin{enumerate}
  \item we have $x>y$ if and only if $\log x > \log y$. 
  \item $x^{\log_{y}n} = n^{\log_{y}x}$ For any $n > 0$. 
  \item $\log (x \pm y) = \log (x) + \log (1 \pm y/x)$. 
\end{enumerate}

\ifdefined\SOLUTION

\fi

\subsection{Merge Sort Correction.}
\begin{enumerate}
\item Consider the Merge subroutine used in the Merge-Sort algorithm. Let $A$ and $B$ be the given input arrays, and let $C$ be the output array returned by the subroutine. State, in your own words, a claim referring to the structure of $C$ at the $i$th iteration. Prove it.
\item Prove the correctness of Merge using your claim.
\end{enumerate}
\ifdefined\SOLUTION
  \paragraph{Solution.}  
  \begin{enumerate}
    \item \begin{claim}
        Assuming the given arrays $A$ and $B$ to \Cref{alg:merge} were sorted then in the end of the $k$th iteration it's hold that $C_{1}\le C_{2} .. C_{k}$. 
      \end{claim}
      \begin{proof}
        By induction.\begin{enumerate}
          \item Base. For $k=1$, the statement holds trivially.
          \item Assumption. Assume that for any $k^{\prime} < k$ the claim is correct. 
          \item Step. Assume, by contradiction, that after the $k$th iteration, the statement does not hold. Then, there must be an $l$ such that $C_{l} > C_{l+1}$. Since in the $k$th iteration, we only set $C_{k}$, either $C_{k-1} > C_{k}$ or in the $k-1$th iteration, $C$ was also not sorted, which contradicts the assumption. Therefore, we are left with only the first option, namely $l = k-1$. Let $j$ and $i$ be the values of the $j$ and $i$ registers at the $k-1$th iteration. Without loss of generality, assume that $C_{k-1}$ was set to be $A_{i}$, thus $A_{i} \le B_{j}$. Additionally, since it is given that $A$ is a sorted array, $A_{i} \le A_{i+1}$. On the other hand, $C_{k}$, by definition, is set to be $C_{k} \leftarrow \min \{ A_{i+1}, B_{j} \}$ and therefore, it has to be greater than $A_{i} = C_{k}$.
        \end{enumerate}
      \end{proof}
    \item The correctness of the claim guarantees that after $k$ iterations, $C$ is sorted and contains $k$ elements. It remains to be shown that if we reach the $|A|+|B|$ iteration, then $C$ contains all the elements in $A$ and $B$. Assume, by contradiction, that there is an element in $A$, denoted by $A_{j}$, that is not in $C$. By the fact that the length of $C$ equals $|A| + |B|$, there must be an element either in $A$ or $B$ that was chosen twice to be placed in $C$. Without loss of generality, let's suppose that it was $A_{j^{\prime}}$, so line number (5) was executed twice for $i = j^{\prime}$. But that contradicts the fact that we proceeded to line number (6), which increases $i$ (and it's the only line that changes $i$'s value).

      
Notice that solutions in which the claim stated in the first section is "after the $k$th iteration, $C$ contains the $k$ smallest elements of $A\cup B$" obtain the correctness of merge directly.
\end{enumerate}

\begin{algorithm}
  \KwResult{Merges two sorted arrays $A$ and $B$.}
\caption{merge alg.} \label{alg:merge}
         Let $C$ be array at length $|A| + |B|$ \\ 
         Let $i, j \leftarrow 0, 0$ \\ 
         \For{ $k \in [|C|]$}{
           \If { $A_{i} \le B_{j}$ }{
             $C_{k} \leftarrow A_{i}$ \\
             $i \leftarrow i + 1$
           }
           \Else{
             $C_{k} \leftarrow B_{j}$ \\
             $j \leftarrow j + 1$
           }
         }
         \Return $C$  
\end{algorithm}


\ifdefined\CHECK

  \paragraph{Check.} Please penalize any solution that does not state a correct claim that refers to the $i$th iteration.

\fi


\fi

\subsection{Master Theorem. (Not a Mandatory).}
Bound the following recursive functions. In each case, assume that $T(1)=T(0)=~1$.
\begin{enumerate}
  \item $T(n) = 7T(n/3) + n^{3}$
  \item $T(n) = T(\sqrt{n}) + 2^n$ 
\end{enumerate}

\ifdefined\SOLUTION
  \paragraph{Solution.} 
\begin{enumerate}
 \item $T(n) = 7T(n/3) + n^{3}$. So, $a = 7$ and $b = 3$, therefore $\frac{a}{b^{c}} < 1$ and thus $T(n) = \Theta(n^{3})$.
\item $T(n) = T(\sqrt{n}) + 2^{n}$. Let $S(m) = T(2^{m}) = S(m/2) + 2^{2^{m}}$. It is clear that $f(n) = \Omega(n^{\log_{2}1} + \varepsilon)$ and $2^{2^{m/2}} = f(m/2) < \frac{1}{2}f(m) = 2^{2^{m}-1}$. Therefore, $S(m) = \Theta(2^{2^{m}}) \Rightarrow T(n) = \Theta(2^{n})$.
\end{enumerate}
\fi 

\subsection{$\beta$-Root Master Theorem Varition.}
Consider the following recursice ralation: 
\begin{equation*}
    T\left(n\right)  = \left\{ \begin{array}{rcl}
        & \alpha T(n^{\frac{1}{\beta}}) + \log^{\gamma} \left( n \right)  & \mbox{for }  n > 1  \\
      & 1 & \mbox{else}  
  \end{array} \right. 
\end{equation*}
When $\alpha, \beta > 1 $ and $\gamma \ge 0$ . Bound $T$ asymptotically tight, namely find $g: \mathbb{N} \rightarrow \mathbb{R}^{+}$ such that $T = \Theta(g)$. Notice that the solution depends on the relation between $\alpha, \beta, \gamma$.
% $\alpha/\beta^{c} > 1$

\ifdefined\SOLUTION
  \paragraph{Solution.}
  Define $S : \mathbb{N} \rightarrow \mathbb{R}^{+}$ to be the function, $S(m) = T(2^{m})$, ths the recursive equation expands to:
  \begin{equation*}
    \begin{split}
      S(m) & = T(2^{m}) = \alpha T(2^{m/\beta}) + \log^{\gamma}(2^{m}) \\
      &= \alpha S(\frac{m}{\beta}) + m^{\gamma}
    \end{split}
  \end{equation*}
  Thus, using the master theorem, we have that: 
  \begin{enumerate}
    \item $ \frac{\alpha}{\beta^{\gamma}} > 1 $ then $S(m) = \Theta(m^{\log_{\beta}\alpha})$ and therfore $T(n) = \Theta( \log^{\log_{\beta}\alpha} n)$. 
    \item  $ \frac{\alpha}{\beta^{\gamma}} = 1 $ then $S(m) = \Theta(m^{\log_{\beta}\alpha})\log m$ and therfore $T(n) = \Theta( \log^{\log_{\beta}\alpha} n \log \log n)$. 
    \item $ \frac{\alpha}{\beta^{\gamma}} < 1 $ then $S(m) = \Theta(m^{\gamma})$ and therfore $T(n) = \Theta( \log^{\gamma} n)$. 
  \end{enumerate}

\ifdefined\CHECK
  \paragraph{Check.} Please penalize any solution that does not contain a correct return to $n$-representation.
\fi

\fi 


\subsection{Recursive Trees.}
Let $\alpha \in (0,1)$, $l\ge 2$ and let $T: \mathbb{N}\rightarrow\mathbb{R}^{+}$ definied as follow:
\begin{equation*}
  \begin{split}
    T\left(n\right)  = \left\{ \begin{array}{rcl}
        & n^{l} + T\left( \alpha n \right) + T\left( (1-\alpha) n \right)  & \mbox{for }  n > 1  \\
      & 1 & \mbox{else}  
  \end{array} \right. 
  \end{split}
\end{equation*}
Bound $T$ asymptotically tight. 
\ifdefined\SOLUTION
  \paragraph{Solution.}  The length of the subarray that is obtained after $i$ left-turns and $j$ right-turns is $\alpha^{i}(1-\alpha)^{j}n$. Therefore, the work associated with these vertices in the recursive tree representation of the running is $(\alpha^{i}(1-\alpha)^{j}n)^{l}$. It can be observed that on the $k$th level, the sum of $i$ and $j$ is equal to $k$, and there are exactly ${k \choose i}$ such vertices. Denote by $h$ a parameter that bounds the recursive height. Then we get that:
  \begin{equation*}
    \begin{split}
      & \sum_{k=0}^{h}{\sum_{i=0}^{k}{ { k \choose i  } \left( \alpha^{i}(1-\alpha)^{k-i} n \right)^{l}} } =  n^{l}\sum_{k=0}^{h}{ \left( \alpha^{l} + (1-\alpha)^{l} \right)^{k} } \\ 
        & \le n^{l}\sum_{k=0}^{\infty}{ \left( \alpha^{2} + (1-\alpha)^{2} \right)^{k} } \\ 
        & \le n^{l} \cdot O(1)
    \end{split}
  \end{equation*}
When the last transition is due to the fact that $\alpha \in (0,1)$, it follows that $\alpha^{2} < \alpha$ and $(1-\alpha)^{2} < 1 - \alpha$. Therefore, in total, $\alpha^{2} + (1-\alpha)^{2} < 1$. On the other hand, the work that has been done is at least $n^{l}$, since in the root of the tree we pay $n^{l}$ regardless of the recursive calls $\Rightarrow T(n) = \Theta(n^{l})$. 

\ifdefined\CHECK
  \paragraph{Check.} Please give a penalty for any solution that does not contain two-sided bounding, namely treating both the upper and lower bounds. Also, notice that any solution that does not rely on the fact that $l\ge 2$ cannot work. 
\fi


\fi
\section*{Extra Questions. (Not a Mandatory).}

\subsection{$T_{1},T_{2}$-Running time. (Not a Mandatory).}
Let $a,b>0$ and $T_{1},T_{2} : \mathbb{N} \rightarrow \mathbb{R}^{+}$ be functions satisfying the equations:
\begin{equation*}
  \begin{split}
    T_{1}\left( n \right) = aT_{1}\left(\frac{n}{2} \right) + bT_{2}\left(\frac{n}{2} \right) + n^{3}\\ 
    T_{2}\left( n \right) = bT_{1}\left(\frac{n}{2} \right) + aT_{2}\left(\frac{n}{2} \right) + n^{2} 
  \end{split}
\end{equation*}
for any $n>1$, and equal to $1$ otherwise. Bound $T_{1}, T_{2}$ asymptotically tight. \textbf{Hint:} Use your linear algebra arsenal.
% $\alpha/\beta^{c} > 1$

\ifdefined\BOOK
\ifdefined\SOLUTION
  \paragraph{Solution.} \textbf{Guideline.} Denote by $A$ the matrix:  
 \begin{equation*}
   \begin{split}
     A = \begin{bmatrix}
        3 & 2 \\
        2 & 3 
\end{bmatrix}
   \end{split}
 \end{equation*}
 And observes that one can write the recurive equations at matrices form:
 \begin{equation*}
   \begin{split}
     \begin{bmatrix}
       T_{1} \\
       T_{2} 
     \end{bmatrix} (n) & = A \begin{bmatrix}
       T_{1} \\
       T_{2} 
     \end{bmatrix} (n/2)  + \begin{bmatrix}
       n^{3} \\
       n^{2}
     \end{bmatrix} \\ 
& = A \left( A  \begin{bmatrix}
       T_{1} \\
       T_{2} 
     \end{bmatrix} (n/4)  + \begin{bmatrix}
       \left(n/2\right)^{3} \\
       \left(n/2\right)^{2}
   \end{bmatrix} \right) + \begin{bmatrix}
       n^{3} \\
       n^{2}
     \end{bmatrix} \\
     & = A^{\log n} \begin{bmatrix}
       T_{1} \\
       T_{2} 
     \end{bmatrix}(1) +\sum_{i}^{\log n}{ A^{i} \begin{bmatrix}
         \left(\frac{n}{2^{i+1}}\right)^{3} \\
         \left(\frac{n}{2^{i+1}}\right)^{2} 
   \end{bmatrix}} \\ 
     & = A^{\log n} \begin{bmatrix}
       T_{1} \\
       T_{2} 
     \end{bmatrix}(1) +\sum_{i}^{\log n}{ A^{i} \left( \frac{1}{2}\begin{bmatrix}
       \left(\frac{n}{2^{i+1}}\right)^{3} +  \left(\frac{n}{2^{i+1}}\right)^{2}\\
       \left(\frac{n}{2^{i+1}}\right)^{3} +  \left(\frac{n}{2^{i+1}}\right)^{2} 
     \end{bmatrix}+ \frac{1}{2}\begin{bmatrix}
       \left(\frac{n}{2^{i+1}}\right)^{3} -  \left(\frac{n}{2^{i+1}}\right)^{2}\\
        \left(\frac{n}{2^{i+1}}\right)^{2} -\left(\frac{n}{2^{i+1}}\right)^{3} 
    \end{bmatrix} \right)} \\ 
    & = \frac{5^{\log n}}{2} \begin{bmatrix}
      T_{1} + T_{2}\\
      T_{1} + T_{2} 
     \end{bmatrix}(1) + \frac{1}{2} \begin{bmatrix}
       T_{1} - T_{2}\\
       T_{2} -T_{1}
     \end{bmatrix}(1) \\ 
     & \ \ \ \ +\sum_{i}^{\log n}{  \frac{5^{i}}{2}\begin{bmatrix}
       \left(\frac{n}{2^{i+1}}\right)^{3} +  \left(\frac{n}{2^{i+1}}\right)^{2}\\
       \left(\frac{n}{2^{i+1}}\right)^{3} +  \left(\frac{n}{2^{i+1}}\right)^{2} 
     \end{bmatrix}+ \frac{1}{2}\begin{bmatrix}
       \left(\frac{n}{2^{i+1}}\right)^{3} -  \left(\frac{n}{2^{i+1}}\right)^{2}\\
        \left(\frac{n}{2^{i+1}}\right)^{2} -\left(\frac{n}{2^{i+1}}\right)^{3} 
    \end{bmatrix}} \\ 
   \end{split}
 \end{equation*}
\fi 
\fi 

  \subsection{Tensor-calculus-program running time analysis. (Not a Mandatory).}
Let $\mathcal{A}_{1},\mathcal{A}_{2},\mathcal{A}_{3},...,\mathcal{A}_{m}$ be algorithms, where each of their source codes is at most $N$ bits long. These algorithms are restricted to the following format: any operation they perform is either iterating over a $k$-nested loop and performing a finite amount of work in each iteration, or calling one of the other algorithms on an input of length $n/2$. Additionally, it is assumed that any $\mathcal{A}_{i}$ can be called by $\mathcal{A}_{j}$ a finite number of times, independent of $N$ and $m$. To clarify, calls to other algorithms cannot be made inside one of the for loops.


Write an algortihm that given the sources of $\mathcal{A}_{1},\mathcal{A}_{2},\mathcal{A}_{3},...,\mathcal{A}_{m}$ bound assymptoticly tight the running time of $\mathcal{A}_{1}$ at time $\Theta\left( N\cdot m + m^{3} \right)$. 
% $\alpha/\beta^{c} > 1$


\subsection{2D-Peaks in linear time. (Not a Mandatory).} Write an algorithm that, given a matrix $M \in \mathbb{R}^{n\times n}$, returns a local peak in linear time. Prove correctness.

\ifdefined\SOLUTION

\fi 
\ifdefined\Book
\fi




\ifdefined\BOOK
\else
\setcounter{chapter}{4}
\fi
\chapter{Recap Recitation.} 



\section{More Sorting, More Correctness.}
Until now, all the algorithms that we have seen were, in some sense, intuitive. We could describe in words, step by step, exactly what the algorithm does. For example, bubble and heapsort both bubble up the greatest element among the remaining elements in each iteration. Merge sort divides the task into subtasks on smaller inputs, starting with sorting the first and second halves of the given array, and then merging the sorted subarrays.

We are about to present another $\Theta(n^{2})$-sorting algorithm, whose correctness is not so obvious. The algorithm was developed by Stanley P. Y. Fung, \cite{Simplesort}, who coined its name - "ICan'tBelieveItCanSort" - due to the surprise of having such a simple sorting algorithm. It's worth mentioning that, despite its simplicity, Fung came up with this algorithm in 2021.


\begin{algorithm}
\SetAlgoLined
\KwResult{Sorting $A_{1},A_{2},..A_{n}$ }
\caption{ "ICan'tBelieveItCanSort"  alg.}
\For{ $ i \in [n]$} {
  \For{ $ j \in [n]$} {
    \If { $A_{i} < A_{j} $} {
      swap $A_{i} \leftrightarrow A_{j}$
    }
  }
}


\end{algorithm}

%\begin{claim}
%  \label{claim:first}
%  After the $i$th iteration, $A_{1} \le A_{2} \le A_{3} .. \le A_{i}$ and $A_{i}$ is the maximum of the whole array. 
%\end{claim}
%\begin{proof}
%  By induction on the iteration number $i$. 
%  \begin{enumerate}
%    \item Base. For $i=1$, it is clear that when $j$ reaches the position of the maximal element, an exchange will occur and $A_{1}$ will be set to be the maximal element. Thus, the condition on line (3) will not be satisfied until the end of the inner loop and indeed, we have that $A_{1}$ at the end of the first iteration is the maximum.
%    \item Assumption. Assume the correctness of the claim for any $i^{\prime} < i$. 
%    \item Step. Consider the $i$th iteration. And observes that if $A_{i} = A_{i-1}$ then $A_{i}$ is also the maximal elememnt in $A$, namely no exchange will be made in $i$th iteration, yet $A_{1} \le A_{2} \le .. \le A_{i-1}$ by the induction assumption, thus  $A_{1} \le A_{2} \le .. \le A_{i-1} \le A_{i}$ and $A_{i}$ is the maximal element, so the claim holds in the end of the iteration. 
%      If $A_{i} < A_{i-1}$ then there exists $k \in [1,i-1]$ such $A_{k} > A_{i}$. Set $k$ to be the minimal position for which the inequlity holds. For Convinet denote by $A^{(j)}$ the array in the begging of the $j$th iteration of the inner loop. And let's split to cases according to $j$ value. 
%      \begin{enumerate}
%        \item $j < k$ By definition of $k$, for any $j < k, A^{(1)}_{j} < A^{(1)}_{i}$, Hence in the first $k-1$ iteration no exchange will be made and we can conclude that $A^{(j)}_{l} = A^{(1)}_{l}$ for any $l \in [n]$ and $j \le k$. 
%        \item $j \ge k$ and $j<i+1$, We claim that for each such $j$ an exhange will always occuer. 
%          \begin{claim}
%            For any $j \in [k,i]$ we have that in the end of the $j$th iteration:  
%            \begin{itemize}
%              \item $A^{(j+1)}_{j} = A^{(j)}_{i}$.
%              \item $A^{(j+1)}_{i} = A^{(j)}_{j} = A^{(1)}_{j}$.
%              \item For any $l > j$ and $l \neq i$ we have $A^{(j+1)}_{l} = A^{(1)}_{l}$.
%            \end{itemize}
%          \end{claim}
%          \begin{proof}
%            Observes that the thired section holds trivally by the defination of the algorithm, it doesn't toach any position greater than $j$ in the first $j$ iterations (inner loop) except the $i$th position. So have to prove only the first two bullrts, And again we are going to prove them by induction.  
%            \begin{enumerate}
%              \item Base. $A^{(1)}_{k}$ is greater than $A_{i}$, and by the above case we have that at the begging of the $k$ iteration $A^{(k)}_{k}=A^{(1)}_{k}, A^{(k)}_{i}=A^{(1)}_{k}$. Therefore the condition on line (3) is satisfied, exchange is been made, and $A^{(k+1)}_{k} =A^{(k)}_{i} = A^{(0)}_{i}$ and $A^{(k+1)}_{i} = A^{(k)}_{k}$. Now  So $A^{(k+1)}_{k+1} = A^{(k)}_{k+1} = A^{(0)}_{k+1}$
%              \item Assumption. Assume the correctness of the claim for any $k\ge j^{\prime} < j \le i$. 
%              \item Step. Consider the $j \in (k,i]$ iteration, By the induction assumption we have that $A^{(j)}_{j-1} = A^{(j-1)}_{i}$ and $A^{(j)}_{i} = A^{(j-1)}_{j-1} = A^{(1)}_{j-1}$. On the otherhand, by the induction assumption of \Cref{claim:first}, $j-1 < i \Rightarrow A^{(1)}_{j-1} \le A^{(1)}_{j}$. Combining the thired bullet we obtain that:                
%                \begin{equation*}
%                  \begin{split}
%                    A^{(j)}_{j} = A^{(1)}_{j} \ge A^{(1)}_{j-1} = A^{(j)}_{i}
%                  \end{split}
%                \end{equation*}
%                And therefore, either there is inequalitiy and exhange be made or there is equality, in both cases after the $i$th iteration we have $A^{(j+1)}_{j} = A^{(j)}_{i}$ and $A^{(j+1)}_{i} = A^{(j)}_{j} = A^{(1)}_{j}$.
%            \end{enumerate}
%          \meend{proof}
%        \item $j > i$, So we know that $A^{(i+1)}_{i}$ is the maximal element in $A$, therfore for any $j$ it holds that $A^{(i+1)}_{i}\ge A^{(i)}_{j}$, it follows that no exchange would been made and $A^{(j)}_{i}$ will remain the maximum till the end of the inner loop.    
%      \end{enumerate}
%  \end{enumerate}
%\end{proof}

\begin{claim}
  \label{claim:first}
  After the $i$th iteration, $A_{1} \le A_{2} \le A_{3} .. \le A_{i}$ and $A_{i}$ is the maximum of the whole array. 
\end{claim}
\begin{proof}
  By induction on the iteration number $i$. 
  \begin{enumerate}
    \item Base. For $i=1$, it is clear that when $j$ reaches the position of the maximum element, an exchange will occur and $A_{1}$ will be set to be the maximum element. Thus, the condition on line (3) will not be satisfied for the remaining $j$-iterations of the inner loop. Therefore, at the end of the first iteration, $A_{1}$ is indeed the maximum.
    \item Assumption. Assume the correctness of the claim for any $i^{\prime} < i$. 
    \item Step. Consider the $i$th iteration. And observe that if $A_{i} = A_{i-1}$ then $A_{i}$ is also the maximal element in $A$, namely no exchange will be made in the $i$th iteration, yet $A_{1} \le A_{2} \le .. \le A_{i-1}$ by the induction assumption, thus  $A_{1} \le A_{2} \le .. \le A_{i-1} \le A_{i}$ and $A_{i}$ is the maximal element, so the claim holds in the end of the iteration. 
      If $A_{i} < A_{i-1}$ then there exists $k \in [1,i-1]$ such that $A_{k} > A_{i}$. Set $k$ to be the minimal position for which the inequality holds. For convenience, denote by $A^{(j)}$ the array in the beginning of the $j$th iteration of the inner loop. And let's split into cases according to $j$ value. 
      \begin{enumerate}
        \item $j < k$ By definition of $k$, for any $j < k, A^{(1)}_{j} < A^{(1)}_{i}$, Hence in the first $k-1$ iterations no exchange will be made and we can conclude that $A^{(j)}_{l} = A^{(1)}_{l}$ for any $l \in [n]$ and $j \le k$. 
        \item $j \ge k$ and $j\le i$, We claim that for each such $j$ an exchange will always occur. (The proof is given below.)
          \begin{claim}
            \label{claim:second}
            For any $j \in [k,i]$ we have that in the end of the $j$th iteration:  
            \begin{itemize}
              \item $A^{(j+1)}_{j} = A^{(j)}_{i}$.
              \item $A^{(j+1)}_{i} = A^{(j)}_{j} = A^{(1)}_{j}$.
              \item For any $l > j$ and $l \neq i$ we have $A^{(j+1)}_{l} = A^{(1)}_{l}$.
            \end{itemize}
          \end{claim}

        \item $j > i$, so we know that $A^{(i+1)}_{i}$ is the maximal element in $A$. Therefore, for any $j$, it holds that $A^{(i+1)}_{i}\ge A^{(i)}_{j}$. It follows that no exchange would be made and $A^{(j)}_{i}$ will remain the maximum til the end of the inner loop. Thus for any $j >i$:   
          \begin{equation*}
            \begin{split}
              A^{(j)}_{i}=A^{(j-1)}_{i}=.. =A^{(i+2)}_{i}=A^{(i+1)}_{i}=A^{(i)}_{i-1}=A^{(0)}_{i-1}=\max A
            \end{split}
          \end{equation*}
          And 
          \begin{equation*}
            \begin{split}
             & A^{(j)}_{1}, A^{(j)}_{2}, .. A^{(j)}_{k-1}, A^{(j)}_{k}, A^{(j)}_{k+1}, .. A^{(j)}_{i-1}, A^{(j)}_{i}, A^{(j)}_{i+1} , A^{(j)}_{i+2} , A^{(j)}_{i+3} .. \\
            = & A^{(0)}_{1}, A^{(0)}_{2}, .. A^{(0)}_{k-1}, A^{(0)}_{i}, A^{(0)}_{k}, .. A^{(0)}_{i-2}, A^{(0)}_{i-1}, A^{(0)}_{i+1} , A^{(0)}_{i+2} , A^{(0)}_{i+3} .. 
            \end{split}
          \end{equation*}
          In particular, for $j = n+1$ (Note that there is no $n+1$th iteration). Clearly, the inequalities are satisfied and we are done.
\end{enumerate}
\end{enumerate}
  \end{proof}
  \begin{proof}[Proof of \Cref{claim:second}.]
  Observe that the third section holds trivially by the definition of the algorithm. It doesn't touch any position greater than $j$ in the first $j$ iterations (inner loop) except the $i$th position. So we have to prove only the first two bullets. Again, we are going to prove them by induction on $j$.  
  \begin{enumerate}
    \item Base. $A^{(1)}_{k}$ is greater than $A_{i}$, and by the above case, we have that at the beginning of the $k$th iteration $A^{(k)}_{k}=A^{(1)}_{k}, A^{(k)}_{i}=A^{(1)}_{k}$. Therefore, the condition on line (3) is satisfied, an exchange is made, and $A^{(k+1)}_{k} =A^{(k)}_{i} = A^{(1)}_{i}$ and $A^{(k+1)}_{i} = A^{(k)}_{k}$. %Now, $A^{(k+1)}_{k+1} = A^{(k)}_{k+1} = A^{(0)}_{k+1}$.
    \item Assumption. Assume the correctness of the claim for any $k \le j^{\prime} < j \le i$. 
    \item Step. Consider the $j \in (k,i]$ iteration. By the induction assumption, we have that $A^{(j)}_{j-1} = A^{(j-1)}_{i}$ and $A^{(j)}_{i} = A^{(j-1)}_{j-1} = A^{(1)}_{j-1}$. On the other hand, by the induction assumption of \Cref{claim:first}, $j-1 < i \Rightarrow A^{(1)}_{j-1} \le A^{(1)}_{j}$. Combining the third bullet, we obtain that:                
      \begin{equation*}
        \begin{split}
          A^{(j)}_{j} = A^{(1)}_{j} \ge A^{(1)}_{j-1} = A^{(j)}_{i}
        \end{split}
      \end{equation*}
      And therefore, either there is an inequality and an exchange is made or there is equality. In both cases, after the $j$th iteration, we have $A^{(j+1)}_{j} = A^{(j)}_{i}$ and $A^{(j+1)}_{i} = A^{(j)}_{j} = A^{(1)}_{j}$.
  \end{enumerate}
\end{proof}

%\section{Master Theorem.} Let $\alpha > 2, \beta > 0, \gamma > 0$ satisfing $2^{\alpha} < \beta$, defining the following running time function:    
%\begin{equation*}
%  \begin{split}
%    T\left( n \right) = \beta T \left( n - \alpha \right) + n^{\gamma}
%  \end{split}
%\end{equation*}
%Bound $T$ asymptoticly tight.
%\paragraph{Solution.} Let define $S(m) = T(\log m)$. Thus the recursive relation expand to: 
%\begin{equation*}
%  \begin{split}
%    S\left( m \right) = T\left( \log m \right) &= \beta T \left( \log m - \alpha \right) + \log^{\gamma}(m) \\
%    &=  \beta T \left( \log m - \log {2^\alpha} \right) +  \log^{\gamma} (m) \\ 
%    &= \beta T \left( \log \left( \frac{m}{2^{\alpha}} \right) \right) + \log^{\gamma}(m) = \beta S(\frac{m}{2^{\alpha}}) + \log^{\gamma}(m) 
%  \end{split}
%\end{equation*}
%Observes that $2^{\alpha} > 1 $ and therefore we can use the generalized master theorem. By the given that $2^{\alpha} < \beta$ we have that $m^{\log_{2^\alpha}(\beta)} $ is (generalized) polynom with positive degree. And therefore there is a positvie $\varepsilon$ such that $\log^{\gamma}(m) = O\left( m^{\log_{2^\alpha}(\beta) -\varepsilon} \right)$. Hence by the master theorem, we conclude that: 
%
%\begin{equation*}
%  \begin{split}    
%    S(m) = \Theta \left(m^{\log_{2^\alpha}(\beta)} \right) \Rightarrow T(n) = \Theta\left( 2^{m \log_{2^\alpha}(\beta)} \right)
%  \end{split}
%\end{equation*}
\section{Master Theorem}
Let $\alpha > 2, \beta > 0, \gamma > 0$ satisfying $2^{\alpha} < \beta$, define the following running time function:
\begin{equation*}
  \begin{split}
    T(n) = \beta T(n - \alpha) + n^{\gamma}
  \end{split}
\end{equation*}
Bound $T$ asymptotically tight.

\paragraph{Solution}
Let us define $S(m) = T(\log m)$. Thus, the recursive relation expands to:
\begin{equation*}
  \begin{split}
    S(m) = T(\log m) &= \beta T(\log m - \alpha) + \log^{\gamma}(m) \\
    &= \beta T(\log m - \log {2^\alpha}) + \log^{\gamma}(m) \\ 
    &= \beta T(\log \left( \frac{m}{2^{\alpha}} \right)) + \log^{\gamma}(m) = \beta S(\frac{m}{2^{\alpha}}) + \log^{\gamma}(m) 
  \end{split}
\end{equation*}
We observe that $2^{\alpha} > 1$ and therefore we can use the generalized master theorem. Given that $2^{\alpha} < \beta$, we have that $m^{\log_{2^\alpha}(\beta)}$ is a (generalized) polynomial with positive degree. Hence, there exists a positive $\varepsilon$ such that $\log^{\gamma}(m) = O(m^{\log_{2^\alpha}(\beta) - \varepsilon})$. Therefore, by the master theorem, we conclude that:
\begin{equation*}
  \begin{split}    
    S(m) = \Theta(m^{\log_{2^\alpha}(\beta)}) \Rightarrow T(n) = \Theta(2^{n \log_{2^\alpha}(\beta)})
  \end{split}
\end{equation*}
%\author{Master theorem and recursive trees.}
% 
%\begin{paragraph}
%    One of the standard methods to analyze the running time of algorithms is to express recursively the number of operations that are made. In the following recitation, we will review the techniques to handle such formulation (solve or bound).  
%\end{paragraph}
%
%\begin{algorithm}
%\SetAlgoLined
%\KwResult{returns the multiplication \(x\cdot y\) where \(x,y \in \mathbb{F}^{n}_{2}\) }
% \ \\ 
% \If{ \(x,y \in \mathbb{F}_{2}\) }
%    { return \(x \cdot y\) } 
% \ \\ 
% 
% \Else {
% define \(x_{l} , x_{r} \leftarrow x \) and \(y_{l} , y_{r} \leftarrow x \) \ \ \ \ \ // \( O \left(n\right) \). \\ 
% \ \\ 
% calculate \(z_0 \leftarrow \text{Karatsuba}\left(x_{l},y_{l}\right)\) \\
% \ \ \ \ \ \ \ \ \ \ \ \ \(z_2 \leftarrow \text{Karatsuba}\left(x_{r},y_{r}\right)\) \\ 
% \ \ \ \ \ \ \ \ \ \ \ \ \(z_1 \leftarrow \text{Karatsuba}\left(x_{r} + x_{l} ,y_{l} + y_{r} \right) - z_0 - z_2 \) \\ 
% \ \\
% return \(z_0 + 2^{\frac{n}{2}}z_1 + 2^{n}z_2\) \ \ \ \ \  // \( O \left(n\right) \). 
% }
%\end{algorithm}






\ifdefined\BOOK
\else
\setcounter{chapter}{5}
\fi

\chapter{Linear Time Sorts and Lower Bounds in the Comparison Model.} 
\section{Heapsort (David's group).}
We will start by introducing the heap-sort algorithm and providing a proof of its correctness.

\begin{algorithm}
    $A \leftarrow$ Build-Heap($A$)\\
  	\For{ $i \in  [n]$}{
      swap $A_{1} \leftrightarrow A_{n-i+1}$\\
      heapsize($A$) $\leftarrow n-i$\\
      heapify($A$, 1)
  	}	
	  return $A$
\caption{Heap-sort$(A)$}
  \end{algorithm}
Correctness. We are going to prove the following statement.
  \begin{claim}   
At the end of the $i$th iteration, $A_{n-i+1},A_{n-i+2},..A_{n}$ are the $i$ largest elements of $A$ placed in order, and $A_{1},A_{2},..A_{n-i}$ is a maximum heap.
  \end{claim}
\begin{proof}
By induction.
\begin{enumerate}
  \item Base. $A_{n}$ is set in line (3) to be the root of the heap, and therefore is the maximum of $A$. After line (4), $A_{1}$ is the parent of the heap's roots as line (4) excludes $A_{n}$ from the heap (So the heap inequality holds for any $j\in [2,(n-1)/2]$). Therefore, by the correctness of heapify, we get that after line (5), $A_{1},A_{2},..A_{n-1}$ is a heap.
  \item Assumption. Assume the correctness of the claim for any $i^{\prime}<i$.
  \item Step. Consider the $i$th iteration. By the induction assumption, $A_{1}$ is a root of the heap $A_{1},A_{2},..A_{n-i+1}$ and therefore is their maximum. So after the swapping in line (3), we get that $A_{n-i+1}$ is the element which is greater than $n-i$ elements in $A$. By using the induction assumption again, we know that it is also less than $A_{n-i+2},A_{n-i+3},..A_{n}$, so after line (3) and by the fact that $A_{n-i+2},A_{n-i+3},..A_{n}$ are the $i-1$ largest elements placed in order, we have that $A_{n-i+1},A_{n-i+2},A_{n-i+3},..A_{n}$ are the $i$ largest elements placed in order. 

    By repeating the same arguments in the base case, we can conclude, based on the correctness of heapify, that after line (5), $A_{1}$ is either the root of a heap or the heap inequality did not hold for some $i\in [2,(n-i)/2]$. In the latter case, this would contradict the induction assumption (since before line (3), $A_{1}..A_{n-i+1}$ were heaps).
\end{enumerate}
\end{proof}

\section{Linear Time Sorts}
\paragraph{ Counting sort.}
Counting sort assumes that each of the $n$ input elements is an integer with a size at most $k$. It runs in $\Theta(n + k)$ time, so when $k = O(n)$, counting sort runs in $\Theta(n)$ time. It first determines, for each input element $x$, the number of elements less than or equal to $x$. Then, it uses this information to place element $x$ directly into its position in the output array. For example, if there are 17 elements less than or equal to $x$, then $x$ will be placed in position 17. However, we need to make some adjustments to this method to handle cases where multiple elements have the \textbf{same value}. We do not want all of them to end up in the same position.

%
%{Counting-sort$(A, n, k)$}
  \begin{algorithm}
  	let B and C be new arrays at size $n$ and $k$ \\ 
  	\For{ $i \in  [0, k]$}{
		$C_{i} \leftarrow 0 $
  	}
	\For{ $j \leftarrow [1, n]$}{
	  $C_{A{j}} \leftarrow C_{A_{j}} + 1 $
 	}
	\For{ $i \in  [1, k]$}{
	  $C_{i} \leftarrow C_{i} + C_{i – 1} $
 	}
	\For{ $j \in [n , 1]$}{
	  $B_{C_{A_{j}}} \leftarrow A_{j}$ \\
	  $C_{A_{j}} \leftarrow C_{A{j}} – 1$ \ \ // to handle duplicate values
	}
	  return $B$
\caption{Counting-sort$(A, n, k)$}
  \end{algorithm}
%

Notice that the Counting sort can beat the lower bound of $\Omega\left(n \log n\right)$ only because it is not a comparison sort. In fact, no comparisons between input elements occur anywhere in the code. Instead, counting sort uses the actual values of the elements to index into an array.

An important property of the counting sort is that it is \textbf{stable}.

\begin{defbox}{Stable Sort.}
 We will say that a sorting algorithm is stable if elements with the same value appear in the output array in the same order as they do in the input array. \end{defbox}

Counting sort's stability is important for another reason: counting sort is often used as a subroutine in radix sort. As we shall see in the next section, in order for radix sort to work correctly, counting sort must be stable.


\paragraph{Radix sort}
\marginnote[Note 1: That introduction was copied word by word from a web source. Do not use for commercial purposes. ]{Note 1: That introduction was copied word by word from a web source. Do not use for commercial purposes.} Radix sort is the algorithm used by the card-sorting machines you now find only in computer museums. The cards have 80 columns, and in each column, a machine can punch a hole in one of 12 places. The sorter can be mechanically "programmed" to examine a given column of each card in a deck and distribute the card into one of 12 bins depending on which place has been punched. An operator can then gather the cards bin by bin, so that cards with the first place punched are on top of cards with the second place punched, and so on.

The Radix-sort procedure assumes that each element in the array $A$ has d digits, where digit 1 is the lowest-order digit and digit $d$ is the highest-order digit.


%
%{radix-sort(A, n, d)}
  \begin{algorithm}
    \For{ $ i \in [1,d]$ } {
        use a stable sort to sort array $A$ on digit $i$
    }
\caption{radix-sort($A$, $n$, $d$)}
  \end{algorithm}
%

\paragraph{Correctness Proof.} By induction on the column being sorted.
\begin{itemize}
  \item Base. Where $d = 1$, the correctness follows immediately from the correctness of our base sort subroutine. 
  \item Induction Assumption. Assume that Radix-sort is correct for any array of numbers containing at most $d-1$ digits. 
  \item Step. Let $A^{\prime}$  be the algorithm output. Consider $x,y \in A$. Assume without losing generality that $x > y$. Denote by $x_{d}, y_{d}$ their $d$-digit and by $x_{/d}, y_{/d}$ the numbers obtained by taking only the first  $d-1$ digits of $x,y$. Separate in two cases:

    \begin{itemize}
      \item   If $x_{d} > y_{d}$ then a scenario in which $x$ appear prior to $y$ is  imply contradiction to the correctness of our subroutine.
      \item   So consider the case in which $x_{d} = y_{d}$. In that case, it must hold that $x_{/d} > y_{/d}$. Then the appearance of $x$ prior to $y$ either contradicts the assumption that the base algorithm we have used is stable or that $x$ appears before $y$ at the end of the $d-1$ iteration. Which contradicts the induction assumption. 
    \end{itemize}
 \end{itemize}

The analysis of the running time depends on the stable sort used as the intermediate sorting algorithm. When each digit lies in the range $0$ to $k – 1$ (so that it can take on $k$ possible values), and $k$ is not too large, counting sort is the obvious choice. Each pass over $n$ $d$-digit numbers then takes $\Theta(n + k)$ time. There are $d$ passes, and so the total time for radix sort is $\Theta\left(d(n + k)\right)$.

\paragraph{Bucket sort. \ctt{Only if there is time.} }
\marginnote[Note 2:  We will return to analyze the expected (average) running time after the lecture on probability.
]{Note 2: We will return to analyze the expected (average) running time after the lecture on probability.
}Bucket sort divides the interval [0, 1) into n equal-sized subintervals, or buckets, and then distributes the n input numbers into the buckets. Since the inputs are uniformly and independently distributed over [0, 1), we do not expect many numbers to fall into each bucket. To produce the output, we simply sort the numbers in each bucket and then go through the buckets in order, listing the elements in each.

%
%{bucket-sort(A, n)}
  \begin{algorithm}
    	let B[0 : n – 1] be a new array \\
	\For{ $i \leftarrow [0, n – 1]$}{
	   make $B_{i}$ an empty list
       	}
	\For{ $i \leftarrow [1, n]$}{
	    insert $A_{i}$ into list $B_{ \lfloor n A_{i} \rfloor} ]$
       	}
	\For{ $i \leftarrow [0, n – 1]$}{
	    sort list $B_{i}$
       	}
	concatenate the lists $B_{0}, B_{1}, .. , B_{n – 1}$ together and\\
	return the concatenated lists
\caption{bucket-sort($A$, $n$)}
  \end{algorithm}
%

%

\section{Sorting in Comparison Model.}

We have learned in the lecture that the runs of any sorting algorithm, which does not assume anything about the input's structure except the ability to compare elements in pairs, can be modeled as a binary tree. At each node, the algorithm compares two elements and, based on the result, moves to either the left or right child. Eventually, we reach the leaf of the tree and output the sorted elements. Notice that the nodes only exist in our imagination; the algorithm is not aware of their existence. We will name that tree the comparison tree.

The height of the comparison tree is (at least) the running time of the algorithm. We are going to demonstrate how to use these ideas to lower-bound the running time of the find-peak problem in the comparison model.


\begin{example}
  Prove that no algorithm can find a peak in less than $\Theta\left( \log n \right)$ time.
\end{example}
\begin{proof}
  Consider the following inputs for the problem, $A^{j}$ will be a tringle that get's is point at position $j$. Namely:  
  \begin{equation*}
      A^{j}_{i} = \begin{cases}
        i & i < j \\
         j + j - i & \text{else}
      \end{cases}
  \end{equation*} 
  Now, let's assume, in contradiction, that there exists an algorithm $\mathcal{A}$ in the comparison model that runs in less than $\log n$ time. This implies that the comparison tree representation of its running has a height less than $\log n$ and the number of its leaves, each associated with a possible output, is less than $n$. Remember that we have $n$ inputs of the form $A^{j}$. 

  By applying the pigeonhole principle, we can conclude that there are distinct $j$ and $j^{\prime}$ such that $\mathcal{A}$ reaches the same leaf when given $A^{j}$ and $A^{j^{\prime}}$ as inputs. Since their peaks are set at different positions, it follows that the algorithm will output a wrong answer for at least one of them.
\end{proof}

%\paragraph{Question.} Suppose that $\mathcal{A}$ outputs some number depending on the input, for example, ''Return $A$[$A$[7]]''. Does the argument above still work (and if so, why)?


\begin{example}
  Prove that no algorithm, in the comparison model, can merge two sorted arrays in time $\Theta(n^{1-\varepsilon})$ for some $\varepsilon>0$.
\end{example}
\begin{proof}
 Assuming, for contradiction, that there exists an algorithm $\mathcal{A}$ in the comparison model that merges two given sorted arrays in time $\Theta\left( n^{1-\varepsilon} \right)$. Now, consider the sorting algorithm obtained by replacing the merge subroutine in merge-sort with $\mathcal{A}$. On one hand, our new algorithm is a sorting algorithm in the comparison model (otherwise it would contradict the correctness of $\mathcal{A}$), while on the other hand, its running time is equal to:
  \begin{equation*}
    \begin{split}
      T(n) = n^{1-\varepsilon} + 2T(n/2) \Rightarrow T(n) = \Theta(n)
    \end{split}
  \end{equation*}
  This contradicts our $\Omega(n\log n)$ lower bound for sorting in the comparison model.
\end{proof}

\newpage

\section{Binaries Trees and How to Encode Them. \ctt{Only if there is time.}} We have already seen, in heaps, that organizing our data in a graph-like structure can offer a speed advantage. For future applications, and in particular for maintaining data in sorted order, we will have to encode our data using binary trees. These trees may not be almost complete and also have to support pointer manipulations, specifically placing a binary tree as a left or right subtree of a given node. To enable this, we will have to treat the \textbf{right}, \textbf{left}, and \textbf{parent} as variables, in contrast to heaps where they are determined completely by the node index. We begin this section by stating definitions.


\begin{definition}
  \begin{enumerate}
    \item Binary Tree: A tree in which any vertex has at most two children.
    \item A descendant of vertex $x$ is a vertex in the subtree whose root is $x$. A left descendant of vertex $x$ is either a vertex in the subtree whose root is the left child of $x$, or $x$'s left child.
    \item An ancestor of $x$ is a vertex to which $x$ belongs as a descendant.
    \item A leaf is a vertex without children.
    \item Height of vertex $x$ is the length of the longest simple path (without cycles) between $x$ and one of the leaves.
    \item Height of the tree is the height of its root, which is usually denoted by $h$.
\end{enumerate}
\end{definition}

We encode a binary tree by associating a field to each vertex $x$, representing its right, left children, and parent. We use the notation $x$.left to refer to the left child of $x$, although the physical implementation may differ conceptually. For example, the way binary trees are implemented in Cormen is through 4 arrays. The first stores the value of $x$, while the others store pointers of specific types. For instance, the array LEFT, where LEFT.$x$ stores the left child of $x$.


If nothing else has been mentioned, then we can assume that we can add additional fields to the vertices.


\section{Binary Search Trees.} Binary search tree (BST) is a binary tree which any node $x$ of it: 
\begin{enumerate}
  \item Contains a field key, storing a number $x$.key. 
  \item Any left descendant $y$ of $x$ satisfies $y$.key $\le$ $x$.key. 
  \item Any right descendant $y$ of $x$ satisfies $y$.key $\ge$ $x$.key. 
\end{enumerate}

\paragraph{Question.} Let $T$ be a binary search tree, Where are the minimum and maximum values of $T$? (most left and right nodes). 

\begin{definition}
Let $T$ be a binary search tree, and let $x$ be a node belonging to it. The predecessor of $x$ will be defined as a vertex $y$ such that $y$.key $\leq x$.key and $y$.key is maximal among the nodes satisfying this condition. If we were to set the values of $T$ in sorted order, then the predecessor of $x$ would be located on its left. The successor of $x$ will be defined as $y$, where $x$ is the predecessor of $y$.
\end{definition}

\paragraph{Functionalitiy of BST.}
\begin{enumerate}
\item Search($T$, key): returns a pointer to the vertex whose key equals key. 
  \item Min($T$): returns a pointer to the vertex with the minimum value in $T$.  
  \item Max($T$): returns a pointer to the vertex with the maximum value in $T$.  
  \item Predecessor($x$): returns a pointer to the predecessor of $x$.  
  \item Successor($x$): returns a pointer to the successor of $x$. 
  \item Insert($T$, key): inserts key into $T$ (creates a new vertex).   
  \item Delete($T$, $x$): removes $x$ from $T$. 
  \item Inorder($T$): outputs $T$'s keys in sorted order.

%\begin{algorithm}[H]
%\SetAlgoLined
%\KwData{$T$ - tree, $x$ - vertex to delete}
%\KwResult{removes $x$ from $T$}
%\Set
\end{enumerate}


\begin{example}Questions from past exams.
  \begin{enumerate}
    \item Write an algorithm that, given a binary search tree $T$, returns a max heap containing all the elements of $T$. What is the lower bound for this problem? Explain.
    \item Write an algorithm that, given a heap $H$, returns a binary search tree containing all the elements of $H$. What is the lower bound for this problem? Explain.
  \end{enumerate}
\end{example}

\paragraph{Solution.} 
  \begin{enumerate}
    \item Remember that any array sorted in reverse order is also a max heap. This is because if $A_{i} \ge A_{j}$ for any $i < j$, then it follows that $A_{i} \ge A_{2i}, A_{2i+1}$ for any $i$. Therefore, we will output the values of $T$ in reverse order. We do this by applying Inorder in reverse, as given in \Cref{alg:treetoheap}. The running time is $\Theta(n)$ and of course it is also the lower bound (as we must read all the inputs for printing it).
      \begin{algorithm}
       \KwData{$r$ - a vertex in BST}
       
       \If{$r$.right is not empty/Null}{
         Reverse-Order($r$.right)
       }
       Print $r$.key \\
       \If{$r$.left is not empty/Null}{
         Reverse-Order($r$.left)
       }
        \caption{Reverse-Order }
\label{alg:treetoheap}
      \end{algorithm}
    \item We will initialize an empty binary search tree and then add the elements to it one by one. Each insertion will cost the current height of the tree, so it might sum up to $\Theta(n^{2})$. However, next week we will see how those insertions could be done in a way that preserves the balance of the tree, namely guaranteeing that the height of the tree will remain logarithmic.
  \end{enumerate}

\begin{example} Adding fields to BSTs. We would like to support $k$-smallest element extraction. Suggest a field that will be used for computing the $k$-smallest element, explain how to maintain it, and write an algorithm that extracts the $k$-smallest element. 
\end{example}

\paragraph{Solution.} We will associate with each vertex of the tree a field that counts the number of nodes whose keys are lower than it (the size of its left subtree). Let's denote it as left-size. The code is given in \Cref{alg:kstat}. Guideline for proving correctness:
Consider the root, and let's separate into cases based on how many elements the root is greater than.
\begin{enumerate}
  \item If the root is strictly greater than $k-1$ elements, then the $k$-smallest element is in its left subtree and is also the $k-1$ smallest element in that subtree.
  \item If the root is greater than strictly fewer than $k-1$ elements, then it is clear that the $k$-smallest element is also greater than it and located in its right subtree. However, in contrast to the previous case, here the order of the $k$-smallest element of the whole tree in the subtree will be the substitution between $k$ and the number of nodes in the $\{$ left-subtree $\cup$ root $\}$.
  \item The third case is when the root is greater than exactly $k-1$ elements, which by definition sets it as the $k$-smallest element.
\end{enumerate}
In the first two cases, we are calling Get-Order on trees (BST with our additional field) with smaller height. Therefore, it is clear how one would prove correctness by induction. We will see in the next recitation how to maintain the tree, update the field, and guarantee that the height of the tree will remain logarithmic.
  \begin{algorithm}
  \KwData{$r$ - a vertex in BST, $k$-stat parameter}
       $m \leftarrow r$.left-size \\ 
       \If{$m = k -1$ }{
         \Return $r$.key 
       }
      \If{$m > k -1$ }{
        \Return Get-Order($r$.left, $k$)
       }\If{$m < k -1$ }{
         \Return Get-Order($r$.right, $k - (m+1)$)
       }
        \caption{Get-Order }
\label{alg:kstat}
      \end{algorithm}
%\begin{example}
%
%\end{example}
%\begin{example}
%
%\end{example}
%\begin{example}
%
%\end{example}
%

      \newpage
      \newpage

\section{Appendix - BST Methods Source: A Mixture of Cormen, Wikipedia, and Codeforces}

\begin{algorithm}
\SetAlgoLined
\KwData{$T$ - tree, key - key to search for}
\KwResult{pointer to vertex with key equal to key}
    \If{$T$ is empty}{
        \Return null\;
    }
    \If{$T$.key equals key}{
        \Return $T$\;
    }
    \If{key is less than $T$.key}{
      \Return Search($T$.left, key)\;
    }
    \Else{
      \Return Search($T$.right, key)\;
    }

\caption{Search} 
\end{algorithm}

\begin{algorithm}
\SetAlgoLined
\KwData{$T$ - tree}
\KwResult{pointer to vertex with minimum value in $T$}
    \If{$T$ is empty}{
        \Return null\;
    }
    \If{$T$.left is empty}{
        \Return $T$\;
    }
    \Return Min($T$.left)\;
\caption{Min} 
\end{algorithm}

\begin{algorithm}
\SetAlgoLined
\KwData{$T$ - tree}
\KwResult{pointer to vertex with maximum value in $T$}
    \If{$T$ is empty}{
        \Return null\;
    }
    \If{$T$.right is empty}{
        \Return $T$\;
    }
    \Return Max($T$.right)\;
    \caption{ Max }
\end{algorithm}

\begin{algorithm}
\SetAlgoLined
\KwData{$x$ - vertex}
\KwResult{pointer to predecessor of $x$}
    \If{$x$.left is not empty}{
      \Return Max($x$.left)\;
    }
    $y \leftarrow x$.parent\;
    \While{$y$ is not empty and $x$ is $y$.left}{
        $x \leftarrow y$\;
        $y \leftarrow y$.parent\;
    }
    \Return $y$\;
\caption{Predecessor}
\end{algorithm}

\begin{algorithm}
\SetAlgoLined
\KwData{$x$ - vertex}
\KwResult{pointer to successor of $x$}
    \If{$x$.right is not empty}{
      \Return Min($x$.right)\;
    }
    $y \leftarrow x$.parent\;
    \While{$y$ is not empty and $x$ is $y$.right}{
        $x \leftarrow y$\;
        $y \leftarrow y$.parent\;
    }
    \Return $y$\;
\caption{Successor}
\end{algorithm}

\begin{algorithm}
\SetAlgoLined
\KwData{$T$ - tree, key - key to insert}
\KwResult{inserts key into $T$}
    newNode $\leftarrow$ create new vertex with key $key$\;
    $y \leftarrow$ null\;
    $x \leftarrow T$\;
    \While{$x$ is not empty}{
        $y \leftarrow x$\;
        \If{$key$ is less than $x$.key}{
            $x \leftarrow x$.left\;
        }
        \Else{
          $x \leftarrow x$.right\;
        }
    }
    newNode.parent $\leftarrow y$\;
    \If{$y$ is null}{
        $T \leftarrow$ newNode\;
    }
    \ElseIf{$key$ is less than $y$.key}{
        $y$.left $\leftarrow$ newNode\;
    }
    \Else{
        $y$.right $\leftarrow$ newNode\;
    }
\caption{Insert}
\end{algorithm}

%\begin{algorithm}
%\SetAlgoLined
%\KwData{$T$: The input tree, $x$: The key to be deleted}
%\If{$x$ is in $T$}{
%  Remove $x$ from $T$\;
%}
%\caption{Delete($T$, $x$)}
%\end{algorithm}
%
%2. Inorder($T$):
\begin{algorithm}
\SetAlgoLined
\KwData{$T$: The input tree}
\If{$T$ is not empty}{
  Inorder($T$.left)\;
  Output $T$.key\;
  Inorder($T$.right)\;
}
\caption{Inorder($T$)}
\end{algorithm}






%\newcommand*{\EXERCISE}{}%
%\input{../tex/texlib/head.tex}


 

%\newcommand*{\RECITATION}{}%


\setcounter{chapter}{4}
\ifdefined\SOLUTION
  \section{DAST. Exercise 4. Solution.}
\else
  \section{DAST. Exercise 4.}
\fi

\ifdefined\CHECK


  \paragraph{Checking.} Please only check (1.2) [bubble sort stability], (2.1), and (3). Please be brutal when it's clear that the student doesn't state formal statements.
\paragraph{}

\fi


 \subsection{Stability.} For each of the following sorts, prove or disprove whether they are stable.
 \begin{enumerate}
   \item Heap sort. 
   \item Bubble sort.
   \item Counting sort.
 \end{enumerate}

\ifdefined\SOLUTION
  \paragraph{Solution.} \\
\begin{enumerate}
  \item Heap sort. No, for a counterexample, consider an array consisting of $n$ identical values. The array is already a heap, and therefore the build subroutine wouldn't swap elements. Yet, on the first iteration of heapsort, the root of the heap, which is also the first element of the array, will be moved to the end of the output array.
  \item Bubble sort. Yes, let's suppose that $x$ and $y$ are two elements in the input array, such that $x = y$ and $x$ appears before $y$ in the input. By the definition of the bubble sort algorithm, which only swaps adjacent elements, $x$ can only pass $y$ if there is an iteration $i$ such that: \begin{enumerate}
      \item $A_{i}=x$ and $A_{i+1}=y$
      \item $A_{i} > A_{i+1}$ 
    \end{enumerate} And it's clear that both conditions can't satisfied simultaneously
\ifdefined\CHECK
  \paragraph{Check.} No special marks.
\fi
  \item Counting sort. Yes, let's consider again $x$ and $y$ are two elements in the input array, such that $x = y$ and $x$ appears before $y$ in the input. Denote by $v,u$ the positions of $x,y$ in $A$. By definition of the algorithm, we set the output in the fourth for loop. Since the loop iterates in decreasing order, we have that $y$ is placed before $x$ at position $C_{y}$. However, the value of $C_{y}$ decreases immediately afterwards and cannot increase again (since no line in the fourth for loop is defined to increase any of the $C$ values). Therefore, it must hold that when the loop reaches the position of $x$ on the $i = v$ iteration, $C_{x}=C_{y}$ is strictly lower than $C_{x}=C_{y}$ in the $i=u$ iteration. This means that $x$ will be placed before $y$ in the output.
 \end{enumerate}
\fi

 \subsection{Sorts That Assume Input Properties.}
 In any of the following sections, we make another assumption about the input. For each section, write an algorithm that uses the assumption to sort the inputs and compute its running time. You don't have to provide a correctness proof.
 \begin{enumerate} 
   \item $x_{1},x_{2},...,x_{n} \in \mathbb{R}$ such that $\max_{i} \{x_{i}\} - \min_{i} \{x_{i}\} \leq 5n$ and $|x_{i} - x_{j}| > 1$ for any $i \neq j$.
%\item $x_{1},x_{2},...,x_{n} \in \mathbb{R}$ and let $k \in \mathbb{N}$, $k < n$ such that $x_{i} \leq x_{i+k}$ for any $i \in [n]$. The running time should depend on $k$.
   \item $x_1, x_2, ..., x_n \in \mathbb{R}$ such that $\max_{i} \{x_{i}\} -\min_{i} \{x_{i}\} \leq 5n$ and $|x_{i} - x_{j}| > \Delta$ for any $i \neq j$, but it is unknown what $\Delta$ is. The running time should depend on $\Delta$.
 \end{enumerate}
\ifdefined\SOLUTION

  \paragraph{Solution.} \\

  \begin{enumerate}
    \item  We are going to use a variation of counting sort. First, we observe that if it is given that $x_{i} < 5n$ for any $i$, then combining the fact that $|x_{i} - x_{j}| > 1$ for any $i,j$, we can conclude that there is a one-to-one mapping between the $x$s and integers in the range $[5n]$, given by $\lfloor x_{i} \rfloor$. Otherwise, if $\lfloor x_{i} \rfloor = \lfloor x_{j} \rfloor$, it implies that $|x_{i} - x_{j}| \le 1$. Therefore, in this case, we can count which elements appear in the mapping image, 'sort them', and associate them with their source in the range. \Cref{alg:auss1} does exactly that, but it also substitutes the minimal element for concentrating the image in $[5n]$. The running time of the algorithm is $\Theta(n)$ since we iterate over the array with a length of at most $5n$ a constant number of times.

      \begin{algorithm}[H]
     %\KwResult{Compare Camels Pair $j,j^{\prime}$.}
     \caption{ Sorts assuming $|x_{i} - x_{j}| > 1$ \\ \ \ \ \ \ and $ \max_{i} \{x_{i}\} -\min_{i} \{x_{i}\} \leq 5n$ } \label{alg:auss1}
     Let $B$ be a $5n$-length array\\
     Let $C$ be a $n$-length array\\
     Let $\xi \leftarrow \min x$
     \For{ $ i \in [5n] $ }{ 
       $B_{i} \leftarrow $ Null
     }
     \For{ $i\in [n]$ } {
       $B_{\lfloor x_{i} - \xi   \rfloor } \leftarrow x_{i}$
     }
     Let $j \leftarrow 0$ \\
     \For{$i \in [5n]$ } {
       \If{ $B_{i}$ is not Null  }{
         $C_{j} \leftarrow B_{i}$ \\
         $ j \leftarrow j + 1$
       }
     }
    \Return $C$
\end{algorithm}
\ifdefined\CHECK
  \paragraph{Check.} Please penalize any solution that does not correct for non-integer values.
\fi
\item We observe that if $\Delta > 1$, then \Cref{alg:auss1} sorts the numbers. However, if $\Delta < 1$, then one of the values of $B$ might get overridden. As a result, the number of non-null elements in $B$ after line (8) will be lower than $n$. Thus, the returned array will have a length less than $n$. To address this issue, we will use binary search to find the value of $\Delta$. Then, we will transform the array $x_{1}, x_{2}, ..., x_{n}$ to $\Delta^{-1} \cdot x_{1}, \Delta^{-1} \cdot x_{2}, ..., \Delta^{-1} \cdot x_{n}$, which satisfies the assumption of the above section, except that $\max x_{i} - \min x_{i} < 5n/\Delta$. This means that the running time of \Cref{alg:auss1} after modifications will be $\Theta(n/\Delta)$. For each guess of $\Delta$, we will check if \Cref{alg:auss1} successfully sorts $x$ into $C$. If it does, then we return. Otherwise, we check for $\Delta \leftarrow \frac{1}{2}\Delta$. In total, the running time will be the sum up to:
  \begin{equation*}
    \begin{split}
      T(n,\Delta) &= 5n \cdot c + 5n \cdot 2 \cdot c + 5n \cdot 2^{2} \cdot c + .. + 5n \cdot \Delta^{-1} \cdot c \\ 
      & \le  c \cdot \Delta^{-1}5n\left( \sum^{\infty} \frac{1}{2^{i}} \right)  = c \cdot 10n/\Delta = O(n/\Delta)
    \end{split}
  \end{equation*}

  \begin{algorithm}[H]
\caption{Sorts assuming $|x_{i} - x_{j}| > \Delta$ \\ \ \ \ \ \ and $ \max_{i} \{x_{i}\} -\min_{i} \{x_{i}\} \leq 5n$} \label{alg:auss2}
     Let $\Delta \leftarrow 1$ \\
     $C \leftarrow $ junk at length $< n $\\ 
     \While{ $C$ length $< n$ }{
     Let $x_{1}^{\prime}, x_{2}^{\prime} .. x_{n}^{\prime} \leftarrow \Delta^{-1}\cdot x_{1},\Delta^{-1} x_{2} .. \Delta^{-1} x_{n}$ \\
     $C \leftarrow $ call to \Cref{alg:auss1} over $x^{\prime}$s \\
     \ \ \ where $\max x^{\prime}_{i} - \min x^{\prime}_{i} < 5 \Delta^{-1} n$ \\
     $\Delta \leftarrow \Delta /2$
   }
    \Return $C$
\end{algorithm}

\end{enumerate}
\fi 
 \subsection{Camels.} 
 Bob is willing to send Alice camels, $c_1, c_2, ..., c_n$ in a specific order. Unfortunately for him, Eve doesn't like him. Hence, she decides to spoil their order by stacking camels as follows: Eve chooses camel $c_i$ and a position $j$, and then moves the $i$th camel $c_i$ between $c_j$ and $c_{j+1}$, She can move as many camels as she wishes. Alice, known for her strictness, wouldn't accept camels standing out of order. Luckily, Bob is allowed to send the camels again in their original order for five times, and Eve cannot choose the same camel twice. In other words, if Eve chooses to move $c_i$ in the first attempt, then she cannot move it in any of the other four attempts.

Alice records the positions of the camels every time they come. Write an algorithm that will help Alice restore the order. (Prove correctness and compute the running time.)
\ifdefined\SOLUTION
  \paragraph{Solution.} Guideline: Let $A \in \mathbb{R}^{5 \times n}$ be the recordings listed by Alice, where $A_{i,j} = k$ if the $j$th camel came at position $k$ on the $i$th attempt.  We can think of the problem as a sorting problem. Denote by $A^{\star}$ an arbitrary arrangement of the camels. Been able to answer whether $A^{\star}_{j} \le A^{\star}_{j^{\prime}}$ regarding the original order, meaning that the inequality $A^{\star}_{j} \le A^{\star}_{j^{\prime}}$ is satisfied if the camel on the $j$ position should be put before the camel on the $j^{\prime}$ position will allow us to sort $A^{\star}$ into the original order. We claim that \Cref{alg:camels} is a true comparator for camels ordering. 

  \begin{claim} 
If the $j$th camel appears before the $j^{\prime}$th camel in the original order, then \Cref{alg:camels} returns $\leq$, otherwise it returns $>$.
  \end{claim}
  \begin{proof}
    Suppose that the $j$th camel appears before the $j^{\prime}$th camel in the original order. Then the only cases where the $j$th camel might appear after the $j^{\prime}$th camel in one of the attempts are the ones where Eve moved at least one of them ( That's a simple claim, complete solution should contains it's proof ). Since Eve is restricted to touching each camel at most once, in at least 3 of the 5 attempts, both camels were not touched and their relative order did not change. Technically, $A_{i,j} \le A_{i,j^{\prime}}$ for at least 3 values of $i$, so in the for loop, the value of $\Delta$ will increase at least 3 times. Repeating the above, but changing the direction of the inequality, proves that if in the original order, $j$ appears after $j^{\prime}$, then $\Delta$ cannot exceed $2$.
  \end{proof}

  Now we have an algorithm. First, take an initial guess for $A^{\star}$ (For example, take $A^{\star}$ to be the setting of camels on the first attempt), Then call to your favorite sorting algorithm (in the comparison model), and use the comparator in \Cref{alg:camels} for executing it. Incorrectness of the suggested algorithm implies the incorrectness of either our comparator or the incorrectness of the sorting algorithm which in both cases is contradiction. Since the running time of \Cref{alg:camels} is $O(1)$, any choice of $\Theta(n\log n)$-time sorting algorithms results a $\Theta(n\log n)$-time algorithm for sorting the camels. That's because:   
  \begin{equation*}
    \begin{split}
      T(n) & \le \text{Time} ( \text{ The sorting alg calls on each iteration to \Cref{alg:camels} } ) \\ 
      & \le \Theta( n \log n) \cdot O(1)  = \Theta(n\log n)
    \end{split}
  \end{equation*}


   \begin{algorithm}
     \KwResult{Compare Camels Pair $j,j^{\prime}$.}
\caption{Camels Comparator.} \label{alg:camels}
     Let $\Delta \leftarrow 0$ \\
     \For{$i \in [5]$ } {
       \If{ $A_{i,j} \le A_{i,j^{\prime}}$  }{
         $\Delta \leftarrow \Delta + 1$
       }
     }
     \If{ $\Delta \ge 3$}{
       \Return \ '$\le$'
     }
     \Else{
       \Return \ '$>$'
     }
\end{algorithm}
\ifdefined\CHECK

  \paragraph{Check.} Please penalize any solution that runs in more than $\Theta(n\log n)$. In addition, penalize solutions whose correctness is independent on the fact that Bob sends the camels five times.
\fi
%Let $A^{\star}_{j}$ be the original position of the $j$th camel (the array we would like to restore).



\fi

\subsection{Predecessor in Binary Search Tree}
\begin{enumerate}
  \item Write pseudocode for the predecessor subroutine.
  \item Prove its correctness.
\end{enumerate}
\ifdefined\SOLUTION
  \paragraph{Solution.} Correctness. Let $z$ be a vertex in the tree such that $z$.key is lower (or equals) than $x$.key. Observe that by the BST property, either $z$ is a left-descendant of $x$ or $z$ and $x$ have a common ancestor $w$ (which might be $z$) such that $z$.key $\le w$.key $\le x$.key. In the second case, it is clear that $z$ cannot be the predecessor unless $w$ is $z$, or in other words, $z$ is a right-ancestor of $x$. By concatenating inequalities, we have that the lowest right-ancestor of $x$ is the one closest from left to $x$ among all its right-ancestors. Let us denote it by $y$. Now, observe that if $z$ is a left-descendant of $x$, then it is greater than $y$ and lower than $x$. Thus, if $x$.left is not empty, then the closest element to $x$ from the left is the maximum of its left descendants. In the other case, where $x$.left is indeed empty, we have that $y$ remains the closest element to $x$ from the left in the whole tree.

 
  Now, \Cref{alg:bst} first checks if $x$.left is not empty. If so, it looks for $x$'s maximal left-descendant. Otherwise, it climbs up until it reaches its first (lowest) right-ancestor.

  
  \begin{algorithm}
     \KwResult{Return the predecessor of $x$.}
\caption{Predecessor.} \label{alg:bst}
\If{ $x$.left is not Null }{
  \Return Tree-Max($x$.left)
}
Let $y \leftarrow x$.parent \\
\While{ $y$ is not Null and $x = y$.left } {
    $x \leftarrow y$\\
    $y \leftarrow x$.parent
}
\Return $y$ 
\end{algorithm}
\fi
\ifdefined\Book
\fi



\newcommand{\image}{\text{ Im } }



\ifdefined\BOOK
\else
\setcounter{chapter}{6}
\fi
\chapter{Probability.} 

\section{ Probability Spaces. }

\begin{definition}
  A probability space is defined by a tuple $(\Omega,P)$, where:
  \begin{enumerate} 
    \item $\Omega$ is a set, called the sample space. Any element $\omega\in \Omega$ is an atomic event. Conceptually, we think of atomic events as possible outcomes of our experiment. Any subset $A \subset \Omega$ is an event. 
    \item $P$, called the probability function, is a function that assigns a number in $[0,1]$ to any event, denoted as $P : 2^\Omega \rightarrow [0,1]$, and satisfies:
      \begin{enumerate}
        \item For any event $A \subset \Omega$, $P(A) = \sum_{\omega\in A}P(\omega)$. 
        \item Normalization over the atomic events to $1$, which means $\sum_{\omega\in\Omega}P(\omega)~=~1$.
      \end{enumerate}
  \end{enumerate}
\end{definition}
\begin{example}
  Consider a dice rolling, where each of the faces is indexed by $1,2,3,4,5,6$ and has an equal chance of being rolled. Therefore, our atomic events are associated with the rolling result, and $P$ is defined as $P(\omega) = \frac{1}{6}$ for any such atomic event.
  An example of an event can be $A=$ ''the dice falls on an even number''. The probability of this outcome is:
  \begin{equation*}
    \begin{split}
      P(A)= \sum_{\omega\in A}{ P(\omega) } = P(\{2\}) + P(\{4\}) + P(\{6\}) = 3\cdot \frac{1}{6} = \frac{1}{2} 
    \end{split}
  \end{equation*}
\end{example}

%\begin{claim}
%Probability function  satisfies the following properties:
%\begin{enumerate}
%  \item $P(\emptyset) = 0$.
%  \item Monotonic, If $A \subset B \subset \Omega$ then $P(A) \le P(B)$.
%  \item Union Bound, $P(A \cup B) \le P(A) + P(B)$.
%  \item Additivity for disjointness events. If $A\cap B = \emptyset$ then $P(A \cup B) = P(A) + P(B)$.
%  \item Denote by $\bar{A}$ the complementary event of $A$, which means $A\cup\bar{A} = \Omega$. Then, $P(\bar{A}) = 1 - P(A)$.
%\end{enumerate}
%\end{claim}
%
\begin{claim}
The probability function satisfies the following properties:
\begin{enumerate}
  \item $P(\emptyset) = 0$.
  \item Monotonicity: If $A \subset B \subset \Omega$, then $P(A) \le P(B)$.
  \item Union Bound: $P(A \cup B) \le P(A) + P(B)$.
  \item Additivity for disjoint events: If $A\cap B = \emptyset$, then $P(A \cup B) = P(A) + P(B)$.
  \item Complementarity: Denote by $\bar{A}$ the complementary event of $A$, which means $A\cup\bar{A} = \Omega$. Then, $P(\bar{A}) = 1 - P(A)$.
\end{enumerate}
\end{claim}

\begin{example}
  Let's proof the additivity of disjointness property. Let $A,B$ disjointness events, so $A \cap B = \emptyset$ then 
  
  \begin{equation*}
    \begin{split}
      P(A\cup B) &= \sum_{w \in A \cup B}P(w) \\ 
      &= \overbrace{\sum_{w \in A, w \notin{B}}P(w)}^{P(A)} + \overbrace{\sum_{w \in B, w \notin A}P(w)}^{P(B)}  +\overbrace{ \sum_{w \in A, w \in  B}P(w) }^{ 0 } \\ 
      &= P(A) + P(B) 
    \end{split}
  \end{equation*}
\end{example}

\begin{definition}
  Let $(\Omega,P)$ be a probability space. A random variable $X$ on $(\Omega,P)$ is a function $X : \Omega \rightarrow \mathbb{R}$. An indicator, is a random variable defined by an event $A \subset \Omega$ as follows   
  \begin{equation*}
    X(\omega) = \begin{cases}
      1 & \omega \in A \\
      0 & \omega \notin A
    \end{cases}
  \end{equation*}
Sometimes, we will use the notation $\{ X = x \}$ to denote the event $A$ such: 
\begin{equation*}
  \begin{split}
    A = \{ \omega : X(\omega) = x \} := \{ X = x \} 
  \end{split}
\end{equation*}
\end{definition}
\begin{example} \label{example:twodic} Consider rolling a pair of dice. Denote by $X : [6]\times [6] \rightarrow [6] $ the random variable that is set to be the result of the first roll. Let $Y$ be defined in almost the same way, but setting the result of the second die. Namely, if we denote by $\{(i,j)\}$ the atomic event associated with sample $i$ on the first die and $j$ on the second die, then:
  \begin{equation*}
    \begin{split}
      X(\{i,j\}) &= i \\ 
      Y(\{i,j\}) &= j  
    \end{split}
  \end{equation*}
  In addition, one can define the random variable $z$ as the sum, $Z = X+Y$. Since the sum is also a function from $\Omega$ to $\mathbb{R}$, $Z$ is also a random variable. An example of an indicator could be $W$, which gets $1$ if $Z \in \{2, 7, 8\}$.
\end{example}


\begin{example}
  Let $X$ be an indicator of event $A$. Then $1 - X$ is the indicator of $\bar{A}$.
\begin{equation*}
   1 -  X(\omega) = \begin{cases}
     0 & \omega \in A \Leftrightarrow  \omega \notin \bar{A} \\
      1 & \omega \notin A\Leftrightarrow  \omega \in \bar{A}
    \end{cases}
  \end{equation*}

\end{example}

\begin{definition}
  We will say that two events $A,B$ are independent if:
\begin{equation*}
    \begin{split}
      P(A \cap B) &= P(A) \cdot P (B) %\Leftrightarrow \\ 
    \end{split}
  \end{equation*}
  Similarly we will say that random variables $X,Y : \Omega \rightarrow \mathbb{R}$  are independent if for any $x\in \image X$ and $y \in \image Y$:   
  \begin{equation*}
    \begin{split}
      P(X = x \cap Y = y) &= P(X = x) \cdot P (Y = y) %\Leftrightarrow \\ 
    \end{split}
  \end{equation*}
  \end{definition}
  \begin{example}
    $X,Y$ defined in \Cref{example:twodic} are independent. 
    \begin{equation*}
      \begin{split}
        P(\{X = i\} \cap \{Y = j\} ) &= \sum_{i^{\prime} = i \text{ and } j^{\prime}=j }P(\{(i^{\prime}, j^{\prime})\}) = P(\{ (i,j) \} ) \\ 
        &= \frac{1}{36} = \frac{1}{6} \cdot \frac{1}{6}  = P(X = i)P(Y = j)
      \end{split}
    \end{equation*}
  \end{example}

  \begin{example}
    Let $A$ and $B$ be independent events. Then, $\bar{A}$ and $B$ are also independent events, since:
    \begin{equation*}
      \begin{split}
    P(B) &= P(B \cap \Omega) = P(B \cap (A \cup \bar{A})) = P((B \cap A) \cup (B \cap \bar{A}))\\
    &= P(B \cap A) + P(B \cap \bar{A}) = P(B)P(A) +    P(B \cap \bar{A} ) \\
    \Rightarrow  P(B \cap \bar{A} ) & = P(B)(1 - P(A)) = P(B)P(\bar{A})
      \end{split}
    \end{equation*}
  \end{example}

  \begin{example}
    Let $X$ and $Y$ be indicators of independent events $A$ and $B$. Then $P(X\cdot Y = 1) = P(X = 1)\cdot P(Y = 1)$. The proof is left as an exercise.
  \end{example}

  \section{ Throwing Keys to Cells. }  
  \begin{example} Imagine that following experiment, we have $m$ cells and $n$ keys (balls, numbers, or your favorite object type). We throw each of the keys independently into the cells. The cells are identical, so the probability of hitting any of them is the same, $1/m$. We would like to analyze how the capacity of the cells is distributed.
    \begin{enumerate}
      \item What is the probability that the first and the second keys will be thrown to the first cell? What is the probability that the first and the second keys will be thrown to the same cell? 
      \item What is the probability that in the first cell there is exactly one key? 
    \end{enumerate}
    Let us define the indicator $X_{i}^{j}$ which indicate that the $j$th key fallen into the $i$th cell. 
    \begin{enumerate}
      \item So first we been asked whether $X_{1}^{1}\cdot  X_{1}^{2} = 1$, Since this happens only if both $X_{1}^{1} = 1$, $X_{1}^{2} = 1$ then by independently we have that: 
        \begin{equation*}
          \begin{split}
            P({X_{1}^{1}\cdot  X_{1}^{2} = 1}) &= P(X_{1}^{1} = 1 \cap  X_{1}^{2} = 1) \\
            & = P(X_{1}^{1} = 1 ) \cdot P( X_{1}^{2} = 1) =\frac{1}{m^2}
          \end{split}
        \end{equation*} Now, to answer if the first and second keys fall into the same cell, we need to check if there exists an $i$ such that $X_{i}^{1}\cdot X_{i}^{2} = 1$. Observes that for any different $i$ and $i^{\prime}$, the $X_{i}^{j}$ and $X_{i^{\prime}}^{j}$ are indicators of disjoint events. This is because $j$ cannot be in both the $i$ and $i^{\prime}$ cells. Therefore, $X_{i}^{1}\cdot X_{i}^{2}$ and $X_{i^{\prime}}^{1}\cdot X_{i^{\prime}}^{2}$ are also indicators of disjoint events. Thus: 
        \begin{equation*}
          \begin{split}
            P(\exists i : X_{i}^1 \cdot X_{i}^{2} = 1) &= P( \bigcup_{i} X_{i}^1 \cdot X_{i}^{2} = 1) \\
            &= \sum_{i}{P( X_{i}^1 \cdot X_{i}^{2} = 1)} = m\cdot \frac{1}{m^{2}} = \frac{1}{m}
          \end{split}
        \end{equation*} 
        We are basically done. However, we want to present the same calculation in a different notation that will be useful for computing expectations later on. Note that the random variable that counts ''how many'' cells both the first and the second fall into is $\sum_{i}{X_{i}^{1}\cdot X_{i}^{2} }$. In other words, the sum can be either $0$ if the keys fall into different cells, or $1$ if they both fall into the same cell.
      \item The event that only the $j$th key falls into the first cell matches to 
        \begin{equation*}
          \begin{split}
            \left\{ X_{1}^{j}\prod_{j\neq j^{\prime}}\left( 1 - X_{1}^{j^{\prime}} \right) = 1 \right\}
          \end{split}
        \end{equation*}
        Therefore, due to the disjointness of $1-X_{1}^{j^{\prime}}$ and $X_{1}^{j^{\prime}}$, the indicator for the first cell containing exactly one key is:
        \begin{equation*}
          \begin{split}
            \left\{ \sum_{j}{X_{1}^{j}\prod_{j\neq j^{\prime}}\left( 1 - X_{1}^{j^{\prime}} \right)} = 1  \right\}
          \end{split}
        \end{equation*} 
        Since the terms in the sum are disjoint and the products are products of independent indicators, we have:
        \begin{equation*}
          \begin{split}
            & P\left(\sum_{j}{X_{1}^{j}\prod_{j\neq j^{\prime}}\left( 1 - X_{1}^{j^{\prime}} \right)} = 1 \right) = \sum_{j}{ P\left( X_{1}^{j}\prod_{j\neq j^{\prime}}\left( 1 - X_{1}^{j^{\prime}} \right) = 1 \right)} \\
            = \ & m \cdot \frac{1}{m}\left( 1 - \frac{1}{m} \right)^{n-1} =  \left( 1 - \frac{1}{m} \right)^{n-1}
          \end{split}
        \end{equation*}
    \end{enumerate}
  \end{example}

\begin{definition}
  Let $X : \Omega\rightarrow \mathbb{R}$ be a random variable, the expectation of $X$ is 
  \begin{equation*}
    \begin{split}
      \expp{X} = \sum_{\omega\in \Omega}{X(\omega)P(\omega)} = \sum_{x \in \image X}{ x P(X = x)} 
    \end{split}
  \end{equation*} Observes that if $P$ is distributed uniformly, then the expectation of $X$ is just the arithmetic mean: \begin{equation*}
    \begin{split}
      \expp{X} = \sum_{\omega\in \Omega}{X(\omega)P(\omega)} =  \frac{1}{|\Omega|}\sum_{\omega\in \Omega}{X(\omega)}  
    \end{split}
  \end{equation*}
\end{definition}

\begin{claim}
   The expectation satisfies the following properties:
   \begin{enumerate}
     \item Monotonic, If $X \le Y$ (for any $\omega \in \Omega$) then $\expp{X} \le \expp{Y}$.   
     \item Linearity, for $a,b \in \mathbb{R}$ it holds that $\expp{aX + by} = a\expp{X} + b \expp{Y}$.   
     \item Independently, if $X,Y$ are independent, then $\expp{X\cdot Y} = \expp{X}\cdot \expp{Y}$.  
     \item For any constant $a \in \mathbb{R}$ we have that $\expp{a} = a$. 
   \end{enumerate}
\end{claim}

\begin{proof} 
  \begin{enumerate}
\item Monotonic, if $X \le Y$ then : 
  \begin{equation*}
    \begin{split}
      \expp{X} = \sum_{\omega\in \Omega}{X(\omega)P(\omega)} \le  \sum_{\omega\in \Omega}{Y(\omega)P(\omega)} = \expp{Y}
    \end{split}
  \end{equation*}
\item Linearity,
  \begin{equation*}
    \begin{split}    
      \expp{a X + b Y} &=  \sum_{\omega\in \Omega}{ \left( aX(\omega) + bY(\omega) \right) P(\omega)} \\ 
      &=a\sum_{\omega\in \Omega}{  X(\omega)  P(\omega) } + b \sum_{\omega\in \Omega}{   Y(\omega)  P(\omega)}
    \end{split}
  \end{equation*}
\item Independently, 
  \begin{equation*}
    \begin{split}
      \expp{X Y} &= \sum_{x,y \in \image X \times \image Y}{ xy P(X = x \cap Y = y)} \\
      &= \sum_{x,y \in \image X \times \image Y}{ xy P(X = x )P(Y = y)} \\
      &=\sum_{x \in \image X}{\sum_{y \in \image Y}{ xy P(X = x )P(Y = y)}} \\
      &=\sum_{x \in \image X}{xP(X=x)\sum_{y \in \image Y}{ y P(Y = y)}} \\
      &=\sum_{x \in \image X}{xP(X=x) \expp{Y} }  \\ 
      &=\expp{X} \expp{Y}% \sum_{x \in \image X}{xP(X=x) }  \\ 
    \end{split}
  \end{equation*}
\item Let $X$ be the random variable which is also the constant function $X(\omega) = a$ for any $\omega \in \Omega$. Then we have that
  \begin{equation*}
    \begin{split}
      \expp{X} & = \sum_{\omega\in \Omega}{X(\omega)P(\omega)} \\ & = \sum_{\omega\in \Omega}{a P(\omega)} = a \cdot 1 = a  
    \end{split}
  \end{equation*}
  \end{enumerate}
\end{proof}

\begin{example} Let $X$ be an indicator of event $A$, what are $\expp{X}$ and $\expp{X^{2}}$? Recall that $X(\omega) = 1$ only if $\omega \in A$ and $0$ otherwise, thus: 
  \begin{equation*}
    X^{k}(\omega) = \begin{cases}
      1^{k} = 1 & \omega \in A \\
      0^{k} = 0 & \text{ else }
    \end{cases} \Rightarrow X^{k}(\omega) = X(\omega)
  \end{equation*}
  Therefore, 
  \begin{equation*}
    \begin{split}
      \expp{X^{k}} = \sum_{\omega \in \Omega}{ X^{k}(\omega)  P(\omega)  } = \sum_{\omega \in \Omega}{ X(\omega)  P(\omega)  }=\expp{X} 
    \end{split}
  \end{equation*}
\end{example}
\begin{example}
  
Consider the experiment of throwing keys into cells again. What is the expected number of keys that fell into the same cell as the first key?  The indicator of the event $j$ and $1$ falling into the same cell is given by $\sum_{i}X_{i}^{1}X_{i}^{j}$ and it remains to sum over all the $j$'s. So:
  \marginnote[ Note 1: Despite the ease of computing the expectation, calculating the exact probability of $\sum_{i}\sum_{j}X_{i}^{1}X_{i}^{j} = L$ for some arbitrary $L$ is a difficult task.]{Note 1: Despite the ease of computing the expectation, calculating the exact probability of $\sum_{i}\sum_{j}X_{i}^{1}X_{i}^{j} = L$ for some arbitrary $L$ is a difficult task.}\begin{equation*}
    \begin{split}
      & \expp{ \sum_{i}\sum_{j}X_{i}^{1}X_{i}^{j}} = \sum_{i}\sum_{j}\expp{X_{i}^{1}X_{i}^{j}} \\  
      = &   \sum_{i}\sum_{j\neq 1}\expp{X_{i}^{1}}\expp{X_{i}^{j}} +  \overbrace{\sum_{i}\expp{X_{i}^{1}}}^{ j = 1} \\ 
      = & m \cdot (n-1) \frac{1}{m^{2}} + m\cdot \frac{1}{m} = \frac{n-1}{m} + 1 
    \end{split}
  \end{equation*}
\end{example}
\section{Running Time as a Random Variable.}
Randomness might appears in algoritmic context in two main cases, in the first the algorithm might behave randomly, means that it flips coins and decided what to do conditioning on the outcomes. In the second case, the input might distributed according to probability function. In both cases the result and running time of the algorithm are random variable. And it's interesting to ask what is the expected running time.

Let us introduce an example for the first case. We are given an array $A_{1}, A_{2}, ..., A_{n}$ and are asked to find the $k$ smallest elements in it. Here, we are going to suggest a random algorithm that is expected to return in linear time, even if we do not make any assumptions about the input, particularly how it is distributed.



%For example, consider an array $A$ such any number of it is sample uniformly from $[0,1]$ which mean that probabiliy of $A_{i}$ to belong to segment $I \subset [0,1]$ is exactly: 
%\begin{equation*}
%  \begin{split}
%    P(A_{i} \in I) = |I|
%  \end{split}
%\end{equation*}
%The following algorithm takes advantage of sorting $A$. It separates the range $[0,1]$ into $n$ buckets $B_{1}..B_{n}$, where $B_{i}$ is associated with all the numbers in the range $[i/n, (i+1)/n)$. First, it inserts any element $A_{i}$ into the $B_{ \lfloor n A_{i} \rfloor} ]$ and then sorts each of the buckets separately (using some naive $\Theta(n^2)$-sort).
%

\begin{algorithm}
  \KwResult{returns the $k$ smallest element in \(A_1 ... A_n \in \mathbb{R}^n \)  }
\If{ left $=$ right} { 
  \Return $A_{\text{left}}$
  }
  pivot $\leftarrow$ select random pivot in [left, right]\\
  pivot $\leftarrow$ partition($A$, left, right, pivot) \\
  \If{ $k$ = pivot } {
    \Return $A_{k}$
  }
  \ElseIf{ $k < $  pivot  }{
  right $\leftarrow$ pivot - 1
  }
  \Else{
  left $\leftarrow$ pivot + 1\\
  $k \leftarrow k - $ pivot      

  }
  \Return call select($A$, left, right,  $k$)

  \caption{select($A$, left, right,  $k$)}
\end{algorithm}
Consider the first call to 'select' and let $X_m$ be the indicator for selecting the index of the $m$th smallest number on line (4). Notice that $X_m$ and the running time of the recursive calls are independent random variables. Additionally, we will assume in induction that $T(n', k) \le 2cn'$ for any $n' < n$. Therefore, the expected running time is:
\begin{equation*}
  \begin{split}
    T(n,k) &= c\cdot n + \sum_{m < k}{ X_{m} \cdot T(n-m, k -m)} \\ 
    & \ \ \ \ + X_{k} \cdot 1  + \sum_{m > k}{ X_{m} \cdot T(m-1, k)} \\ 
     \Rightarrow \expp{T(n,k)} \le & cn +2c  + \sum_{m < k}{ \expp{X_{m} \cdot T(n-m, k -m)}} \\
    & \ \ \ \  \  + \sum_{m > k}{\expp{ X_{m} \cdot T(m-1, k) }} \\ 
    & \le c\cdot n + 2c +  2c \sum_{m < k}{ \frac{n-m}{n} }  + 2c\sum_{m > k}{ \frac{m-1}{n}} \\
    & \le c\cdot n + 2c +  2c \sum_{m < k}{ \frac{n-m}{n} }  + 2c\sum_{m > k}{ \frac{m}{n}} 
  \end{split}
\end{equation*}

\begin{claim}
  The sum above is maximal when $k = \lfloor n/2 \rfloor $. 
\end{claim}
\begin{proof}
  We will prove that for $k=i+1$, the sum is greater than $k=i$ if $i<\lfloor n/2 \rfloor$. Denote $S_{i} =\sum_{m < i}{ \frac{n-m}{n} }  + \sum_{m > i}{ \frac{m}{n}}$. Then, the substitution of $S_{i+1} - S_{i}$ becomes:
  \begin{equation*}
    \begin{split}
      S_{i+1} - S_{i} = \frac{n-i - 1}{n} - \frac{i}{n} = \frac{n-2i-1}{n}
    \end{split}
  \end{equation*}
  And that quantity is positive for any $i < \lfloor n/2 \rfloor$. By symmetry, we obtain that for any $i > \lceil n/2 \rceil + 1$, the quantity $S_i - S_{i+1}$ is positive. Hence, $\lfloor n/2 \rfloor$ is a global maximum.
\end{proof}
Therefore, the expectation is bounded by:
\begin{equation*}
  \begin{split}
    \le & c\cdot n + 2c +  2c \sum_{m < \lfloor n/2 \rfloor }{ \frac{n-m}{n} }  + 2c\sum_{m > \lfloor n/2 \rfloor }{ \frac{m}{n}} \\ 
    = &  c\cdot n + 2c +  2 \cdot  2c\sum_{m > \lfloor n/2 \rfloor }{ \frac{m}{n}} \\ 
    = &  c\cdot n + 2c +  2 \cdot  2c \cdot \frac{  (n/2) \cdot (n/2 - 1)  }{2n} \\ 
    \le & cn + 2c + n/2 \cdot 2c  \le 2c \cdot n
  \end{split}
\end{equation*}

%\begin{algorithm}
%    	let B[0 : n – 1] be a new array \\
%	\For{ $i \leftarrow [0, n – 1]$}{
%	   make $B_{i}$ an empty list
%       	}
%	\For{ $i \leftarrow [1, n]$}{
%	    insert $A_{i}$ into list $B_{ \lfloor n A_{i} \rfloor} ]$
%       	}
%	\For{ $i \leftarrow [0, n – 1]$}{
%	    sort list $B_{i}$
%       	}
%	concatenate the lists $B_{0}, B_{1}, .. , B_{n – 1}$ together and\\
%	return the concatenated lists
%\caption{bucket-sort($A$, $n$)}
%  \end{algorithm} 
%  Denote by $X_{i} : [n] \rightarrow [n]$ the random variable that counts the number of elements fallen in the $i$th bucket, and by $X_{i}^{j}$ the indicator that $A_{i}\in B_{i}$, So $P(X_{i}^{j}=1) = P(A_{i} \in B_{j}) = |[j/n, (j+1)/n)|=1/n$. The expectation of the sorting running time is:
%  \begin{equation*}
%    \begin{split}
%    \expp{T} &= \expp{  \text{ Inserting into buckets  }   +   \sum_{i} { \text{Sorting $i$th bucket  } }} \\ 
%    & = \expp{ \Theta(n) +   \sum_{i} { X_{i}^{2}  }} = \Theta(n) +  \sum_{i}{ \expp{X_{i}^{2}} }  \\
%  \expp{X_{i}^{2}} &= \expp{\left( \sum X_{i}^{j} \right)^{2}} = \expp{\sum_{j,j^{\prime}} X_{i}^{j} X_{i}^{j^{\prime}} } =  \sum_{j, j^{\prime}} \expp{X_{i}^{j} X_{i}^{j^{\prime}}} \\ 
%  & = \sum_{j \neq j^{\prime}} \expp{X_{i}^{j} X_{i}^{j^{\prime}}} + \sum_{j} \expp{X_{i}^{j} X_{i}^{j}} \\
%    & =  \sum_{j \neq j^{\prime}} \expp{X_{i}^{j} X_{i}^{j^{\prime}}} + \sum_{j} \expp{X_{i}^{j}}  \\
%        & = 2 { n \choose 2  } \left( \frac{1}{n} \right)^{2} +  n\cdot \frac{1}{n} \\ 
%        & = \frac{n-1}{n} + 1  = 2 - \frac{1}{n} \Rightarrow \expp{T} = \Theta(n) + n\left( 2 - \frac{1}{n} \right) = \Theta(n)
%    \end{split}
%  \end{equation*}




\newcommand*{\SOLUTION}{}
\input{./ex6-2024.tex}
\chapter{Heaps - Recitation 4} 
\author{Correctness, Loop Invariant And Heaps.}


%\begin{paragraph}
  Apart from quantifying the resource requirement of our algorithms, we are also interested in proving that they indeed work. In this Recitation, we will demonstrate how to prove correctness via the notation of loop invariant. In addition, we will present the first (non-trivial) data structure in the course and prove that it allows us to compute the maximum efficiently.

%\end{paragraph}


\subsection*{Correctness And Loop Invariant.}
In this course, any algorithm is defined relative to a task/problem/function, And it will be said correctly if, for any input, it computes desirable output. For example, suppose that our task is to extract the maximum element from a given array. 
So the input space is all the arrays of numbers, and proving that a given algorithm is correct requires proving that the algorithm's output is the maximal number for an arbitrary array. Formally:  
\begin{defbox}{Correctness.}
We will say that an algorithm \( \mathcal{A}\) (an ordered set of operations) computes \( f:D_1 \rightarrow D_2 \) if for every \(x \in D_1 \Rightarrow f(x) = \mathcal{A}(x)\). Sometimes when it's obvious what is the goal function \(f\), we will abbreviate and say that \( \mathcal{A}\) is correct.       
\end{defbox}
\paragraph{}
Other functions \(f\) might be including any computation task: file saving, summing numbers, posting a message in the forum, etc. Let's dive back into the maximum extraction problem and see how one should prove correctness in practice.     
\paragraph{Task: Maximum Finding.} \textit{Given $n\in \mathbb{N}$ numbers $a_1, a_2, \cdots a_n \in \mathbb{R}$ write an Algorithm that returns their maximum.} 

Consider the following suggestion. How would you prove it correct?  

%{Maximum finding.}
\begin{algorithm*}[H]
% \SetAlgoLined
 \SetKwInOut{Input}{input}
 \Input{ Array  $ a_1, a_2, .. a_n $  }
 let \(b \leftarrow a_1 \) \\ 
 \For{\(i \in [2, n] \) } { 
        \(b \leftarrow \max \left(b, a_i \right) \)
    } 
 return \( b \) 
 %\caption{maximum alg.}
\end{algorithm*}

Usually, it will be convenient to divide the algorithms into subsections and then characterize and prove their correctness separately. One primary technique is using the notation of Loop Invariant. Loop Invariant is a property that is characteristic of a loop segment code  and satisfies the following conditions:
\begin{defbox}{Loop Invariant.} 
\begin{enumerate}
  \item Initialization. The property holds (even) before the first iteration of the loop.    
    \item Conservation. As long as one performs the loop iterations, the property still holds.
    \item Termination. Exiting from the loop carrying information.
\end{enumerate}
\end{defbox}

Let's denote by $b^{(i)}$ the value of $b$ at line number $2$ at the $i$th iteration for $i\ge2$ and define $b^{(1)}$ to be its value in its initialization. What is the Loop Invariant here? \textbf{Claim.} \textit{"at the \(i\)-th iteration, $b^{(i)} = \max{ \{ a_1 ... a_{i} \} } $"}. 
\paragraph{Proof.} Initialization, clearly, $ b^{(1)} = a_{1} = \max{ \{ a_1 \} } $. Conservation, by induction, we have the base case from the initialization part, assume the correctness of the claim for any $i^\prime < i$, and consider the $i$th iteration (of course, assume that $i<n$). Then:  
\begin{equation*}
  \begin{split}
b^{(i)} = \max{ \{ b^{(i-1)}, a_{i} \} } = \max{ \{ \max{ \{ a_1, .. a_{i-2}, a_{i-1} \} }, a_{i} \} } = \max{ \{  a_{1}, .. a_{i} \} }
  \end{split}
\end{equation*} 
And that completes the Conservation part. Termination, followed by the conservation, at the $n$ iteration, $b^{(i)}$ is seted to $\max{ \{ a_1 ,a_2 .. a_n  \}}$. 

\paragraph{Task: Element finding.}  \textit{Given $n\in \mathbb{N}$ numbers $a_1, a_2, \cdots a_n \in \mathbb{R}$ and additional number $x \in \mathbb{R}$ write an Algorithm that returns $i$ s.t $a_{i} = x$ if there exists such $i$ and} False \textit{otherwise.} 


%{Element finding.}
\begin{algorithm}[H]
\SetKwInOut{Input}{input}
 \Input{ Array  $ a_1, a_2, .. a_n $  }
% \SetAlgoLined
 \For{ \(i \in [n] \) } { 
	\If { \(a_{i} = x\) }{
	  return \(i, a_{i}\)
        }
    } 
    return False 
\end{algorithm}

\paragraph{Correctness Proof.} First, let's prove the following loop invariant. 
\subparagraph{Claim} \textit{Suppose that the running of the algorithm reached the i-th iteration, then $x \notin \{ a_{1} .. a_{i-1} \}$.} 
\textbf{Proof.} Initialization, for $i=1$ the claim is trivial. Let's use that as the induction base case for proving Conservation. Assume the correctness of the claim for any $i^{\prime} < i$. And consider the $i$th iteration. By the induction assumption, we have that $x \notin \{a_1 .. a_{i-2} \} $, and by the fact that we reached the $i$th iteration, we have that in the $i-1$ iteration, at the line (2) the conditional weren't satisfied (otherwise, the function would return at line (3) namely $x \neq a_{i-1}$. Hence, it follows that $ x \notin \{ a_1, a_2 .. a_{i-2}, a_{i-1} \} $.     
  \subparagraph{} Separate to cases. First, consider the case that given the input $a_1 .. a_n$, the algorithm return False. In this case, we have by the termination property that $x \notin \{ a_1 .. a_n \} $. Now, Suppose that the algorithm returns the pair $\left( i, x \right)$, then it means that the conditional at the line (2) was satisfied at the $i$th iteration. So, indeed $a_{i} = x$, and the algorithm returns the expected output.        


  \newpage


  \paragraph{Leading Problem.} find $k$ largest elements. \ctt{ Next year present the finding $k$-largest elements in $\Theta(n +k\log k)$ time as the goal of the recitation. } 
\begin{defbox}{Heap}
  Let $n \in \mathbb{N}$ and consider the sequence $H = H_{1}, H_{2} \cdots H_{n} \in \mathbb{R} \left( * \right)$. we will say that $H$ is a Heap if for every $i \in [n]$ we have that: $H_{i} \le H_{2i}, H_{2i + 1}$ when we think of the value at indices greater than $n$ as $H_{i>n} = -\infty$. 
  \begin{equation*}
    \begin{split}
      \Leftrightarrow
    \end{split}
  \end{equation*} 
That definition is equivalent to the following recursive definition: Consider an almost complete binary tree where each node is associated with a number. Then, we will say that this binary tree is a heap if the root's value is lower than its children's values, and each subtree defined by its children is also a heap. There is a one-to-one mapping between these definitions by setting the array elements on the tree in order.
\end{defbox}

\input{tree-1.tex}
\paragraph{Checking vital signs.}Are the following sequences are heaps? 
\begin{enumerate}
  \item 1,2,3,4,5,6,7,8,9,10 (Y)
  \item 1,1,1,1,1,1,1,1,1,1  (Y)
  \item 1,4,3,2,7,8,9,10     (N)
  \item 1,4,2,5,6,3	     (Y)
\end{enumerate}
\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
	\input{tree-2.tex}
  \end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
	\input{tree-3.tex}
  \end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
	\input{tree-4.tex}
  \end{subfigure}
  \caption{The trees representations of the heaps above. The node which fails to satisfy the Heap inequality is colorized.}
\end{figure}
How much is the cost (running time) to compute the min of $H$? (without changing the heap). ($O\left( 1 \right)$). Assume that option 4 is our Superpharm Line. Let's try to imagine how we should maintain the line. After serving the customer at the top, what can be said on $ \{ H_{2}, H_{3}\}$? or $\{H_{i>3}\}?$ (the second highest value is in $\{H_{2}, H_{3} \}$.)   
\paragraph{Subtask: Extracting Heap's Minimum.} \textit{Let $H$ be an Heap at size $n$, Write algorithm which return $H_1$, erase it and returns $H^\prime$, an Heap which contain all the remain elements.} 
\textbf{Solution:} 


%{Extract-min.}
\begin{algorithm}[H]
\SetKwInOut{Input}{input}
 \Input{ Heap  $ H_1, H_2, .. H_n $  }
% \SetAlgoLined
ret $\leftarrow H_{1} $ \\
$ H_{1} \leftarrow H_{n} $  \\
$ n \leftarrow n -1 $ \\
Heapify-down$\left( 1 \right)$ \\
return ret  
\end{algorithm}





%{Heapify-down.}
\begin{algorithm}[H]
\SetKwInOut{Input}{input}
 \Input{ Array  $ a_1, a_2, .. a_n $  }
% \SetAlgoLined
next  $\leftarrow i  $ \\
left  $\leftarrow 2i $ \\
right $\rightarrow 2i +1 $ \\ 
\If{ left $ < n \text{ and }  H_{\text{left}} < H_{\text{next}}$ } {
  next $\leftarrow$ left 
}
\If{ right $ < n \text{ and }  H_{\text{right}} < H_{\text{next}}$ } {
  next $\leftarrow$  right
}
\If{ $ i \neq $ next } {
  $ H_{i} \leftrightarrow H_{\text{next}} $ \\ 
  Heapify-down$\left( \text{next}  \right)$
}
\end{algorithm}


 

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.23\textwidth}
	\input{tree-41.tex}
  \end{subfigure}
\begin{subfigure}[b]{0.23\textwidth}
	\input{tree-42.tex}
  \end{subfigure}
\begin{subfigure}[b]{0.23\textwidth}
	\input{tree-43.tex}
  \end{subfigure}
\begin{subfigure}[b]{0.23\textwidth}
	\input{tree-44.tex}
  \end{subfigure}
  \caption{Running Example, Extract.} 
\end{figure}


\paragraph{Claim.} Assume that $H$ satisfies the Heap inequality for all the elements except the root. Namely for any $i \neq 1$ we have that $H_{i} \le H_{2i}, H_{2i+1}$. Then applying Heapify-down on $H$ at index $1$ returns a heap.  
\paragraph{Proof.} By induction on the heap size.  
 
\begin{itemize} 
  \item Base, Consider a heap at the size at most three and prove for each by considering each case separately. (lefts as exercise).  
  \item Assumption, assume the correctness of the Claim for any tree that satisfies the heap inequality except the root, at size $n^{\prime} < n$. 
  \item Induction step. Consider a tree at size $n$ which and assume w.l.g (why could we?) that the right child of the root is the minimum between the triple. Then, by the definition of the algorithm, at line 9, the root exchanges its value with its right child. Given that before the swapping, all the elements of the heap, except the root, had satisfied the heap inequality, we have that after the exchange, all the right subtree's elements, except the root of that subtree (the original root's right child) still satisfy the inequality. As the size of the right subtree is at most $n-1$, we could use the assumption and have that after line (10), the right subtree is a heap.  
 
    Now, as the left subtree remains the same (the values of the nodes of the left side didn't change), we have that this subtree is also a heap. So it's left to show that the new tree's root is smaller than its children's. Suppose that is not the case, then it's clear that the root of the right subtree (heap) is smaller than the new root. Combining the fact that its origin must be the right subtree, we have a contradiction to the fact that the original right subtree was a heap (as its root wasn't the minimum element in that subtle).  
 
\end{itemize} 
\paragraph{Question.} How to construct a heap? And how much time will it take?   

%{Build.}
\begin{algorithm}[H]
  \SetKwInOut{Input}{input}
% \SetAlgoLined
  \Input{ Array $ H = H_{1} .. H_{n} $ }
  $ i \leftarrow \lfloor \frac{1}{2}n  \rfloor $ \\
  \While { $ i > 1 $ }
  { 
    Heapify-down $\left( H, i \right)$ \\ 
    $ i \leftarrow i - 1 $  
  }
return $H_{1} .. H_{n}$
\end{algorithm}

We can compute a simple upper bound on the running time of Build as follows. Each call to Heapify costs $O(\log n)$ time, and Build makes $O(n)$ such calls. Thus, the running time is $O(n \log n)$. This upper bound, though correct, is not as tight as it can be.

Let's compute the tight bound. Denote by $U_h$ the subset of vertices in a heap at height $h_{i}$. Also, let $c > 0 $ be the constant quantify the work that is done in each recursive step, then we can express the total cost as being bonded from above by: 

\begin{equation*}
  \begin{split}
    T\left( n \right) & \le \sum_{ h_{i} =1}^{ \log n }{c \cdot \left( \log n -  h_{i} \right)  |U_{h_{i}}|   } 
  \end{split}
\end{equation*}

By counting arguments, we have the inequality $ 2^{\log n - h_{i}}|U_{i}| \le n $ (Proving this argument is left as an exercise). We thus obtain:  

\begin{equation*}
  \begin{split}
    T\left( n \right)  & \le  \sum_{ h_{i} =1}^{ \log n }{c \cdot \left( \log n -  h_{i} \right) \frac{n}{2^{log n - h_{i}} }} = c n \sum_{ j = 1}^{ \log n }{ 2^{-j} \cdot j  }  \\ 
      \le &  c n \sum_{ j = 1}^{ \infty }{ 2^{-j} \cdot j  } 
  \end{split}
\end{equation*}
And by the Ratio test for inifinte serires $ \lim_{j\rightarrow \infty} \frac{(j+1)2^{-j-1}}{j2^{-j}} \rightarrow \frac{1}{2}$ we have that the serire covergence to constant. Hence: $ T\left( n \right) = \Theta\left( n \right) $. 


\paragraph{Heap Sort.}   
Combining the above, we obtain a new sorting algorithm. Given an array, we could first Heapify it (build a heap from it) and then extract the elements by their order. As we saw, the construction requires linear time, and then each extraction costs $\log n$ time. So, the overall cost is $O\left( n\log n  \right)$. Correction follows immediately from Build and Extract correction.  

%{Heap-Sort.}
\begin{algorithm}[H]
\SetKwInOut{Input}{input}
 \Input{ Array  $ H_1, H_2, .. H_n $  }
% \SetAlgoLined
  $H \leftarrow $ build $ \left( x_1, x_2 .. x_{n}  \right) $ \\ 
  ret $ \leftarrow $ Array $ \{ \} $ \\
  \For {  $ i \in [n] $ } {
  ret$_{i}$ $\leftarrow$ extract $H$
  }
  return ret. 
\end{algorithm}


\paragraph{Adding Elements Into The Heap.} (Skip if there is no time).


%{Heapify-up.}
\begin{algorithm}[H]
  \SetKwInOut{Input}{input}
 \Input{ Heap  $ H_1, H_2, .. H_n $  }
% \SetAlgoLined
parent $\leftarrow \lfloor i/2 \rfloor $ \\
\If{ \text{parent} $  > 0 \text{ and }  H_{\text{parent}} \le H_{i}$ } { 
  $ H_{i} \leftrightarrow H_{\text{parent}} $ \\ 
  Heapify-up$\left( \text{parent}  \right)$
}
\end{algorithm}





%{Insert-key.}
\begin{algorithm}[H]
\SetKwInOut{Input}{input}
 \Input{ Heap  $ H_1, H_2, .. H_n $  }
% \SetAlgoLined
$ H_{n} \leftarrow v $ \\ 
Heapify-up$\left( n \right)$\\
$ n \leftarrow n + 1 $ 
\end{algorithm}




\newpage 
\subsection*{Example, Median Heap}

\paragraph{Task:}Write a datastructure that support insertion and deltion at $O\left( \log n \right) $ time and in addition enable to extract the median in $O\left( \log n  \right)$ time. 

\paragraph{Soultlion.} We will define two separate Heaps, the first will be a maximum heap and store the first $ \lfloor n/2 \rfloor $ smallest elements, and the second will be a minimum heap and contain the $ \lceil n/2 \rceil$ greatest elements. So, it's clear that the root of the maximum heap is the median of the elements. Therefore to guarantee correctness, we should maintain the balance between the heap's size. Namely, promising that after each insertion or extraction, the difference between the heap's size is either $0$ or $1$.


%{Median-Insert-key.}
\begin{algorithm}[H]  
\SetKwInOut{Input}{input} 
\Input{ Array  $ H_1, H_2, .. H_n , v $  } 
\If {$ H_{\max, 1} \le v \le  H_{\min, 1}$ } {
	\If{ size $(H_{\max}) - $ size$(H_{\min}) = 1$} {
       Insert-key ( $H_{min}$, $v$ )
    }
    \Else{
	    Insert-key ( $H_{max}$, $v$ )
    }
}
\Else{
median $\leftarrow$ Median-Extract $H$ \\
\If{ $v < $ median  }{
   Insert-key ( $H_{max}$, $v$ ) \\
   Insert-key ( $H_{min}$, $median$ ) \\
}
\Else{
   Insert-key ( $H_{min}$, $v$ ) \\
   Insert-key ( $H_{max}$, $median$ ) \\
}
}
\end{algorithm}



%{Median-Extract.}
\begin{algorithm}[H]
\SetKwInOut{Input}{input}
 \Input{ Array  $ H_1, H_2, .. H_n $  }
% \SetAlgoLined
median $\leftarrow$ extract $H_{max}$ \\   
\If{ size($H_{min}$) - size($H_{max}$) $> 0$ }{
  temp $ \leftarrow $ extract $H_{\min}$ \\
  Insert-key ( $H_{max}$, temp )    \\
}
return median 
\end{algorithm}


\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
	\input{tree-r2.tex}
  \end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
	\input{tree-r1.tex}
  \end{subfigure}
  \caption{ Example for Median-Heap, the left and right trees are maximum and minimum heaps.  }
\end{figure}

\iffalse 
  \newpage

\section{ Appendix. Exercise from last year }

\paragraph{Question.} Consider the sets $X = \{x_1,x_2 .. x_n\}$, $Y = \{y_1, y_2 .. y_n\}$. Assume that each of the values $x_i,y_i$ is unique. Write an Algorithm which compute the $k$ most small items in $X \oplus Y = \{ x_{i} + y_{j} : x_{i} \in X , y_{j} \in Y  \} $ at $ O \left( n + k\log k  \right) $ time. 

\textbf{Solution.} Notice that If $a \in X$ is greater than $i$ elements of $X$ and $b \in Y$ greater than $j$ elements of $Y$. Then, $a + b$  greater than $i\cdot j$ elements of $X \oplus Y$. Denote by $X^\prime = \{ x^{\prime}_{1} .. x^{\prime}_{n}$ ( $Y^{\prime}$ ) The elements of $X$ in sorted order. So it's clear that if $x_{i}+y_{j} = x^{\prime}_{i^{\prime}} + y^{\prime}_{j^{\prime}}$ is one of the $k$ smallest elements of $X\oplus Y$ then $i^{\prime}j^{\prime} \ge k$. So we will create a heap of elements that respect that inequality and then query that heap.


%{Heappush.}
\begin{algorithm}[H]
% \SetAlgoLined
$ H_{X} \leftarrow $ build $\left( X \right)$  \\ 
$ H_{Y} \leftarrow $ build $\left( Y \right)$  \\
$ S_{X} \leftarrow $ extract-$k$ $\left( H_{X} \right)$  \\ 
$ S_{Y} \leftarrow $ extract-$k$ $\left( H_{Y} \right)$  \\
$ H_{XY} \leftarrow $ Heap $(\{ \} )$ \\
\For{ $i \in [k]$ } {
  \For { $j \in [k/i]$ } {
  	Heappush( $H_{XY}$, $S_{X,i} + S_{Y, j}$ )    
  }
}
return extract-$k$ ( $H_{XY}$ ) 
\end{algorithm}

\fi



\chapter{Quicksort And Liner Time Sorts  - Recitation 6} 
\author{Quicksort, Countingsort, Radixsort, And Bucketsort.}


%\begin{paragraph}
Till now, we have quantified the algorithm performance against the worst-case scenario. And we saw that according to that measure, In the comparisons model, one can not sort in less than $\Theta\left( n\log n \right) $ time. In this recitation, we present two new main concepts that, in some instances, achieve better analyses. The first one is the Exception Complexity; By Letting the algorithm behave non-determinately, we might obtain an algorithm that most of the time runs to compute the task fast. Yet we will not success get down beneath the $\Theta\left(n\log n\right)$ lower bound, but we will go back to use that concept in the pending of the course. The second concept is to restrict ourselves to dealing only with particular inputs. For example, We will see that if we suppose that the given array contains only integers in a bounded domain, then we can sort it in linear time.  

%\end{paragraph}

\subsection*{Quicksort.}
The quicksort algorithm is a good example of a \textbf{non-determistic} algorithm that has a worst-case running time of $\Theta\left(n^{2}\right)$. Yet its expected running time is $\Theta\left(n\log n\right)$. Namely, fix an array of $n$ numbers. The running of Quicksort over that array might be different. Each of them is a different event in probability space, and the algorithm's running time is a random variable defined over that space. Saying that the algorithm has the worst space complexity of $\Theta(n^{2})$ means that there exists an event in which it runs $\Theta\left(n^{2}\right)$ time with non-zero probability. But practically, the interesting question is not the existence of such an event but how likely it would happen. It turns out that the expectation of the running time is $\Theta\left(n\log n\right)$.  

What is the exact reason that happens? By giving up on the algorithm behavior century, we will turn the task of engineering a bad input impossible.    
 

%{randomized-partition$(A, p, r)$}
  \begin{algorithm}[H]
      $i \leftarrow $ random $(p, r)$ \\
      $A_{r} \leftrightarrow A_{i} $ \\
      return Partition $(A, p, r)$
    \end{algorithm}

 
%{randomized-quicksort $(A, p, r)$}
      \begin{algorithm}[H]
	\If{ $p < r$ }{
	  $q \leftarrow $ randomized-partition $(A, p, r)$ \\
	  randomized-quicksort $(A, p, q-1)$\\
	  randomized-quicksort $(A, q+1, r)$
	}
      \end{algorithm}



\paragraph{Partitioning.}
To complete the correctness proof of the algorithm (most of it passed in the Lecture), we have to prove that the partition method is indeed rearranging the array such that all the elements contained in the right subarray are greater than all the elements on the left subarray.  

%which rearranges the subarray $ A_{p : r}$ in place, returning the index of the dividing point between the two sides of the partition.


%{Partition$(A, p, r)$}
  \begin{algorithm}[H]
    $ x \leftarrow A_{r} $ \\
    $ i \leftarrow p - 1 $ \\
    \For{ $j \in [p, r-1]$ }{
      \If{ $ A_{j} \le x  $}{
	$ i \leftarrow i + 1 $ \\
	$ A_{i} \leftrightarrow A_{j} $\\
      }
    }
   $ A_{i+1} \leftrightarrow A_{r} $\\
   return $ i+1$
  \end{algorithm}


\paragraph{claim.} At the beginning of each iteration of the loop of lines 3–6, for any array index $k$, the following conditions hold:
\begin{itemize}
  \item  if $p \le k \le i$, then $A_{k} \le x$.
  \item  if $i + 1 \le k \le j – 1$, then $A_{k} > x$.
  \item  if $k = r$, then $A_{k} = x$.
\end{itemize}
\paragraph{Proof.}
\begin{enumerate}
  \item Initialization: Prior to the first iteration of the loop, we have $i = p – 1$ and $ j= p$. Because no values lie between $p$ and $i$ and no values lie between $i + 1$ and $j – 1$, the first two conditions of the loop invariant are trivially satisfied. The assignment in line 1 satisfies the third condition.
  \item Maintenance: we consider two cases, depending on the outcome of the test in line 4, when $A_{j} > x$: the only action in the loop is to increment $j$. After $j$  has been incremented, the second condition holds for $A_{j – 1}$, and all other entries remain unchanged. When $A_{j} \le x$: the loop increments $i$, swaps $A_{i}$ and $A_{j}$, and then increments $j$. Because of the swap, we now have that $A_{i} \le x$, and condition 1 is satisfied. Similarly, we also have that $A_{j – 1} > x$, since the item that was swapped into $A_{j – 1}$ is, by the loop invariant, greater than $x$.
  \item Termination: Since the loop makes exactly $r – p$ iterations, it terminates, whereupon$ j= r$. At that point, the unexamined subarray $A_{j}, A_{j+1}..  A_{r – 1}$ is empty, and every entry in the array belongs to one of the other three sets described by the invariant. Thus, the values in the array have been partitioned into three sets: those less than or equal to $x$ (the low side), those greater than $x$ (the high side), and a singleton set containing $x$ (the pivot).


\end{enumerate}

\input{quick-1.tex}

\subsection*{Linear Time Sorts}
\paragraph{ Counting sort.} Counting sort assumes that each of the n input elements is an integer at the size at most $k$. It runs in $\Theta \left(n + k\right)$ time, so that when $k = O(n)$, counting sort runs in $\Theta\left(n\right)$ time.
Counting sort first determines, for each input element $x$, the number of elements less than or equal to $x$. It then uses this information to place element $x$ directly into its position in the output array. For example, if 17 elements are less than or equal to $x$, then $x$ belongs in output position 17. We must modify this scheme slightly to handle the situation in which several elements have the \textbf{same value}, since we do not want them all to end up in the same position.


%{Counting-sort$(A, n, k)$}
  \begin{algorithm}[H]
  	let B and C be new arrays at size $n$ and $k$ \\ 
  	\For{ $i \in  [0, k]$}{
		$C_{i} \leftarrow 0 $
  	}
	\For{ $j \leftarrow [1, n]$}{
	  $C_{A{j}} \leftarrow C_{A_{j}} + 1 $
 	}
	\For{ $i \in  [1, k]$}{
	  $C_{i} \leftarrow C_{i} + C_{i – 1} $
 	}
	\For{ $i \in [n , 1]$}{
	  $B_{C_{A_{j}}} \leftarrow A_{j}$ \\
	  $C_{A_{j}} \leftarrow C_{A{j}} – 1$ \ \ // to handle duplicate values
	}
	  return $B$
  \end{algorithm}


Notice that the Counting sort can beat the lower bound of $\Omega\left(n \log n\right)$ only because it is not a comparison sort. In fact, no comparisons between input elements occur anywhere in the code. Instead, counting sort uses the actual values of the elements to index into an array.

An important property of the counting sort is that it is \textbf{stable}.

\begin{defbox}{Stable Sort.}
 We will say that a sorting algorithm is stable if elements with the same value appear in the output array in the same order as they do in the input array. \end{defbox}

Counting sort's stability is important for another reason: counting sort is often used as a subroutine in radix sort. As we shall see in the next section, in order for radix sort to work correctly, counting sort must be stable.

\paragraph{Radix sort}
Radix sort is the algorithm used by the card-sorting machines you now find only in computer museums. The cards have 80 columns, and in each column, a machine can punch a hole in one of 12 places. The sorter can be mechanically "programmed" to examine a given column of each card in a deck and distribute the card into one of 12 bins depending on which place has been punched. An operator can then gather the cards bin by bin, so that cards with the first place punched are on top of cards with the second place punched, and so on.

The Radix-sort procedure assumes that each element in the array $A$ has d digits, where digit 1 is the lowest-order digit and digit $d$ is the highest-order digit.



%{radix-sort(A, n, d)}
  \begin{algorithm}[H]
    \For{ $ i \in [1,d]$ } {
        use a stable sort to sort array $A$ on digit $i$
    }
  \end{algorithm}


\paragraph{Correctness Proof.} By induction on the column being sorted.
\begin{itemize}
  \item Base. Where $d = 1$, the correctness follows immediately from the correctness of our base sort subroutine. 
  \item Induction Assumption. Assume that Radix-sort is correct for any array of numbers containing at most $d-1$ digits. 
  \item Step. Let $A^{\prime}$  be the algorithm output. Consider $x,y \in A$. Assume without losing generality that $x > y$. Denote by $x_{d}, y_{d}$ their $d$-digit and by $x_{/d}, y_{/d}$ the numbers obtained by taking only the first  $d-1$ digits of $x,y$. Separate in two cases:

    \begin{itemize}
      \item   If $x_{d} > y_{d}$ then a scenario in which $x$ appear prior to $y$ is  imply contradiction to the correctness of our subroutine.
      \item   So consider the case in which $x_{d} = y_{d}$. In that case, it must hold that $x_{/d} > y_{/d}$. Then the appearance of $x$ prior to $y$ either contradicts the assumption that the base algorithm we have used is stable or that $x$ appears before $y$ at the end of the $d-1$ iteration. Which contradicts the induction assumption. 
    \end{itemize}
 \end{itemize}

The analysis of the running time depends on the stable sort used as the intermediate sorting algorithm. When each digit lies in the range $0$ to $k – 1$ (so that it can take on $k$ possible values), and $k$ is not too large, counting sort is the obvious choice. Each pass over $n$ $d$-digit numbers then takes $\Theta(n + k)$ time. There are $d$ passes, and so the total time for radix sort is $\Theta\left(d(n + k)\right)$.

%When d is constant and k = O(n), we can make radix sort run in linear time. More generally, we have some flexibility in how to break each key into digits.
%Lemma 8.4
%Given n b-bit numbers and any positive integer r ≤ b, RADIX-SORT correctly sorts these numbers in $\Theta\left((b/r)(n + 2r)\right)$ time if the stable sort it uses takes Θ(n + k) time for inputs in the range 0 to k.

%Proof For a value r ≤ b, view each key as having d = ⌈b/r⌉ digits of r bits each. Each digit is an integer in the range 0 to 2r – 1, so we can use counting sort with k = 2r – 1. (For example, we can view a 32-bit word as having four 8-bit digits, so that b = 32, r = 8, k = 2r – 1 = 255, and d = b/r = 4.) Each pass of counting sort takes $ \Theta \left(n + k \right) = \Theta\left(n + 2r\right)$ time and there are d passes, for a total running time of $\Theta\left(d(n + 2r)\right) = \Theta\left((b/r)(n + 2r)\right)$.

%Given n and b, what value of r ≤ b minimizes the expression (b/r)(n + 2r)? As r decreases, the factor b/r increases, but as r increases so does 2r. The answer depends on whether b < ⌊lg n⌋. If b < ⌊lg n⌋, then r ≤ b implies (n + 2r) = Θ(n). Thus, choosing r = b yields a running time of (b/b)(n + 2b) = Θ(n), which is asymptotically optimal. If b ≥ ⌊lg n⌋, then choosing r = ⌊lg n⌋ gives the best running time to within a constant factor, which we can see as follows.1 Choosing r = ⌊lg n⌋ yields a running time of Θ(bn/log n). As r increases above ⌊lg n⌋, the 2r term in the numerator increases faster than the r term in the denominator, and so increasing r above ⌊lg n⌋ yields a running time of Ω(bn / log n). If instead r were to decrease below ⌊lg n⌋, then the b/r term increases and the n + 2r term remains at Θ(n).

%Is radix sort preferable to a comparison-based sorting algorithm, such as Quicksort? If b = O(log n), as is often the case, and r ≈ log n, then the radix sort's running time is Θ(n), which appears to be better than Quicksort's expected running time of Θ(n log n). The constant factors hidden in the Θ-notation differ, however. Although radix sort may make fewer passes than Quicksort over the n keys, each pass of radix sort may take significantly longer. Which sorting algorithm to prefer depends on the characteristics of the implementations, of the underlying machine (e.g., Quicksort often uses hardware caches more effectively than radix sort), and of the input data. Moreover, the version of radix sort that uses counting sort as the intermediate stable sort does not sort in place, which many of the Θ(n log n)-time comparison sorts do. Thus, when primary memory storage is at a premium, an in-place algorithm such as Quicksort could be the better choice.


\paragraph{Bucket sort.}
%Bucket sort assumes that the input is drawn from a uniform distribution and has an average-case running time of O(n). Like counting sort, bucket sort is fast because it assumes something about the input. Whereas counting sort assumes that the input consists of integers in a small range, bucket sort assumes that the input is generated by a random process that distributes elements uniformly and independently over the interval [0, 1). (See Section C.2 for a definition of a uniform distribution.)
Bucket sort divides the interval [0, 1) into n equal-sized subintervals, or buckets, and then distributes the n input numbers into the buckets. Since the inputs are uniformly and independently distributed over [0, 1), we do not expect many numbers to fall into each bucket. To produce the output, we simply sort the numbers in each bucket and then go through the buckets in order, listing the elements in each.
%The BUCKET-SORT procedure on the next page assumes that the input is an array A[1 : n] and that each element A[i] in the array satisfies 0 ≤ A[i] < 1. The code requires an auxiliary array B[0 : n – 1] of linked lists (buckets) and assumes that there is a mechanism for maintaining such lists. (Section 10.2 describes how to implement basic operations on linked lists.) Figure 8.4 shows the operation of bucket sort on an input array of 10 numbers.


%{bucket-sort(A, n)}
  \begin{algorithm}[H]
    	let B[0 : n – 1] be a new array
	\For{ $i \leftarrow [0, n – 1]$}{
	   make $B_{i}$ an empty list
       	}
	\For{ $i \leftarrow [1, n]$}{
	    insert $A_{i}$ into list $B_{ \lfloor n A_{i} \rfloor} ]$
       	}
	\For{ $i \leftarrow [0, n – 1]$}{
	    sort list $B_{i}$
       	}
	concatenate the lists $B_{0}, B{1}, .. , B_{n – 1}$ together and\\
	return the concatenated lists
  \end{algorithm}




\chapter{AVL Trees  - Recitation 7} 


%\begin{paragraph}
%\begin{paragraph}
    In this recitation, we will review the new data structures you have seen - Binary search trees and, specifically, AVL trees. We will revise the different operations, review the essential concepts of balance factor and rotations and see some examples. Finally, if there is time left - we will prove that the height of an AVL tree is $O(\log(n))$.
%\end{paragraph}
    
\section{AVL trees}
    Reminders:
    \begin{defbox}{Binary Tree.}
        A binary tree is a tree $(T,r)$ with $r\in V$, such that $\deg(v)
            \leq 2$ for any $v\in V$.
    \end{defbox}
    \begin{defbox}{Height.} A tree's height $h(T)$ (sometimes $h(r)$) is defined to be the
    length of the
    longest
    simple path from $r$ to a leaf.
    \end{defbox}
    \begin{defbox}{ Binary Search Tree. } A binary search tree is a binary tree $(T,r)$ such that for any
    node $x$ (root of a subtree) and a node in that subtree $y$:
    \begin{enumerate}
        \item if $y$ is in the left subtree of $x$ then $y.key \leq x.key$
        \item if $y$ is in the right subtree of $x$ then $x.key < y.key$
    \end{enumerate}
        Note that this is a (relatively) local property.
    \end{defbox}
For example:\\
    \begin{center}
        \begin{tikzpicture}[ level distance=1.5cm,
        level 1/.style={sibling distance=3.5cm},
        level 2/.style={sibling distance=2cm}, scale=.65]
            \node {40}
                child{node{15}
                        child{node{10} child{node{5}} child{node{} edge from parent
            [draw=none]}}
                        child{node{20}}}
                child{node{60}
                        child{node{50}
                                child{node{45}}
                                child{node{55}
                                        child{node{52}}
                                        child{node{} edge from parent [draw=none]}
                                }
                                }
                        child{node{70} child{node{} edge from parent [draw=none]}
                                child{node{80}}}
            };
    \end{tikzpicture}
    \end{center}
\begin{remark} Go over the properties and calculate the tree's height. Make sure you understand the definitions!
\end{remark}

Last time, we saw some operations that can be performed on BSTs, and
proved correctness for some of them. These were: $Search(x), Min(T), Max(T), Pred(x), Succ(x),
Insert(x), Delete(x)$. All of these operations take $O(h(T))$ in the worst case.\\

The main two operations that may cause problems are $Insert$ and $Delete$, as
they change the tree's height (consider inserting $81,82,83,84$ to our
working example).
To address this problem, we introduce another field: for each node $v$, add a field of $h(v)$ = the height of the subtree whose root is $v$. This allows us to maintain the AVL property:
\begin{defbox}{AVL Tree.} An AVL tree is a balanced BST, such that for any node $x$, its left and right subtrees' height differs in no more than $1$. In other words:
        $$|h(left(x)) - h(right(x))|\leq 1$$
    \end{defbox}
    This field allows us to calculate the Balance Factor for each node in $O
    (1)$:
    \begin{defbox}{Balance Factor.} For each node $x\in T$, it's Balance Factor is defined 
    $$hd
    (x) := h(left(x)) - h(right(x))$$ In AVL trees, we would like to maintain
    $|hd(x)| \leq 1$
    \end{defbox}
\begin{example} For our working example, the node $60$'s $hd$ is $h(50) - h
(70) = 1$, and $hd(50) = h(45)- h(55) = -1$. You can check and see that this is an AVL tree.
\end{example}
So to make sure that we can actually maintain time complexity $O(\log(n))$, we would want to:
\begin{enumerate}
    \item Show that for an AVL tree, $h(T) = \theta(log(n))$ (If there's time
    left)
    \item See how to correct violations in AVL property - using \textbf{rotations}
    \item See how to $Delete$ and $Insert$, while maintaining the height field.
\end{enumerate}

%\newpage
\subsection{Rotations}
\begin{figure}[h]
  \centering
\includegraphics[scale=.3]{Recitations/rotations.png}\\
\end{figure}
Rotations allow us to maintain the AVL property in $O(1)$ time (you have
discussed this in the lecture - changing subtree's roots).
In
this
schematic
representation of rotations, $x,y$ are nodes and $\alpha,\beta,\gamma$ are subtrees.
Note that the BST property is maintained! \\

%\includegraphics[scale=.5]{Recitations/rotationEg.png}

The Balance factor allows us to identify if the AVL property was violated, and moreover -
the exact values of the bad Balance factors will tell us which rotations to do to fix
this:\\
Taken from last year's recitation:\\
\includegraphics[scale=.6]{Recitations/violations.png}

Let us analyze one of these cases:\\
\begin{example} Let us see why R rotation Fixes LL violation:\\
\begin{figure}[h]
  \centering
\includegraphics[scale=.5]{Recitations/LLviolation.jpg}\\
This is the general form of LL violations.
\end{figure}

Let us analyze the heights of subtrees. Denote $h$ the height of $A_L$ before inserting $v$. So $A_R$'s height has also to be $h$.
If its height were $h+1$, the insertion would not have created a violation in
$B$ ($A$'s height would have stayed the same). If it were $h-1$, the violation would have appeared first in $A$ (not in $B$). Thus, $A$'s height is $h+1$.
$B_R$'s height is also $h$: If it was $h+1$ or $h+2$, no violation in $B$ had
occurred.

After the rotation, the tree looks like this:
\begin{figure}[h]
  \centering
  \includegraphics[scale=.9]{Recitations/LLviolationfix.jpg}
\end{figure}
So all the nodes here maintain AVL property; why is it maintained?
\end{example}\\
Detect the violation in the following tree, and perform the necessary rotations:\\
\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.31\textwidth}
            \begin{tikzpicture}[ level distance=1.5cm,
        level 1/.style={sibling distance=3.5cm},
        level 2/.style={sibling distance=2cm}, scale=.65]
            \node {15}
	    child{node{5} child{node{3}} child{node{12}  child{node{10}
            child{node{6}} child{node{} edge from parent [draw = none]}}
            child{node{13}}}} child{node{20} child{node{18}} child{node{23}}}
            ;
    \end{tikzpicture}
  \end{subfigure}
\begin{subfigure}[b]{0.31\textwidth}
        \begin{tikzpicture}[ level distance=1.5cm,
        level 1/.style={sibling distance=3.5cm},
        level 2/.style={sibling distance=2cm}, scale=.65]
            \node {15}
	    child{node{} child{node{3}} child{node{10}
            child{node{6}} child{node{12} child{node{} edge
            from parent [draw = none]} child{node{13}} }}
            }
             child{node{20} child{node{18}} child{node{23}}}
            ;
    \end{tikzpicture}
  \end{subfigure}
\begin{subfigure}[b]{0.31\textwidth}
  \begin{tikzpicture}[ level distance=1.5cm,
        level 1/.style={sibling distance=3.5cm},
        level 2/.style={sibling distance=2cm}, scale=.65]
            \node {15}
                child{node{10} child{node{5} child{node{3}} child{node{6}}
            } child{node{12}
            child{node{} edge from parent [draw = none]} child{node{13}}}}
             child{node{20} child{node{18}} child{node{23}}}
            ;
    \end{tikzpicture}

\end{subfigure}
  \caption{ 
The first node in which a violation occurred is $5$, this is an RL violation.
Perform R rotation on the right child. Then perform L rotation on the root of the relevant subtree ($5$):
}
\end{figure}
\subsection{Delete, Insert.}
The principles of the $Delete$ and $Insert$ operations are the same as in
regular BST, but we will need to rebalance the tree in order to preserve AVL
property.
A single insertion or deletion may change the height difference of subtrees
by at most 1, and might affect only the subtrees with roots along the path
from $r$ to the point of insertion/ deletion.
More concretely - we will add a recursive operation of traversing the tree
"back up" and checking violations. Had we found one - we will fix it using
rotations. Since rotations can be done in $O(1)$, the entire correction
process will take $O(\log(n))$, so we maintain a good time complexity.



%{AVL Insert(r,x)}
  \begin{algorithm}[H]
    Call Insert $(r,x)$ // (The standard insertion routine for BST) \\  
    Let $p$ be the new node appended to the tree. \\
    \While{ $p$.parent $\neq \emptyset $ } {
      Check which of the four states we are in.\\ 
      Apply the right rotation. \\ 
      Update the height of each touched vertex  \\
      $p \leftarrow$ $p$.parent \\  
    }
  \end{algorithm}



\iffalse
\newpage
\subsection{Delete, Insert}
The principles of the $Delete$ and $Insert$ operations are the same as in
regular BST, but we will need to rebalance the tree in order to preserve AVL
property.
A single insertion or deletion may change the height difference of subtrees
by at most 1, and might affect only the subtrees with roots along the path
from $r$ to the point of insertion/ deletion.
More concretely - we will add a recursive operation of traversing the tree
"back up" and checking violations. Had we found one - we will fix it using
rotations. Since rotations can be done in $O(1)$, the entire correction
process will take $O(\log(n))$, so we maintain a good time complexity.
\subsubsection{Delete}
This is the regular BST delete. We will need to traverse up the
tree and detect violations (if occurred).\\
\includegraphics[scale=.6]{Recitations/DeleteBST.jpg}\\
\begin{remark} Go over the procedure quickly - explain the 3 cases - no
children, one child (easy), and two children (using successor).
\end{remark}
\begin{example}\\
    Delete $20$ from the tree at the beginning of the recitation:\\
    \includegraphics[scale=.3]{Recitations/DeleteEx.jpg}
\end{example}
%    Consider the original BST we looked at:\\
%
%\begin{tikzpicture}[ level distance=1.5cm,
%        level 1/.style={sibling distance=3.5cm},
%        level 2/.style={sibling distance=2cm}, scale=.65]
%            \node {40}
%                child{node{15}
%                        child{node{10} child{node{5}} child{node{} edge from parent
%            [draw=none]}}
%                        child{node{20}}}
%                child{node{60}
%                        child{node{50}
%                                child{node{45}}
%                                child{node{55}
%                                        child{node{52}}
%                                        child{node{} edge from parent [draw=none]}
%                                }
%                                }
%                        child{node{70} child{node{} edge from parent [draw=none]}
%                                child{node{80}}}
%            };
%    \end{tikzpicture}
%
%Let's $Insert(4)$:\\
%
%\begin{tikzpicture}[ level distance=1.5cm,
%        level 1/.style={sibling distance=3.5cm},
%        level 2/.style={sibling distance=2cm}, scale=.65]
%            \node {40}
%                child{node{15}
%                        child{node{(-2)10} child{node{(-1)5}child{node{4}}
%            child{node{} edge
%            from
%            parent
%            [draw = none]} }
%            child{node{} edge from
%            parent
%            [draw=none]}}
%                        child{node{20}}}
%                child{node{60}
%                        child{node{50}
%                                child{node{45}}
%                                child{node{55}
%                                        child{node{52}}
%                                        child{node{} edge from parent [draw=none]}
%                                }
%                                }
%                        child{node{70} child{node{} edge from parent [draw=none]}
%                                child{node{80}}}
%            };
%    \end{tikzpicture}\\
%
%So we need to fix LL violation. This is by performing R rotation of the root
%of the subtree in which the violation first appeared: \\
%
%\begin{tikzpicture}[ level distance=1.5cm,
%        level 1/.style={sibling distance=3.5cm},
%        level 2/.style={sibling distance=2cm}, scale=.65]
%            \node {40}
%                child{node{15}
%                        child{node{(0)5} child{node{(0)4}} child{node{10}}
%             }
%                        child{node{20}}}
%                child{node{60}
%                        child{node{50}
%                                child{node{45}}
%                                child{node{55}
%                                        child{node{52}}
%                                        child{node{} edge from parent [draw=none]}
%                                }
%                                }
%                        child{node{70} child{node{} edge from parent [draw=none]}
%                                child{node{80}}}
%            };
%    \end{tikzpicture}
%\begin{remark}
%    RR rotations are symmetric
%\end{remark}
\subsubsection{Insert}
Once again - this is the regular BST insert; we will need to traverse up the
tree and detect violations (if occurred).\\
\includegraphics[scale=.6]{Recitations/InsertBST.jpg}\\
\begin{example}
    Insert $65$ tho the previous tree (after the deletion):\\
    \begin{tikzpicture}[ level distance=1.5cm,
        level 1/.style={sibling distance=3.5cm},
        level 2/.style={sibling distance=2cm}, scale=.65]
            \node{50} child{node{40} child{node{10} child{node{5}
            }child{node{15}}} child{node{45}}}
            child{node{60} child{node{55}
                                    child{node{52}} child{node{} edge from
                                                            parent [draw=none]}}
                                    child{node{70}
                                            child{node{} edge from parent
            [draw=none]} child{node{80}}}}

    \end{tikzpicture}\\
    So after the insertion:\\
        \begin{tikzpicture}[ level distance=1.5cm,
        level 1/.style={sibling distance=3.5cm},
        level 2/.style={sibling distance=2cm}, scale=.65]
            \node{50} child{node{40} child{node{10} child{node{5}
            }child{node{15}}} child{node{45}}}
            child{node{60} child{node{55}
                                    child{node{52}} child{node{} edge from
                                                            parent [draw=none]}}
                                    child{node{70}
                                            child{node{65}} child{node{80}}}}

    \end{tikzpicture}\\
    And no violation occurred.\\
    \newpage
\end{example}
\begin{example}
    Insert 53 to the previous tree:\\
    \begin{tikzpicture}[ level distance=1.5cm,
        level 1/.style={sibling distance=3.5cm},
        level 2/.style={sibling distance=2cm}, scale=.65]
            \node{50} child{node{40} child{node{10} child{node{5}
            }child{node{15}}} child{node{45}}}
            child{node{60} child{node{55}
                                    child{node{52} child{node{} edge from
            parent [draw=none]} child{node{53}}} child{node{}
            edge
            from
                                                            parent [draw=none]}}
                                    child{node{70}
                                            child{node{65}} child{node{80}}}}

    \end{tikzpicture}\\
    Detect LR violation in $55$: Perform $L$ rotation on $52$:\\
    \begin{tikzpicture}[ level distance=1.5cm,
        level 1/.style={sibling distance=3.5cm},
        level 2/.style={sibling distance=2cm}, scale=.65]
            \node{50} child{node{40} child{node{10} child{node{5}
            }child{node{15}}} child{node{45}}}
            child{node{60} child{node{55}
                                    child{node{53} child{node{52}}
            child{node{} edge from parent[draw=none]}}
            child{node{}
            edge
            from
                                                            parent [draw=none]}}
                                    child{node{70}
                                            child{node{65}} child{node{80}}}}

    \end{tikzpicture}\\
    perform $R$ rotation on $55$:\\
    \begin{tikzpicture}[ level distance=1.5cm,
        level 2/.style={sibling distance=3cm},
        level 1/.style={sibling distance=5cm},
        level 3/.style={sibling distance=1.5cm},scale=.65]
            \node{50} child{node{40} child{node{10} child{node{5}
            }child{node{15}}} child{node{45}}}
            child{node{60} child{node{53}
                                    child{node{52}}
            child{node{55}}}
                                    child{node{70}
                                            child{node{65}} child{node{80}}}}

    \end{tikzpicture}\\

\end{example}

\newpage
\fi
\subsection{Exam Question.}
Consider the following question from 2018 exam. 
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.8]{avl-q-exam.png}\\
\end{figure}



\begin{figure}[H]
  \begin{subfigure}[b]{0.47\textwidth}
    \paragraph{Solution.} 

How would it look like an algorithm which computes the 'sum over' query? Or for given $x$, which vertices have a key less than $x$? 
Consider the path $v_1,v_2,v_3...v_n$ on which the Search subroutine went over when looking for $x$. If $v_i$.key $ <  v_{i+1}$.key then it means that $x$ is 
greater then $v_{i}$.key. Furthermore, $x$ is also greater than all the keys in the left subtree of $v_{i}$.

\paragraph{}

Let us transform that insight into an 
Algorithm. Define for each vertex a field that stores the summation of all its left subtree keys plus its own key; call it 'Leftsum'.  
Then computing the sumover query will be done by summing these fields of the vertices in the right turn on the searching path. 
\end{subfigure} 
\hfill
\begin{subfigure}[b]{0.49\textwidth}
  \includegraphics[scale=0.07]{oavl-q.png}
   \end{subfigure} 
   \hfill
\end{figure}



%{Sumover($r$,$x$)}
  \begin{algorithm}[H]
    \If{ $r$ is None }{
      return $0$ 
    }\Else{
      \If{$x > r$.key }{
	return $r$.Leftsum + Sumover( $r$.right, $x$ )  
      }\Else{
      	return Sumover( $r$.left, $x$ )  
      }
    }
  \end{algorithm}


So it has left to show how one could maintain the tree and guarantee a logarithmic height. Consider a vertex $v$ and suppose that both
his direct children are correct AVL trees and have the correct value at the Leftsum field. We know that:
\begin{enumerate}
  \item There is a rotation that balances the tree at the cost of $O(1)$ time.
  \item All the descendants of $v$, at a distance at least two from it,  will remain the same in the sense that their subtree has not changed. 
\end{enumerate}
Therefore we will have to recompute the Sumleft filed for only a $O(1)$ vertices (which are the children and the grandchildren of $v$ previews the rotation). 
Each computation could be done in $O(1)$ time. 



%{AVL-Leftsum Insert(r,x)}
  \begin{algorithm}[H]
    Call Insert $(r,x)$ // (The standard insertion routine for BST) \\  
    Let $p$ be the new node appended to the tree. \\
    \While{ $p$.parent $\neq \emptyset $ } {
      Apply the right rotation. \\
      \For{ any vertex $v$ such that its pointers have changed, sorted by height } {
	$v$.Leftsum $\leftarrow$ $v$.key + $v$.left.Leftsum + $v$.left.right.Leftism 		
      }
      $p \leftarrow$ $p$.parent \\  
    }
  \end{algorithm}


\paragraph{Running time.} The work is done over vertices along a single path, and on each vertex only $O(1)$ time 
is spent, Then we have that the total running time of the algorithm is proportional to the tree height. 
Combining the fact that we maintain the AVL property, it follows that the total time is $\Theta\left( \log n \right)$.  




\section*{Appendix}
\subsection{AVL tree's height}
Let $n_h$ be the minimal number of nodes in an AVL tree of height
$h$.\\
\begin{thm}
    $n_h$ is strictly increasing in $h$.
\end{thm}
\begin{proof} Exercise.
\end{proof}
\begin{thm}
$n_{h} = n_{h-1} + n_{h-2} + 1$.
\end{thm}
\begin{proof}For an AVL tree of height $0$, $n_0 = 1$,
and of height $1$, $n_1 = 2 $ (by checking directly).\\
    Let's look at a minimal AVL tree of height $h$. By the AVL property, one
of its subtrees is of height $h-1$ (WLOG - the left subtree) and by
minimality, its left subtree has to have $n_{h-1}$ nodes. $T$'s right subtree
thus has to be of height $h-2$: It can't be of height $h-1$: $n_{h-1} >
n_{h-2}$ by the previous theorem, and if the right subtree is of height $h-1$ -
we could switch it with an AVL tree of height $h-2$, with fewer nodes - so
less nodes in $T$, contradicting minimality. So the right subtree has
$n_{h-2}$ nodes (once again, by minimality), and thus the whole tree has
    $n_{h} = n_{h-1} + n_{h-2} + 1$ (added the root) nodes.
\end{proof}
\begin{cor}$n_h > 2n_{h-2} + 1$
\end{cor}
\begin{cor}$h = O(\log(n))$
\end{cor}
\begin{proof}
    Assume $k$ is even (why can we assume that?).
    It can be shown by induction that:
    \[
        n_h>2n_{h-2}+1>2(2n_{h-4}+1)+1=4n_{h-4}+(1+2)\ldots >
        2^{\frac{h}{2}}+\sum_{i=0}^{\frac{h}{2}-1}
        2^i=\sum_{i=1}^{\frac{h}{2}}2^i = \frac{2^\frac{h}{2} - 1}{2-1} =  2^{\frac{h}{2}} - 1
    \]
    So $n_h \geq 2^{\frac{h}{2}} - 1$, thus
    \[
        h\leq 2\log(n_h + 1)
    \]
    and for generall AVL tree with $n$ nodes and height $h$:
    \[
         h\leq 2\log(n_h + 1) \leq 2\log(n + 1) = O(\log(n))
    \]
%    By $n_h$'s minimality:
%    $$n \geq n_h \geq F_h = C (\varphi^h - (-\varphi)^h) \geq
%    \tilde{C}\varphi^h$$
%    Thus
%    $h \leq \frac{1}{\tilde{C}} \log_{\varphi}(n)$
\end{proof}
\begin{remark}
    In fact, one can show that $n_h > F_h$ and $F_h$ is the $h$'th Fibonacci
    number. Recall that $F_h = C(\varphi^h - (\psi)^h)$, and this gives a
    tighter bound on $n_h$.
\end{remark}

\chapter{Graphs  - Recitation 9} 

\usetikzlibrary{positioning, arrows}
\tikzset{main node/.style={circle,draw,minimum size=0.8cm,inner sep=0pt},}
\tikzset{edge/.style = {->,> = latex'}}

\iffalse
\newtheorem{prop}{Proposition}
\newtheorem{ex}{Exercise}
\newtheorem{sol}{Solution}
\newtheorem{theorem}{Theorem} \newtheorem{thm}{Theorem}[section]
\newtheorem{conj}[thm]{Conjecture} \newtheorem{lemma}[thm]{Lemma}
\newtheorem{corollary}[thm]{Corollary} \newtheorem{claim}[thm]{Claim}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{definition}{Definition} \newtheorem{remark}{Remark}
   
\pagestyle{empty}

\setlength{\textwidth}{6.5in}
\setlength{\evensidemargin}{0.0in}
\setlength{\oddsidemargin}{0.0in}
\setlength{\topmargin}{-0.25in}
\setlength{\textheight}{9.0in}
\setlength{\baselineskip}{1.3\baselineskip}
\setlength{\parindent}{.0in}
\fi
\tikzset{
node of list/.style = { 
             draw, 
             fill=orange!20, 
             minimum height=6mm, 
             minimum width=6mm,
             node distance=6mm
   },
link/.style = {
     -stealth,
     shorten >=1pt
     },
array element/.style = {
    draw, fill=white,
    minimum width = 6mm,
    minimum height = 10mm
  }
}

\def\LinkedList#1{%
  \foreach \element in \list {
     \node[node of list, right = of aux, name=ele] {\element};
     \draw[link] (aux) -- (ele);
     \coordinate (aux) at (ele.east);
  } 
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\vspace{0.2in}

\section{Graphs}
This is an important section, as you'll see graphs A LOT in this course and in the courses to follow. 

\subsection{Definitions, Examples, and Basics}

\begin{figure}[h]
\begin{subfigure}[b]{0.65\textwidth}
\begin{definition}
A \textbf{non-directed graph} $G$ is a pair of two sets - $G=(V, E)$ - $V$ being a set of vertices and $E$ being a set of couples of vertices, which represent edges ("links") between vertices.
\end{definition}

\textbf{Example}: $G= (\{1,2,3,4\},\ \{\{1,2\}, \{1,3\}, \{3,4\}\})$ is the following graph:\\ 
\end{subfigure}
\begin{subfigure}[b]{0.05\textwidth}
  \
\end{subfigure}
  \begin{subfigure}[b]{0.25\textwidth}
    \begin{tikzpicture}
\node[main node](1){$1$};
\node[main node](2)[below = 1cm of 1]{$2$};
\node[main node](3)[right = 1cm of 1]{$3$};
\node[main node](4)[below = 1cm of 3]{$4$};

\path[draw, thick]
(1) edge node {} (2)
(1) edge node {} (3)
(3) edge node {} (4);
\end{tikzpicture}
\end{subfigure}
\end{figure}
\begin{figure}[h]
\begin{subfigure}[b]{0.65\textwidth}
\begin{definition}
A \textbf{directed graph} $G$ is a pair of two sets - $G=(V, E)$ - $V$ being a set of vertices and $E\subseteq V\times V$ being a set of directed edges ("arrows") between vertices.
\end{definition}
\textbf{Example}: $G=(\{1,2,3,4\},\ \{(1, 2), (1, 4), (4, 1), (4,3)\})$ is the following graph (note that it has arrows): \\ 
\end{subfigure}
\begin{subfigure}[b]{0.05\textwidth}
  \
\end{subfigure}
  \begin{subfigure}[b]{0.25\textwidth}
    \begin{tikzpicture}
\node[main node](1){$1$};
\node[main node](2)[below = 1cm of 1]{$2$};
\node[main node](3)[right = 1cm of 1]{$3$};
\node[main node](4)[below = 1cm of 3]{$4$};

\draw[edge] (1) to (2);
\draw[edge] (1) to (4);
\draw[edge] (4) to (1);
\draw[edge] (4) to (3);
\end{tikzpicture}
\end{subfigure}
\end{figure}

\begin{definition}
  A \textbf{weighted graph} composed by a graph $G=\left( V,E \right)$ (either non-directed or directed) and a weight function $w : E \rightarrow \mathbb{R}$. Usually (but not necessary), we will think about the quantity $w(e)$, where $e \in E$, as the length of the edge.  
\end{definition}

Now that we see graphs with our eyes, we can imagine all sorts of uses for them... For example, they can represent the structure of the connections between friends on Facebook, or they can even represent which rooms in your house have doors between them.

\begin{remark}
Note that directed graphs are a \textbf{generalization} of non-directed graphs, in the sense that every non-directed graph can be represented as a directed graph. Simply take every non-directed edge $\{v,u\}$ and turn it into two directed edges $(v,u), (u,v)$. 
\end{remark}

\begin{remark}
Note that most of the data structures we discussed so far - Stack, Queue, Heap, BST - can all be implemented using graphs.
\end{remark}

Now let's define some things in graphs:

\begin{definition} (Path, circle, degree)
\begin{enumerate} 
\item A \textbf{simple path} in the graph $G$ is a series of unique vertices (that is, no vertex appears twice in the series) $v_1, v_2, ..., v_n$ that are connected with edges in that order. 
\item A \textbf{simple circle} in the graph $G$ is a simple path such that $v_1 = v_n$. 
\item The \textbf{distance} between two vertices $v,u\in V$ is the length of the shortest path between them ($\infty$ if there is no such path).
\end{enumerate}
\end{definition}

\begin{remark}
Note that for all $u,v,w\in V$ the triangle inequality holds regarding path lengths. That is:
$$dist(u,w)\leq dist(u,v) + dist(v, w)$$
\end{remark}

\begin{definition} (connectivity)
\begin{enumerate}
  \item Let $ G =\left( V,E \right)$ be a non-directed graph. A \textbf{connected component} of $G$ is a subset $U \subseteq V$ of maximal size in which there exists a path between every two vertices. 
\item A non-directed graph $G$ is said to be a \textbf{connected} graph if it only has one connected component.
\item Let $ G = \left(V, E\right)$ be a directed graph. A \textbf{strongly connected component} of $G$ is a subset $U \subseteq V$ of maximal size in which for any pair of vertices $u,v \in U$ there exist both directed path from $u$ to $v$ and a directed path form $v$ to $u$.   
\end{enumerate}
\end{definition}
%\begin{figure}[h]
  %\begin{subfigure}[b]{0.4\textwidth} 
%An \textbf{example} of a non-connected graph: \\ \\ \\  \\
%\end{subfigure}
%\begin{subfigure}[b]{0.05\textwidth}
  %\
%\end{subfigure}
%\begin{subfigure}[b]{0.25\textwidth}
%\begin{tikzpicture}
%\node[main node](1){$1$};
%\node[main node](2)[below = 1cm of 1]{$2$};
%\node[main node](3)[right = 1cm of 1]{$3$};
%\node[main node](4)[below = 1cm of 3]{$4$};
%
%\draw[edge] (1) to (2);
%\draw[edge] (3) to (4);
%\end{tikzpicture}
%\end{subfigure}
%\end{figure}

\begin{prop}
Let $G=(V,E)$ be some graph. If $G$ is connected, then $|E| \geq |V|-1$
\end{prop}

\begin{proof}
We will perform the following cool process: Let $\{e_1,...,e_m\}$ be an enumeration of $E$, and let $G_0=(V,\emptyset)$. We will build the graphs $G_1, G_2,... G_m=G$ by adding edges one by one. Formally, we define - 
$$\forall i\in[m]\ \ G_i=(V,\{e_1,...,e_i\})$$
$G_0$ has exactly $|V|$ connected components, as it has no edges at all. Then $G_1$ has $|V|-1$. From there on, any edges do one of the following:
\begin{enumerate}
    \item Keeps the number of connected components the same (the edge closes a cycle)
    '\item Lowers the number of connected components by $1$ (the edges does not close a cycle)
\end{enumerate}

So in general, the number of connected components of $G_i$ is $\geq |V|-i$. 
Now, if $G_m=G$ is connected, it has just one connected component! This means:
$$1 \geq |V| - |E|\ \implies \ |E|\geq|V|-1$$

\end{proof}


\subsection{Graph Representation}

Okay, so now we know what graphs are. But how can we represent them in a computer? There are two main options. The first one is by \textbf{array of adjacency lists}. Given some graph $G$, every slot in the array will contain a linked list. Each linked list will represent a list of some node's neighbors. The second option is to store edges in an \textbf{adjacency matrix}, a $ |V| \times |V| $ binary matrix in which the $v,u$-cell equals $1$ if there is an edge connecting $u$ to $v$. That matrix is denoted by $A_{G}$ in the example below. Note that the running time analysis might depend on the underline representation.     

\textbf{Question.} What is the memory cost of each of the representations? Note that while holding an adjacency matrix requires storing $|V|^2$ bits regardless of the size of $E$, Maintaining the edges by adjacency lists costs linear memory at the number of edges and, therefore, only $ \Theta \left(  |V| + |E| \right) $ bits. 

\paragraph{Example.} Consider the following directed graph:
\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.25\textwidth}
    \begin{tikzpicture}
      \node[main node](1){$1$};
      \node[main node](2)[below = 1cm of 1]{$2$};
      \node[main node](3)[right = 1cm of 1]{$3$};
      \node[main node](4)[below = 1cm of 3]{$4$};

      \draw[edge] (1) to (2);
      \draw[edge] (1) to (4);
      \draw[edge] (4) to (1);
      \draw[edge] (4) to (3);
      \draw[edge] (1) to (3);
      \draw[edge] (3) to (1);
    \end{tikzpicture}  
    \\ \\  
  \end{subfigure} 
  \begin{subfigure}[b]{0.49\textwidth}
    \begin{tikzpicture}
      \foreach \index/\list in {1/{2,3,4,null}, 2/{null}, 3/{1, null}, 4/{1, 3, null}} {
	\node[array element] (aux) at (0,-\index) {\index};
	\LinkedList{\list}
      }
    \end{tikzpicture}  
  \end{subfigure}
  \caption{ Persenting $G$ by array of adjacency lists.  }
\end{figure}

%Its representation will be: \\ \\

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.25\textwidth}
        \begin{tikzpicture}
\node[main node](1){$1$};
\node[main node](2)[below = 1cm of 1]{$2$};
\node[main node](3)[right = 1cm of 1]{$3$};
\node[main node](4)[below = 1cm of 3]{$4$};

\draw[edge] (1) to (2);
\draw[edge] (1) to (4);
\draw[edge] (4) to (1);
\draw[edge] (4) to (3);
\draw[edge] (1) to (3);
\draw[edge] (3) to (1);
\end{tikzpicture} \\ 
  \end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}       
  \begin{tikzpicture}
    \node (1) at (0,0) { };
    \node  (2) at (1,1.7) { $ A_{G} = 
	  \begin{bmatrix}
	0 & 1 & 1 & 1\\
	0 & 0 & 0 & 0\\
	1 & 0 & 0 & 0\\
	1 & 0 & 1 & 0
	\end{bmatrix}
      $ };
      \end{tikzpicture}
\end{subfigure}
\caption{ Presenting $G$ by adjacency matrix. }
\end{figure}

\subsection{Breadth First Search (BFS)}

One natural thing we might want to do is to travel around inside a graph. That is, we would like to visit all of the vertices in a graph in some order that relates to the edges. 
Breadth-first search constructs a breadth-first tree, initially containing only its root, which is the source vertex $s$. Whenever the search discovers a white vertex $v$ in the course of scanning the adjacency list of a gray vertex $u$, the vertex $v$ and the edge ($u$, $v$) are added to the tree. We say that $u$ is the predecessor or parent of $v$ in the breadth-first tree. Since every vertex reachable from $s$ is discovered at most once, each vertex reachable from $s$ has exactly one parent. (There is one exception: because $s$ is the root of the breadth-first tree, it has no parent.) Ancestor and descendant relationships in the breadth-first tree are defined relative to the root $s$ as usual: if $u$ is on the simple path in the tree from the root $s$ to vertex $v$, then $u$ is an ancestor of $v$, and $v$ is a descendant of $u$.
The breadth-first-search procedure BFS on the following page assumes that the graph $G = (V, E)$ is represented using adjacency lists. It denotes the queue by $Q$, and it attaches three additional attributes to each vertex $v$ in the graph:

\begin{enumerate}
    \item $v$.visited is a boolean flag which indicate wheter $v$ was allready visited.
    \item $\pi(v)$ is $v$’s predecessor in the breadth-first tree. If $v$ has no predecessor because it is the source vertex or is undiscovered, then $\pi(v)$ is None/NULL.
\end{enumerate}


%{BFS($G, s$)}
  \begin{algorithm}[H]
    \For {$v\in V$}{
	 $v.visited \leftarrow $ False 
     }
 $Q\leftarrow$ new Queue \\ 
 $Q$.Enqueue($s$) \\
 $s$.visited $\leftarrow$ True \\
 \While {$Q$ is not empty} {
 $u\leftarrow Q$.Dequeue() \\
	\For {neighbor $w$ of $u$} {
	\If {$w$.visited is False}{
		 $w$.visited$\leftarrow$ True \\
		 $\pi(w) \leftarrow u $ \\
		 $Q$.Enqueue($w$)

	      }
	}
      }
  \end{algorithm}


\underline{Correctness}: The example should be enough to explain the correctness. A concrete proof can be found in the book, page 597.

\underline{Runtime}: We can analyse the runtime line-by-line:
\begin{itemize}
\item Lines 1-2: $|V|$ operations, all in $O(1)$ runtime, for a total of $O(|V|)$.
\item Lines 3-6: $O(1)$
\item Lines 7-8: First we need to understand the number of times the $while$ loop iterates. We can see that every vertex can only enter the queue ONCE (since it is then tagged as "visited"), and therefore it runs $\leq |V|$ times. All operations are $O(1)$, and we get a total of $O(|V|)$. 
\item Lines 9-13: Next, we want to understand the number of times this $for$ loop iterates. 
The for loop starts iterating once per vertex, and then the number of its iterations is the same as the number of neighbors that this vertex has. Thus, it runs $O(|E|)$ times.
\end{itemize}
So all in all we get a runtime of $O(|V|+|E|)$

\subsection{Usage of BFS}
Now we have a way to travel through a graph using the edges. How else can we use it?\\ \\ 
\textbf{Exercise}: Present and analyse an algorithm $CC(G)$ which receives some undirected graph $G$ and outputs the number of connected components in $G$ in $O(|V|+|E|)$. \\

\textbf{Solution}: Consider the following algorithm: (Now read the algorithm, it may be in the next page because LaTeX is dumb...)

%{CC($G$)}
  \begin{algorithm}[H]
     count $\leftarrow 0$\\
     \For {$v\in V$ }{
	\If {$v$.visited = False} {
	  count$\leftarrow$count$+1$ \\
	  BFS($G, v$)
	}
      }
	\Return count
  \end{algorithm}


\subsection{Depth First Search (DFS)}
As its name implies, depth-first search searches "deeper" in the graph whenever possible. Depth-first search explores edges out of the most recently discovered vertex $v$ that still has unexplored edges leaving it. Once all of $ v$'s edges have been explored, the search "backtracks" to explore edges leaving the vertex from which $v$ was discovered. This process continues until all vertices that are reachable from the original source vertex have been discovered. If any undiscovered vertices remain, then depth-first search selects one of them as a new source, repeating the search from that source. The algorithm repeats this entire process until it has discovered every vertex.


%{DFS($G$)}
  \begin{algorithm}[H]
   \bf{DFS}( $G$): \\
    \For {$v\in V$}{
 	$vi$.visited $\leftarrow False$
    }
    time $ \leftarrow 1 $\\
    \For {$v\in V$}{
      \If { not $v$.visited } {
	$\pi \left( v \right)  \leftarrow $ null \\ 
	Explore( $G,v$ ) 
     } 
   }

  \end{algorithm}

  \begin{algorithm}[H]
    \bf{Explore}($G,v$): \\
     Previsit($v$)
    \For {$\left( v,u \right) \in E  $}{
      \If { not $u$.visited } {
	$ \pi \left( u \right) \leftarrow v $ \\ 
	Explore( $G, u$ ) 
      }
    }
    Postvisit($v$)
  \end{algorithm}
  \begin{algorithm}[H]
    \bf{Previsit}($v$): \\
    pre($v$) $\leftarrow $ time \\
    time $\leftarrow$ time $+1$
  \end{algorithm}
 \begin{algorithm}[H]
   \bf{Postvisit} ($v$): \\ 
    post($v$) $\leftarrow $ time \\
    time $\leftarrow$ time $+1$
  \end{algorithm}



\paragraph{Properties of depth-first search.} Depth-first search yields valuable information about the structure of a graph. Perhaps the most basic property of depth-first search is that the predecessor subgraph $G_{\pi}$ does indeed form a forest of trees since the structure of the depth-first trees exactly mirrors the structure of recursive calls of explore-function. That is, $u$ = $\pi\left( v \right)$ if and only if explore($G, v$) was called during a search of $ u$'s adjacency list. Additionally, vertex $v$ is a descendant of vertex $u$ in the depth-first forest if and only if $v$ is discovered during the time in which $u$ is gray.
Another important property of depth-first search is that discovery and finish times have a parenthesis structure. If the explore procedure were to print a left parenthesis "$(u$" when it discovers vertex $u$ and to print a right parenthesis r``$u)$" when it finishes $u$, then the printed expression would be well-formed in the sense that the parentheses are properly nested.

The following theorem provides another way to characterize the parenthesis structure.

\paragraph{Parenthesis theorem}
In any depth-first search of a (directed or undirected) graph $G = (V, E)$, for any two vertices $u$ and $v$, exactly one of the following three conditions holds:

\begin{enumerate}
  \item the intervals [pre($u$), post($u$)] and [pre($v$), post($v$)] are entirely disjoint, and neither $u$ nor $v$ is a descendant of the other in the depth-first forest.
  \item the interval [pre($u$), post($u$)] is contained entirely within the interval [pre($v$), post($v$)], and $u$ is a descendant of $v$ in a depth-first tree, or
  \item the interval [pre($v$), post($v$)] is contained entirely within the interval [pre($u$), post($u$)], and $v$ is a descendant of $u$ in a depth-first tree.
  \end{enumerate}

  \paragraph{Proof.} We begin with the case in which pre($u$) $<$ pre($v$). We consider two subcases, according to whether pre($v$) $<$ post($u$). The first subcase occurs when pre($v$) $<$ post($u$), so that $v$ was discovered while $u$ was still gray, which implies that $v$ is a descendant of $u$. Moreover, since $v$ was discovered after $u$, all of its outgoing edges are explored, and $v$ is finished before the search returns to and finishes $u$. In this case, therefore, the interval [pre($v$), post($v$)] is entirely contained within the interval [pre($u$), post($u$)]. In the other subcase, post($u$) $<$ pre($v$), and by defintion, pre($u$) $<$ post($u$) $<$ pre(v) $<$ post($v$), and thus the intervals [pre($u$), post($u$)] and [pre($v$), post($v$)] are disjoint. Because the intervals are disjoint, neither vertex was discovered while the other was gray, and so neither vertex is a descendant of the other.



\paragraph{  Corollary. Nesting of descendants' intervals.}
  Vertex $v$ is a proper descendant of vertex $u$ in the depth-first forest for a (directed or undirected) graph $G$ if and only if pre($u$) $<$ pre($v$) $<$ post($v$) $<$ post($u$).

  
  %%\paragraph{Problem. Two Chess Pieces.}
 %Moti has a tree with n nodes. He is willing to share it with you, which means you can operate on it.

%Initially, there are two chess pieces on the node 1 of the tree. In one step, you can choose any piece, and move it to the neighboring node. You are also given an integer $d$. You need to ensure that the distance between the two pieces doesn't ever exceed $d$.

%Each of these two pieces has a sequence of nodes which they need to pass in any order, and eventually, they have to return to the root. As a curious boy, he wants to know the minimum steps you need to take.

%\paragraph{Solution.} We can find that for any $d$-th ancestor of some $b_i$, the first piece must pass it some time. Otherwise, we will violate the distance limit. The second piece must pass the $d$-th ancestor of each $b_i$ as well. Then we can add the $d$-th ancestor of each $a_i$ to the array $b$, and add the $d$-th ancestor of each $b_i$ to the array $a$.
%
%Then we can find now we can find a solution that each piece only needs to visit its nodes using the shortest route, without considering the limit of $d$, and the total length can be easily computed. We can find that if we adopt the strategy that we visit these nodes according to their DFS order(we merge the array of $a$ and $b$, and sort them according to the DFS order, if the first one is from $a$, we try to move the first piece to this position, otherwise use the second piece), and move the other piece one step closer to the present piece only if the next step of the present piece will violate the distance limit, then we can ensure the movement exactly just let each piece visit its necessary node without extra operations.
%





\ifdefined\BOOK
\else
\setcounter{chapter}{10}
\fi
\chapter{Minimum Spanning Tree Recitation.} 


\section{The MST Problem.}

\begin{definition}
  A spanning tree $T$ of a graph $G=(V,E)$ is a subset of edges in $E$ such that $T$ is a tree (having no cycles), and the graph $(V,T)$ is connected.   
\end{definition}

\begin{problem}[MST] Let $G = (V,E)$ be a weighted graph with weight function $w : E \rightarrow \mathbb{R}$. We extend the weight function to subsets of $E$ by defining the weight of $X\subset E$ to be $w(X)= \sum_{e \in X}w(e)$. The minimum spanning tree (MST) of $G$ is the spanning tree of $G$ that has the minimal weight according to $w$. Note that in general, there might be more than one MST for $G$. 
\end{problem}

\begin{definition}
  \label{def:cut}
  Let $U \subset V$. We define the cut associated with $U$ as the set of outer edges of $U$, namely all the edges $(u,v)\in E$ such that $u\in U$ and $v \notin U$. We use the notation $X = (U, \bar{U})$ to represent the cut. We say that $E^{\prime} \subset E$ respects the cut if $E^{\prime} \cap X = \emptyset$.
\end{definition} 

\begin{lemma}[The Cut-Lemma] 
  \label{lemma:cut}
  Let $T$ be an MST of $G$. Consider a forest $F \subset T$ and a cut $X$ that respects $X$ (i.e. $F \cap X = \emptyset$). Then $F \cup \text{ arg}\min_{e} w(e)$ is also contained in some MST. Note that it does not necessarily have to be the same tree $T$. 
\end{lemma}

\begin{proof} Separate to cases: 
  \begin{itemize}
    \item If $e \in T$, then $F \cup \{ e \} \subset T$ and we done.
    \item Otherwise, consider the second case where $e \notin T$. This means that $T \cup \{ e \}$ has $|V|$ edges and therefore must have a cycle. Let $\Gamma = T \cup \{ e \}$ and let $x$ and $y$ be the endpoints of $e$ (namely $e = (x,y)$). Denote the subset of vertices defining the cut $X$ by $U$. Without loss of generality, let's assume $x \in U$ and $y \in \bar{U}$.


Since $T$ is connected, there is a path $x \rightsquigarrow y$ in $T$, denote it by $\mathcal{P}$. Additionally, because $e \notin T$, we have that $e \notin \mathcal{P}$. This means that there must be another edge in $\mathcal{P}$ connecting a vertex in $U$ to a vertex in $\bar{U}$\footnote{Otherwise, walking along $\mathcal{P}$ cannot take one out of $U$, leading to a contradiction as $\mathcal{P}$ leads to $y$.}. Let $e^{\prime}$ be that edge, we have:\begin{enumerate}
    \item Both $e^{\prime}, e \in X$ So $w(e) \le w(e^{\prime})$.
    \item $e \cup \mathcal{P}$ is a cycle in $\Gamma$. 
  \end{enumerate}

By using the fact that subtracting an edge from a cycle doesn't harm connectivity (see \Cref{claim:subtract}), we can conclude that $\Gamma/\{e^{\prime}\}$ is connected. Since it has $|V|-1$ edges, it must be a spanning tree. On the other hand, by:
  \begin{equation*}
    \begin{split}
      w\left( \Gamma/\{e^{\prime}\} \right) = w\left( T \right) + \overbrace{w(e) - w(e^{\prime})}^{ \le 0} \le w\left( T \right) 
    \end{split}
  \end{equation*}
  So $\Gamma/\{e^{\prime}\}$ is an MST.  Finally, to close the proof, observe that $F \cup \{e\} \subset \Gamma/\{e^{\prime}\}$. This means that, we have found an MST that contains $F \cup \{e\}$. 
  \end{itemize}

\end{proof}
\section{Kruskal Algorithm.} This algorithm constructs the MST iteratively by holding a forest $F$ contained in an MST and then looking for the minimal edge in a cut that it respects. Note, that since $F$ has no cycles, any edge $e \subset E$ that does not create a cycle in $F$ must belong to a cut $X$ that is respected by $F$.

By ensuring that the edges are examined in increasing weight order, we can determine that the first edge that does not create a cycle is also the one with the minimum weight among them. Therefore, according to \Cref{lemma:cut}, we can conclude that the forest obtained by adding $e$ into $F$ is contained in an MST.

\begin{algorithm} 
\SetAlgoLined
\KwResult{Returns MST of given $G=(V,E, w)$ }
\caption{ Kruskal alg.}
sorts  $E$ according to $w$ \\
define $F_{0} = \emptyset$ and $i \leftarrow 0$ \\
\For{ $ e \in E$ in sorted order } {
  \If { $F_{i} \cup \{e\}$ has no cycle} {
    $F_{i+1} \leftarrow F_{i} \cup \{e\}$ \\
    $i \leftarrow i + 1$
  }
}
\Return $F_{i}$
  \label{alg:krus}
\end{algorithm}

\begin{claim}
  For any $i$, $F_{i} \subset $ of an MST.
\end{claim}
\begin{proof}
  By induction. 
  \begin{enumerate}
    \item Base. Let $T$ be an arbitrary MST of $G$. $F_{0} = \emptyset \subset T $.
    \item Assumption. Assume correctness for any $j < i$. 
    \item Step. By the induction assumption, there is an MST $T$ such that $F_{i-1} \subset T$. Denote by $e=(x,y)$ the edge for which $F_{i} = F_{i-1}\cup \{e\}$. According to the algorithm definition, $F_{i} = F_{i-1} \cup \{e\}$ has no cycles (line number (4)). This means that with respect to $F_{i-1}$, $x$ and $y$ belong to two different connected components. Denote the connected component of $x$ by $U$, and the cut it defines by $X = (U, \bar{U})$. It is clear that $F_{i-1}$ respects $X$ (otherwise $U$ would not be a connected component of $F_{i-1}$).
      

      We calim that $e$ is the minaiml edge in $X$. Any other $e^{\prime}$ with $w(e^{\prime}) < w(e)$ is either already in $F_{i-1}$ and therefore cannot be in $X$, or it closes a cycle in $F_{j}$ for some $j<i$. Since $F_{j} \subset F_{i-1}$, it also closes a cycle in $F_{i-1}$. Therefore, it cannot be an edge connecting between $U$ and $\bar{U}$, namely $e^{\prime} \notin X$.


So, if $F_{i-1}$ respects $X$ and $e$ is the minimal edge in $X$, then it follows from \Cref{lemma:cut} that $F_{i} = F_{i-1} \cup \{ e\}$ is contained in an MST.
  \end{enumerate}
\end{proof}

\begin{problem}
  \label{problem:opkrus}
  Let $E^{\prime} \subset E$ such $E$ contains both an MST $T$ and a cycle $C$. Let $e$ be a maximal edge in $C$. Prove that $E^{\prime}/ \{e\}$ contains an MST.  
\end{problem}

\begin{solution} Separate to cases:
  \begin{itemize}
    \item If $e \notin T$, then we done.
    \item So, it is left to prove for $e \in T$. Let $(x,y) = e$ and consider the forest $F = T/\{e\}$.

Since $T$ is a spanning tree, subtracting $e$ from $T$ divides $T$ into two connected components, $U$ and $\bar{U}$, corresponding to all vertices that can be reached from $x$ and $y$, respectively. Let $X$ be the cut $X = (U, \bar{U})$. Note that $F$ respects $X$. On the other hand, since $(x,y)$ is an edge in cycle $C$, there is another path from $x$ to $y$ that does not contain $e$. This path must have a non-trivial intersection with $X$ (otherwise, walking through the path cannot lead to a vertex in $\bar{U}$).

Therefore, there exists an edge $e^{\prime} \neq e$ such that $e \in C \cap X$. Let $e^{\prime} = (u,v)$ and assume, without loss of generality, that $u \in U$ and $v \in \bar{U}$. Since $U$ and $\bar{U}$ are connected components, there are paths $x \rightsquigarrow u$ and $v \rightsquigarrow y$ that connect with $e$ and $e^{\prime}$. This creates a cycle in $T \cup \{e^{\prime}\}$. Using the fact that subtracting an edge from a cycle does not harm the graph's connectivity, it follows that $T^{\prime} = T \cup \{e^{\prime}\} / \{e\}$ is connected and therefore a spanning tree as well.

Furthermore, $w(T^{\prime}) = w(T) - w(e) + w(e^{\prime}) \leq w(T)$. Finally, observe that $T^{\prime} \subset E^{\prime} / \{e\}$, and we get the required result.
  \end{itemize}
\end{solution}


\begin{problem}
  Consider \Cref{problem:opkrus} and the it's solution, give an example to a graph $G$ and subset of edges $E^{\prime}$ such that $e^{\prime}$ defined in the solution is not the minimal edge in $C$. 
\end{problem}

\begin{problem}
  Give an example to a graph with unique MST in which the second spanning tree, share no edge with the MST. 
\end{problem}

\section{Appendix.}

\begin{claim}
  \label{claim:subtract}
  Let $G$ be a connected graph containing a cycle $C$. Then the subtraction of any edge in $C$ gives a connected graph. 
\end{claim}
\begin{proof}
Assume, by contradiction, that a graph $G^{\prime} = G / \{ e \} $, where $e \in C$, is not connected. This means that there are two vertices $u$ and $v$ that have a path between them in $G$, but no such path exists in $G^{\prime}$. Denote this path by $\mathcal{P}$ and observe that $e \in \mathcal{P}$, otherwise, $\mathcal{P}$ would also be a path from $u$ to $v$ in $G^{\prime}$.

Denote the ends of $e$ by $(x,y)=e$. Also, denote $C$ by $\braket{x_{0},x_{1},..x_{i},x,y,y_{0},..,y_{j}}$, where $y_{j}=x_{0}$ and there is an inequality for any other pair of vertices (we used the cycle definition). Then, there is a path $x \rightsquigarrow y$ in $C$, defined by 
\begin{equation*}
  \begin{split}
\braket{x_{i},x_{i-1},..,x_{1},x_{0},y_{j-1},y_{j-2},..,y_{0},y}
  \end{split}
\end{equation*}
 We denote this path by $\mathcal{P}^{\prime}$. By replacing $e$ in $\mathcal{P}$ with $\mathcal{P}^{\prime}$, we obtain a path $u \rightsquigarrow x \rightsquigarrow^{\mathcal{P}^\prime} y \rightsquigarrow v$, which is a path between $u$ and $v$ that does not contain $e$. This contradicts the assumption that there is no path between $u$ and $v$ in $G^{\prime}$.
\end{proof}






\chapter{Union Find - Recitation 11} 

\usetikzlibrary{positioning, arrows}
\tikzset{main node/.style={circle,draw,minimum size=0.8cm,inner sep=0pt},}
\tikzset{edge/.style = {->,> = latex'}}

\iffalse
  \newtheorem{prop}{Proposition}
  \newtheorem{ex}{Exercise}
  \newtheorem{sol}{Solution}
  \newtheorem{theorem}{Theorem} \newtheorem{thm}{Theorem}[section]
  \newtheorem{conj}[thm]{Conjecture} \newtheorem{lemma}[thm]{Lemma}
  \newtheorem{corollary}[thm]{Corollary} \newtheorem{claim}[thm]{Claim}
  \newtheorem{proposition}[thm]{Proposition}
  \newtheorem{definition}{Definition} \newtheorem{remark}{Remark}

  \pagestyle{empty}

  \setlength{\textwidth}{6.5in}
  \setlength{\evensidemargin}{0.0in}
  \setlength{\oddsidemargin}{0.0in}
  \setlength{\topmargin}{-0.25in}
  \setlength{\textheight}{9.0in}
  \setlength{\baselineskip}{1.3\baselineskip}
  \setlength{\parindent}{.0in}
\fi
\tikzset{
  node of list/.style = { 
    draw, 
    fill=orange!20, 
    minimum height=6mm, 
    minimum width=6mm,
    node distance=6mm
  },
  link/.style = {
    -stealth,
    shorten >=1pt
  },
  array element/.style = {
    draw, fill=white,
    minimum width = 6mm,
    minimum height = 10mm
  }
}

\def\LinkedList#1{%
  \foreach \element in \list {
    \node[node of list, right = of aux, name=ele] {\element};
    \draw[link] (aux) -- (ele);
    \coordinate (aux) at (ele.east);
  } 
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\vspace{0.2in}

\section{Union Find.} 

We have mentioned that to find efficiently the minimal spanning tree using Kruskal, One has to answer quickly about whether a pair of vertices $v,u$ share the same connectivity component. In this recitation, we will present a data structure that will allow us to query the belonging of a given item and merge groups at an efficient time cost. 

The problem defines as follows. Given $n$ items $x_1 ... x_{n}$, we would like to maintain the partition of them into disjoints sets by supporting the following operations:  
\begin{enumerate}
  \item Make-Set$(x)$ create an empty set whose only member is $x$. We could assume that this operation can be called over $x$ only once. 
  \item Union$(x,y)$ merge the set which contains $x$ with the one which contains $y$. 
  \item Find-Set$(x)$ returns a pointer to the set holding $x$. 
\end{enumerate}

Notice that the native implementation using pointers array, $A$, defined to store at place $i$ a pointer to the set containing $x$ can perform the Find-Set operation at $O\left( 1 \right)$. The bottleneck of that implementation is that the merging will require us to run over the whole items and changes their corresponding pointer at $A$ one by one. Namely, a running time cost of $\Theta\left( n \right)$ time. Let's review a diffrent approch:

\paragraph{Linked Lists Implementation.}
One way to have a non-trivial improvement is to associate each set with a linked list storing all the elements belonging to the set. Each node of those linked lists contains, in addition to its value and sibling pointer, a pointer for the list itself (the set). Consider the merging operation again. It's clear that having those lists allow us to unify sets by iterating and updating only the elements that belong to them. Still, one more trick is needed to achieve a good running cost. 



%{Uinon$(x,y)$}
  \begin{algorithm}[H]
    \If{size $A[x] \ge $  size$A[y]$  }{
      size $A[x] \leftarrow $   size $A[x]$ +  size $A[y]$ \\
      \For{  $z \in A[y]$ }{
	$A[z] \leftarrow A[x]$ \\ 
      }
      $A[x] \leftarrow A[x] \cup A[y]$ // $O\left( 1 \right)$ concatenation of linked lists.  
    }\Else{
      Union $\left( y,x \right)$ 
    }
  \end{algorithm}


Executing the above over sets at linear size requires at least linear time. Let's analyze what happens when merging $n$ times. As we have seen in graphs, the runtime can be measured by counting the total number of operations each item/vertex does along the whole running. So we can ask ourselves how many times an item change its location and its set pointer. Assume that at the time when $x$ were changed $A[x]$ contains (before the merging) $t$ elements then immediately after that $A[x]$ will store at least $2t$ elements: 
\begin{equation*}
  \begin{split}
    \text{size } A^{(t+1)}[x] \leftarrow  \text{size } A^{(t)}[x] +  \text{size } A^{(t)}[y] \ge 2A^{(t)}[x]  
  \end{split}
\end{equation*}
Hence, if we list down the sizes of the $x$'s set at the moments merging occurred, we could write only $\log n$ numbers before exceeding the maximal size ($n$). That proves that the number of times the vertex changed his pointer is bounded by $\log n$, and the total number of actions costs at most $\Theta\left( n\log n \right)$. 

Notice that in the case in which $m = O\left( 1 \right)$, we will still pay much more than needed. Anyhow the next implementation is going to give us (eventually) a much faster algorithm.

%\begin{figure}[h]
  %\centering
  %\begin{subfigure}[b]{0.25\textwidth}
    %\begin{tikzpicture}
      %\node[main node](1){$1$};
      %\node[main node](2)[below = 1cm of 1]{$2$};
      %\node[main node](3)[right = 1cm of 1]{$3$};
      %\node[main node](4)[below = 1cm of 3]{$4$};
%
      %\draw[edge] (1) to (2);
      %\draw[edge] (1) to (4);
      %\draw[edge] (4) to (1);
      %\draw[edge] (4) to (3);
      %\draw[edge] (1) to (3);
      %\draw[edge] (3) to (1);
    %\end{tikzpicture}  
    %\\ \\  
  %\end{subfigure} 
  %\begin{subfigure}[b]{0.49\textwidth}
    %\begin{tikzpicture}
      %\foreach \index/\list in {1/{2,3,4,null}, 2/{null}, 3/{1, null}, 4/{1, 3, null}} {
	%\node[array element] (aux) at (0,-\index) {\index};
%	%\LinkedList{\list}
      %}
    %\end{tikzpicture}  
  %\end{subfigure}
  %\caption{Presenting $G$ by an array of adjacency lists.  }
%\end{figure}


\paragraph{Forest Implementation.} Instead of associating each set with a linked list, one might attach a first. The vertices hold the values of the items, and we could think about the root of each tree as the represtive of the tree. If two vertices $x,y$ share the same root, then it's clear they belong to the same set. At the initialization stage, Make-Set defines the vertices as roots of trivial trees (single root without any descendants). Then the find method is: 

%{Find$(x)$}
  \begin{algorithm}[H]
    \While{ $ \pi(x) \neq  $ None } { 
      $ x \leftarrow \pi\left( x \right)$
    }
    Return $x$ 
  \end{algorithm}    


We will see that a slight change should be set for the last improvement. But before that, let's try to mimic the decision rule above. Even those, we could define a size field for each root and get the same algorithm as above. Instead, we will define another field that, from first sight, looks identical. Let the rank$\left( v \right)$ of the node $v$ be the height of the $v$. Recall that tree's height defines to be the longest path from the root to one of the vertices. 

\paragraph{Union By Rank Huristic\protect\footnote{Corman calls that rule a heuristic, but please notice that heuristics usually refers to methods that seem to be efficient empirically, yet it doesn't clear how to prove their advantage mathematically. Still, in that course, we stick to Corman terminology.}}. So as we said, first, we will ensure how to mimic the $\log n$ complexity proof under the first implementation. 


%{Uinon$(x,y)$}
  \begin{algorithm}[H]
    $x\leftarrow $ Find$\left( x \right)$ \\
    $y\leftarrow $ Find$\left( y \right)$ \\
    \If{ $ x \neq y $ }{
      \uIf{ rank$\left( y \right) < $  rank$\left( x \right)$ }{
	$\pi\left( y \right) \leftarrow x$ \\
      }\uElseIf {   rank$\left( y \right) = $  rank$\left( x \right)$ } { 
	$\pi\left( y \right) \leftarrow x$ \\
	rank$\left( x \right) \leftarrow $  rank$\left( x \right) + 1 $ \\  
      } \Else { 
	$\pi\left( x \right) \leftarrow y$ \\
      }
    }
  \end{algorithm}

The decision rule in lines (4-8) preserves the correctness of the following claim:
Claim, let $M(r)$ a lower bound over the size of the tree at rank $r$. Then $M\left( r+1 \right) \ge 2M\left( r \right)$. The proof is left as an exercise. Assuming the correctness of the claim, it holds that $M\left( \log n \right) \ge n $. So it immediately follows that the running time takes at most $n\log n$. We could get even tighter bound by noticing that the rank bounds a single query. And therefore, the total cost is at most $m \cdot \log n$. 

\paragraph{Path Compression Heuristic.} The final trick to yield a sub-logarithmic time algorithm is to compress the brunch on which we have already passed and reduce the number of duplicated transitions.     

%{Find$(x)$}
  \begin{algorithm}[H]
    \If{ $ \pi\left( x \right) \neq $ None}{
      $\pi\left( x \right) \leftarrow $Find$\left( \pi\left( x \right) \right)$
    }
    \Else {
	Return $x$
    }
    Return $\pi\left(x\right)$ 
  \end{algorithm}    

Let's analyze the query cost by counting the edges on which the algorithm went over. Denote by finding ($v^{(t)}$) the query which was requested at time $t$ and let $P^{(t)}=v,v_{2} .. v_{k}$ be the vertices path on which the algorithm climbed from $v$ up to his root. Now, observes that by compressing the path, the ranks of the vertices in $P$ must be distinct. Now consider any partition of the line into a set of buckets (segments) $\mathcal{B}= \left\{ B_{i} | B_{i} = [b_{i},b_{i+1}] \right\}$. 


\begin{equation*}
  \begin{split}
    T\left( n, m  \right) &= \text{ direact parent move } + \text{ climbing moves  } =    \\
    &=  \text{ direact parent move } + \text{ stage exchange } +  \text{ inner stage } = \\ 
    & \le m + m \cdot | \mathcal{B} | + \sum_{ B \in \mathcal{B} }{ \text{ steps inside B  }   }\\
    & \le m + m \cdot | \mathcal{B} | + \sum_{ B \in \mathcal{B} }{ \sum_{ rank(u) \in B} { \text{ steps inside B started at }u }  }\\
    & \le m + m \cdot | \mathcal{B} | + \sum_{ B \in \mathcal{B} }{ \sum_{ rank(u) \in B}{ |B| }  } %i\\ 
    %& \le m + m \cdot | \mathcal{B} | + \sum_{ B \in \mathcal{B} }{ \frac{n}{\min{B} } |B| }
  \end{split}
\end{equation*}

For example, consider our last calculation, In which we divided the ranges of ranks into $\log n$ buckets at length $1$, $B_{r} = \{r\}$, then as the size of the subtrees at rank $r$ is at least $2^{r}$ we have that the size of $|B_{r}|$ is at most $\frac{n}{2^{r}}$ and that's why:   
\begin{equation*}
  \begin{split}
    \sum_{ b \in \mathcal{B} }{ \sum_{ rank(u) \in B}{ |B| } } &\le \sum_{ b \in \{ [i] | i \in [ \log n ]  \}  }{ \frac{n}{2^{r}} \cdot 1    } \le  2\cdot n
  \end{split}
\end{equation*}

So the total time is at most $m + m \log n + 2n = \Theta\left( n \right)$. And if we would take the $ \log \log n $ buckets such that $B_{i}$ stores the  $i$th $ \log n /  \log \log n $ numbers. Then the sum above will become:   

\begin{equation*}
  \begin{split}
    & \sum_{ b \in \mathcal{B} }{ \sum_{ rank(u) \in B}{ |B| } }  \le \sum_{ b \in \{ B \in \mathcal{B}  \}  }{ \frac{n}{2^{ \frac{  i \log n} { \log \log n  }  }} \cdot \log \log n }    \le  2\cdot n \\ 
    & \le n  \sum_{ b \in \{ B \in \mathcal{B}  \}  }{ 1 \cdot  \cdot \log \log n }    \le  n \left( \log \log n  \right)^{2}   \\
    & \Rightarrow m + m \cdot \log \log n + n \left( \log \log n  \right)^{2}  
  \end{split}
\end{equation*}

Could we do even better? Yes, Consider a nonunifom parttion $ B_{r} = \{ r,  r+ 1 ... 2^{r} \} $. So first question one should ask is, what is $ |\mathcal{B}|$? ( $ \log^{*}\left( n \right) $ ). On the other hand, the number of vertices in which their rank belongs to the $i$th bucket is at most:    
\begin{equation*}
  \begin{split}
    & \text{ maximal number of nodes at rank } r + \text{ maximal number of nodes at rank } (r+1) + \\ 
    & \text{ maximal number of nodes at rank } (r+2) + ... +  \text{ maximal number of nodes at rank } 2^{r} \\  
    & \frac{n}{2^{r}} + \frac{n}{2^{r+1}} + \frac{n}{2^{r+2}} + ... + \frac{n}{2^{2^{r}}} \Rightarrow | \left\{ v \in B_{r} \right\} | \le 2 \cdot \frac{n}{2^{r}} 
  \end{split}
\end{equation*}

\begin{equation*}
  \begin{split}
    \sum_{ b \in \mathcal{B} }{ \sum_{ rank(u) \in B}{ |B| } }  \le  \sum_{ b \in \mathcal{B} }{ \sum_{ rank(u) \in B}{ \frac{2n}{2^{r}} |B|  } }  \le \sum_{ b \in \mathcal{B} }{ \sum_{ rank(u) \in B}{ 2n } } \le \log^{(*)}\left( n \right) \cdot 2n 
  \end{split}
\end{equation*}



\input{../texlib/tail}



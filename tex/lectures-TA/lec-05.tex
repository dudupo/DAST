
\title{Quicksort And Liner Time Sorts  - Recitation 6} 
\author{Quicksort, Countingsort, Radixsort, And Bucketsort.}
\input{../texlib/head}

%\begin{abstract}
Till now, we have quantified the algorithm performance against the worst-case scenario. And we saw that according to that measure, In the comparisons model, one can not sort in less than $\Theta\left( n\log n \right) $ time. In this recitation, we present two new main concepts that, in some instances, achieve better analyses. The first one is the Exception Complexity; By Letting the algorithm behave non-determinately, we might obtain an algorithm that most of the time runs to compute the task fast. Yet we will not success get down the $\Theta\left(n\log n\right)$ lower bound, but we will go back to use that concept in the pending of the course. The second concept is to restrict ourselves to dealing only with particular inputs. For example, We will see that if we suppose that the given array contains only integers in a bounded domain, then we can sort it in linear time.  

%\end{abstract}

\subsection{Quicksort.}
The quicksort algorithm is a good example of a \textbf{non-determistic} algorithm that has a worst-case running time of $\Theta\left(n^{2}\right)$. Yet its expected running time is $\Theta\left(n\log n\right)$. Namely, fix an array of $n$ numbers. The running of Quicksort over that array might be different. Each of them is a different event in probability space, and the algorithm's running time is a random variable defined over that space. Saying that the algorithm has the worst space complexity of $\Theta(n^{2})$ means that there exists an event in which it runs $\Theta\left(n^{2}\right)$ time with non-zero probability. But practically, the interesting question is not the existence of such an event but how likely it would happen. It turns out that the expectation of the running time is $\Theta\left(n\log n\right)$.  

What is the exact reason that happens? By giving up on the algorithm behavior century, we will turn the task of engineering a bad input impossible.    
 
\begin{algbox}{randomized-partition$(A, p, r)$}
  \begin{algorithm}[H]
      $i \leftarrow $ random $(p, r)$ \\
      $A_{r} \leftrightarrow A_{i} $ \\
      return Partition $(A, p, r)$
    \end{algorithm}
\end{algbox}
 \begin{algbox}{randomized-quicksort $(A, p, r)$}
      \begin{algorithm}[H]
	\If{ $p < r$ }{
	  $q \leftarrow $ randomized-partition $(A, p, r)$ \\
	  randomized-quicksort $(A, p, q-1)$\\
	  randomized-quicksort $(A, q+1, r)$
	}
      \end{algorithm}
\end{algbox}


\paragraph{Partitioning.}
To complete the correctness proof of the algorithm (most of it passed in the Lecture), we have to prove that the partition method is indeed rearranging the array such that all the elements contained in the right subarray are greater than all the elements on the left subarray.  

%which rearranges the subarray $ A_{p : r}$ in place, returning the index of the dividing point between the two sides of the partition.

\begin{algbox}{Partition$(A, p, r)$}
  \begin{algorithm}[H]
    $ x \leftarrow A_{r} $ \\
    $ i \leftarrow p - 1 $ \\
    \For{ $j \in [p, r-1]$ }{
      \If{ $ A_{j} \le x  $}{
	$ i \leftarrow i + 1 $ \\
	$ A_{i} \leftrightarrow A_{j} $\\
      }
    }
   $ A_{i+1} \leftrightarrow A_{r} $\\
   return $ i+1$
  \end{algorithm}
\end{algbox}

\paragraph{claim.} At the beginning of each iteration of the loop of lines 3–6, for any array index $k$, the following conditions hold:
\begin{itemize}
  \item  if $p \le k \le i$, then $A_{k} \le x$.
  \item  if $i + 1 \le k \le j – 1$, then $A_{k} > x$.
  \item  if $k = r$, then $A_{k} = x$.
\end{itemize}
\paragraph{Proof.}
\begin{enumerate}
  \item Initialization: Prior to the first iteration of the loop, we have $i = p – 1$ and $ j= p$. Because no values lie between $p$ and $i$ and no values lie between $i + 1$ and $j – 1$, the first two conditions of the loop invariant are trivially satisfied. The assignment in line 1 satisfies the third condition.
  \item Maintenance: we consider two cases, depending on the outcome of the test in line 4, when $A_{j} > x$: the only action in the loop is to increment $j$. After $j$  has been incremented, the second condition holds for $A_{j – 1}$ and all other entries remain unchanged. When $A_{j} \le x$: the loop increments $i$, swaps $A_{i}$ and $A_{j}$, and then increments $j$. Because of the swap, we now have that $A_{i} \le x$, and condition 1 is satisfied. Similarly, we also have that $A_{j – 1} > x$, since the item that was swapped into $A_{j – 1}$ is, by the loop invariant, greater than $x$.
  \item Termination: Since the loop makes exactly $r – p$ iterations, it terminates, whereupon$ j= r$. At that point, the unexamined subarray $A_{j}, A_{j+1}..  A_{r – 1}$ is empty, and every entry in the array belongs to one of the other three sets described by the invariant. Thus, the values in the array have been partitioned into three sets: those less than or equal to $x$ (the low side), those greater than $x$ (the high side), and a singleton set containing $x$ (the pivot).


\end{enumerate}

\input{quick-1.tex}

\subsection{Linear Time Sorts}
\paragraph{ Counting sort.} Counting sort assumes that each of the n input elements is an integer at size at most $k$. It runs in $\Theta \left(n + k\right)$ time, so that when $k = O(n)$, counting sort runs in $\Theta\left(n\right)$ time.
Counting sort first determines, for each input element $x$, the number of elements less than or equal to $x$. It then uses this information to place element $x$ directly into its position in the output array. For example, if 17 elements are less than or equal to $x$, then $x$ belongs in output position 17. We must modify this scheme slightly to handle the situation in which several elements have the \textbf{same value}, since we do not want them all to end up in the same position.

\begin{algbox}{Counting-sort$(A, n, k)$}
  \begin{algorithm}[H]
  	let B and C be new arrays at size $n$ and $k$ \\ 
  	\For{ $i \in  [0, k]$}{
		$C_{i} \leftarrow 0 $
  	}
	\For{ $j \leftarrow [1, n]$}{
	  $C_{A{j}} \leftarrow C_{A_{j}} + 1 $
 	}
	// $C_{i}$ now contains the number of elements equal to i. \\ 
	\For{ $i \in  [1, k]$}{
	  $C_{i} \leftarrow C_{i} + C_{i – 1} $
 	}
	// $C_{i}$ now contains the number of elements less than or equal to $i$. \\
	// Copy $A$ to $B$, starting from the end of $A$. \\
	\For{ $i \in [n , 1]$}{
	  $B_{C_{A_{j}}} \leftarrow A_{j}$ \\
	  $C_{A_{j}} \leftarrow C_{A{j}} – 1$\\
	  // to handle duplicate values
	}
	  return $B$
  \end{algorithm}
\end{algbox}

Notice that Counting sort can beat the lower bound of $\Omega\left(n \log n\right)$ only because it is not a comparison sort. In fact, no comparisons between input elements occur anywhere in the code. Instead, counting sort uses the actual values of the elements to index into an array.

An important property of counting sort is that it is \textbf{stable}.

\begin{defbox}{Stable Sort.}
 We will say that a sorting algortim is stable if elements with the same value appear in the output array in the same order as they do in the input array. \end{defbox}

Counting sort's stability is important for another reason: counting sort is often used as a subroutine in radix sort. As we shall see in the next section, in order for radix sort to work correctly, counting sort must be stable.

\paragraph{Radix sort}
Radix sort is the algorithm used by the card-sorting machines you now find only in computer museums. The cards have 80 columns, and in each column, a machine can punch a hole in one of 12 places. The sorter can be mechanically "programmed" to examine a given column of each card in a deck and distribute the card into one of 12 bins depending on which place has been punched. An operator can then gather the cards bin by bin, so that cards with the first place punched are on top of cards with the second place punched, and so on.

The Radix-sort procedure assumes that each element in array $A$ has d digits, where digit 1 is the lowest-order digit and digit $d$ is the highest-order digit.


\begin{algbox}{radix-sort(A, n, d)}
  \begin{algorithm}[H]
    \For{ $ i \in [1,d]$ } {
        use a stable sort to sort array $A$ on digit $i$
    }
  \end{algorithm}
\end{algbox}

Lemma 8.3
Given $n$ $d$-digit numbers in which each digit can take on up to $k$ possible values, Radix-sort correctly sorts these numbers in $\Theta\left(d(n + k)\right)$ time if the stable sort it uses takes $\Theta\left(n + k\right)$ time.

\textbf{Proof.} The correctness of radix sort follows by induction on the column being sorted. The analysis of the running time depends on the stable sort used as the intermediate sorting algorithm. When each digit lies in the range $0$ to $k – 1$ (so that it can take on $k$ possible values), and $k$ is not too large, counting sort is the obvious choice. Each pass over $n$ $d$-digit numbers then takes $\Theta(n + k)$ time. There are $d$ passes, and so the total time for radix sort is $\Theta\left(d(n + k)\right)$.

%When d is constant and k = O(n), we can make radix sort run in linear time. More generally, we have some flexibility in how to break each key into digits.
%Lemma 8.4
%Given n b-bit numbers and any positive integer r ≤ b, RADIX-SORT correctly sorts these numbers in $\Theta\left((b/r)(n + 2r)\right)$ time if the stable sort it uses takes Θ(n + k) time for inputs in the range 0 to k.

%Proof For a value r ≤ b, view each key as having d = ⌈b/r⌉ digits of r bits each. Each digit is an integer in the range 0 to 2r – 1, so that we can use counting sort with k = 2r – 1. (For example, we can view a 32-bit word as having four 8-bit digits, so that b = 32, r = 8, k = 2r – 1 = 255, and d = b/r = 4.) Each pass of counting sort takes $ \Theta \left(n + k \right) = \Theta\left(n + 2r\right)$ time and there are d passes, for a total running time of $\Theta\left(d(n + 2r)\right) = \Theta\left((b/r)(n + 2r)\right)$.

%Given n and b, what value of r ≤ b minimizes the expression (b/r)(n + 2r)? As r decreases, the factor b/r increases, but as r increases so does 2r. The answer depends on whether b < ⌊lg n⌋. If b < ⌊lg n⌋, then r ≤ b implies (n + 2r) = Θ(n). Thus, choosing r = b yields a running time of (b/b)(n + 2b) = Θ(n), which is asymptotically optimal. If b ≥ ⌊lg n⌋, then choosing r = ⌊lg n⌋ gives the best running time to within a constant factor, which we can see as follows.1 Choosing r = ⌊lg n⌋ yields a running time of Θ(bn/log n). As r increases above ⌊lg n⌋, the 2r term in the numerator increases faster than the r term in the denominator, and so increasing r above ⌊lg n⌋ yields a running time of Ω(bn / log n). If instead r were to decrease below ⌊lg n⌋, then the b/r term increases and the n + 2r term remains at Θ(n).

%Is radix sort preferable to a comparison-based sorting algorithm, such as quicksort? If b = O(log n), as is often the case, and r ≈ log n, then radix sort's running time is Θ(n), which appears to be better than quicksort's expected running time of Θ(n log n). The constant factors hidden in the Θ-notation differ, however. Although radix sort may make fewer passes than quicksort over the n keys, each pass of radix sort may take significantly longer. Which sorting algorithm to prefer depends on the characteristics of the implementations, of the underlying machine (e.g., quicksort often uses hardware caches more effectively than radix sort), and of the input data. Moreover, the version of radix sort that uses counting sort as the intermediate stable sort does not sort in place, which many of the Θ(n log n)-time comparison sorts do. Thus, when primary memory storage is at a premium, an in-place algorithm such as quicksort could be the better choice.


\paragraph{Bucket sort.}
%Bucket sort assumes that the input is drawn from a uniform distribution and has an average-case running time of O(n). Like counting sort, bucket sort is fast because it assumes something about the input. Whereas counting sort assumes that the input consists of integers in a small range, bucket sort assumes that the input is generated by a random process that distributes elements uniformly and independently over the interval [0, 1). (See Section C.2 for a definition of a uniform distribution.)
Bucket sort divides the interval [0, 1) into n equal-sized subintervals, or buckets, and then distributes the n input numbers into the buckets. Since the inputs are uniformly and independently distributed over [0, 1), we do not expect many numbers to fall into each bucket. To produce the output, we simply sort the numbers in each bucket and then go through the buckets in order, listing the elements in each.
%The BUCKET-SORT procedure on the next page assumes that the input is an array A[1 : n] and that each element A[i] in the array satisfies 0 ≤ A[i] < 1. The code requires an auxiliary array B[0 : n – 1] of linked lists (buckets) and assumes that there is a mechanism for maintaining such lists. (Section 10.2 describes how to implement basic operations on linked lists.) Figure 8.4 shows the operation of bucket sort on an input array of 10 numbers.

\begin{algbox}{bucket-sort(A, n)}
  \begin{algorithm}[H]
    	let B[0 : n – 1] be a new array
	\For{ $i \leftarrow [0, n – 1]$}{
	   make $B_{i}$ an empty list
       	}
	\For{ $i \leftarrow [1, n]$}{
	    insert $A_{i}$ into list $B_{ \lfloor n A_{i} \rfloor} ]$
       	}
	\For{ $i \leftarrow [0, n – 1]$}{
	    sort list $B_{i}$
       	}
	concatenate the lists $B_{0}, B{1}, .. , B_{n – 1}$ together and\\
	return the concatenated lists
  \end{algorithm}
\end{algbox}

\input{../texlib/tail}


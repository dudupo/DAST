
\input{../texlib/head}
\begin{document}
\ifdefined\BOOK
\else
\setcounter{chapter}{6}
\fi
\chapter{Probability.} 

\section{ Probability Spaces. }

\begin{definition}
  A probability space defined by a tuple $(\Omega,P)$ such that:
  \begin{enumerate} 
    \item $\Omega$ is a set, called the sample space. Any element $\omega\in \Omega$ is named an atomic event. Conceptually, we think of atomic events as possible outcomes of our experiment. Any subset $A \subset \Omega$ is an event. 
    \item $P$, called the probability function, is a function that assigns a number in $[0,1]$ to any event, denoted as $P : 2^\Omega \rightarrow [0,1]$, and satisfies:
      \begin{enumerate}
        \item For any event $A \subset \Omega$, $P(A) = \sum_{\omega\in A}P(w)$. 
        \item Normalization, over the atomic events, to $1$, which means $\sum_{\omega\in\Omega}P(\omega)~=~1$.
      \end{enumerate}
  \end{enumerate}
\end{definition}

\begin{example}
  Consider a dice rolling, where each of the faces is indexed by $1,2,3,4,5,6$ and has an equal chance of being rolled. Therefore, our atomic events are associated with the rolling result, and $P$ is defined as $P(\omega) = \frac{1}{6}$ for any such atomic event.
  An example of an event can be $A=$ ''the dice falls on an even number''. The probability of this outcome is:
  \begin{equation*}
    \begin{split}
      P(A)= \sum_{\omega\in A}{ P(\omega) } = P(\{2\}) + P(\{4\}) + P(\{6\}) = 3\cdot \frac{1}{6} = \frac{1}{2} 
    \end{split}
  \end{equation*}
\end{example}

\begin{claim}
Probability function  satisfies the following properties:
\begin{enumerate}
  \item $P(\emptyset) = 0$.
  \item Monotonic, If $A \subset B \subset \Omega$ then $P(A) \le P(B)$.
  \item Union Bound, $P(A \cup B) \le P(A) + P(B)$.
  \item Additivity for disjointness events. If $A\cap B = \emptyset$ then $P(A \cup B) = P(A) + P(B)$.
  \item Denote by $\bar{A}$ the complementary event of $A$, which means $A\cup\bar{A} = \Omega$. Then, $P(\bar{A}) = 1 - P(A)$.
\end{enumerate}
\end{claim}


\begin{example}
  Let's proof the additivity of disjointness property. Let $A,B$ disjointness events, so $A \cap B = \emptyset$ then 
  
  \begin{equation*}
    \begin{split}
      P(A\cup B) &= \sum_{w \in A \cup B}P(w) \\ 
      &= \overbrace{\sum_{w \in A, w \notin{B}}P(w)}^{P(A)} + \overbrace{\sum_{w \in B, w \notin A}P(w)}^{P(B)}  +\overbrace{ \sum_{w \in A, w \in  B}P(w) }^{ 0 } \\ 
      &= P(A) + P(B) 
    \end{split}
  \end{equation*}
\end{example}

\begin{definition}
  Let $(\Omega,P)$ be a probability space. A random variable $X$ on $(\Omega,P)$ is a function $X : \Omega \rightarrow \mathbb{R}$. An indicator, is a random variable defined by an event $A \subset \Omega$ as follows   
  \begin{equation*}
    X(\omega) = \begin{cases}
      1 & \omega \in A \\
      0 & \omega \notin A
    \end{cases}
  \end{equation*}
Sometimes, we will use the notation $\{ X = x \}$ to denote the event $A$ such: 
\begin{equation*}
  \begin{split}
    A = \{ \omega : X(\omega) = x \} := \{ X = x \} 
  \end{split}
\end{equation*}
\end{definition}
\begin{example}
  \ctt{Add dice roll, as an example.}
\end{example}


\begin{definition}
  We will say that two random variable $X,Y : \Omega \rightarrow \mathbb{R}$  are independent if for any $x\in \image X$ and $y \in \image Y$:   
  \begin{equation*}
    \begin{split}
      P(X = x \cap Y = y) &= P(X = x) \cdot P (Y = y) %\Leftrightarrow \\ 
    \end{split}
  \end{equation*}
\end{definition}

\section{ \ctt{ Throw Keys to Cells. } }

\ctt{Add the description of throwing keys to cells. Define the random variable $X_{i}^{j}$.} 

\begin{definition}
  Let $X : \Omega\rightarrow \mathbb{R}$ be a random variable, the expectation of $X$ is 
  \begin{equation*}
    \begin{split}
      \expp{X} = \sum_{\omega\in \Omega}{X(\omega)P(\omega)} = \sum_{x \in \image X}{ x P(X = x)} 
    \end{split}
  \end{equation*}
  Observes that if $P$ is distributed uniformly, then the expectation of $X$ is just the arithmetic mean: 
\begin{equation*}
    \begin{split}
      \expp{X} = \sum_{\omega\in \Omega}{X(\omega)P(\omega)} =  \frac{1}{|\Omega|}\sum_{\omega\in \Omega}{X(\omega)}  
    \end{split}
  \end{equation*}
\end{definition}

\begin{claim}
   The expectation satisfies the following properties:
   \begin{enumerate}
     \item Monotonic, If $X \le Y$ (for any $\omega \in \Omega$) then $\expp{X} \le \expp{Y}$.   
     \item Linearity, for $a,b \in \mathbb{R}$ it holds that $\expp{aX + by} = a\expp{X} + b \expp{Y}$.   
     \item Independently, if $X,Y$ are independent, then $\expp{X\cdot Y} = \expp{X}\cdot \expp{Y}$.  
     \item For any constant $a \in \mathbb{R}$ we have that $\expp{a} = a$. 
   \end{enumerate}
\end{claim}

\begin{proof} 
  \begin{enumerate}
\item Monotonic, if $X \le Y$ then : 
  \begin{equation*}
    \begin{split}
      \expp{X} = \sum_{\omega\in \Omega}{X(\omega)P(\omega)} \le  \sum_{\omega\in \Omega}{Y(\omega)P(\omega)} = \expp{Y}
    \end{split}
  \end{equation*}
\item Linearity,
  \begin{equation*}
    \begin{split}    
      \expp{a X + b Y} &=  \sum_{\omega\in \Omega}{ \left( aX(\omega) + bY(\omega) \right) P(\omega)} \\ 
      &=a\sum_{\omega\in \Omega}{  X(\omega)  P(\omega) } + b \sum_{\omega\in \Omega}{   Y(\omega)  P(\omega)}
    \end{split}
  \end{equation*}
\item Independently, 
  \begin{equation*}
    \begin{split}
      \expp{X Y} &= \sum_{x,y \in \image X \times \image Y}{ xy P(X = x \cap Y = y)} \\
      &= \sum_{x,y \in \image X \times \image Y}{ xy P(X = x )P(Y = y)} \\
      &=\sum_{x \in \image X}{\sum_{y \in \image Y}{ xy P(X = x )P(Y = y)}} \\
      &=\sum_{x \in \image X}{xP(X=x)\sum_{y \in \image Y}{ y P(Y = y)}} \\
      &=\sum_{x \in \image X}{xP(X=x) \expp{Y} }  \\ 
      &=\expp{X} \expp{Y}% \sum_{x \in \image X}{xP(X=x) }  \\ 
    \end{split}
  \end{equation*}
\item Let $X$ be the random variable which is also the constant function $X(\omega) = a$ for any $\omega \in \Omega$. Then we have that
  \begin{equation*}
    \begin{split}
      \expp{X} & = \sum_{\omega\in \Omega}{X(\omega)P(\omega)} \\ & = \sum_{\omega\in \Omega}{a P(\omega)} = a \cdot 1 = a  
    \end{split}
  \end{equation*}
  \end{enumerate}
\end{proof}

\begin{example}
  \ctt{Expectation of indicators and their multiplication.}
  Let $X$ be an indicator of event $A$, what are $\expp{X}$ and $\expp{X^{2}}$? Recall that $X(\omega) = 1$ only if $\omega \in A$ and $0$ otherwise, thus: 
  \begin{equation*}
    X^{k}(\omega) = \begin{cases}
      1^{k} = 1 & \omega \in A \\
      0^{k} = 0 & \text{ else }
    \end{cases}
  \end{equation*}
  Therefore, 
  \begin{equation*}
    \begin{split}
      \expp{X^{k}} = \sum_{\omega \in \Omega}{ X^{k}(\omega)  P(\omega)  } = \sum_{\omega \in \Omega}{ X^{k}(\omega)  P(\omega)  }
    \end{split}
  \end{equation*}
\end{example}
\begin{example}
  \ctt{How many keys trowed into the same cell as the first key thrown to?}
\end{example}

\begin{algorithm}
    	let B[0 : n – 1] be a new array \\
	\For{ $i \leftarrow [0, n – 1]$}{
	   make $B_{i}$ an empty list
       	}
	\For{ $i \leftarrow [1, n]$}{
	    insert $A_{i}$ into list $B_{ \lfloor n A_{i} \rfloor} ]$
       	}
	\For{ $i \leftarrow [0, n – 1]$}{
	    sort list $B_{i}$
       	}
	concatenate the lists $B_{0}, B_{1}, .. , B_{n – 1}$ together and\\
	return the concatenated lists
\caption{bucket-sort($A$, $n$)}
  \end{algorithm}

  Denote by $X_{i} : [n] \rightarrow [n]$ then random variable that counts the number of elements fallen in the $i$th bucket. The Expectation of the sorting running time is:   
  
  \begin{equation*}
    \begin{split}
    \expp{T} &= \expp{  \text{ Inserting into buckets  }   +   \sum_{i} { \text{Sorting $i$th bucket  } }} \\ 
    & = \expp{ \Theta(n) +   \sum_{i} { X_{i}^{2}  }} = \Theta(n) +  \sum_{i}{ \expp{X_{i}^{2}} }  \\
  \expp{X_{i}^{2}} &= \expp{\left( \sum X_{i}^{j} \right)^{2}} = \expp{\sum_{j,j^{\prime}} X_{i}^{j} X_{i}^{j^{\prime}} } =  \sum_{j, j^{\prime}} \expp{X_{i}^{j} X_{i}^{j^{\prime}}} \\ 
  & = \sum_{j \neq j^{\prime}} \expp{X_{i}^{j} X_{i}^{j^{\prime}}} + \sum_{j} \expp{X_{i}^{j} X_{i}^{j}} \\
    & =  \sum_{j \neq j^{\prime}} \expp{X_{i}^{j} X_{i}^{j^{\prime}}} + \sum_{j} \expp{X_{i}^{j}}  \\
        & = 2 { n \choose 2  } \left( \frac{1}{n} \right)^{2} +  n\cdot \frac{1}{n} \\ 
        & = \frac{n-1}{n} + 1  = 2 - \frac{1}{n} \Rightarrow \expp{T} = \Theta(n) + n\left( 2 - \frac{1}{n} \right) = \Theta(n)
    \end{split}
  \end{equation*}

\input{../texlib/tail}


